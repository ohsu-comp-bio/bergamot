 1/1: ls
 1/2: from GenerateAbundanceFile import *
 1/3: generate_meta_file
 1/4: data_dir = '/home/exacloud/lustre1/BioCoders/ProjectCollaborations/DPerunderai_RNA161116DP/results/STAR/'
 1/5: %paste
 1/6: line
 1/7:
    files = [f for f in os.listdir(data_dir)]
    files.sort()
    project = files[0].split('_')[0]
    out_f = open('w', project + '_metadata.txt')
    out_f.write('SampleID'+'\t'+'Identifier'+'\t'+'Sample'+'\t'+'lane'+'\n')
    for f in files:
        project, identifier, sample, lane, r1, o1 = f.split('_')
        pertinent = [f, identifier, sample, lane]
        out_f.write('\t'.join(pertinent)+'\n')
 1/8:
    files = [f for f in os.listdir(data_dir)]
    files.sort()
    project = files[0].split('_')[0]
    out_f = open(project + '_metadata.txt','w')
    out_f.write('SampleID'+'\t'+'Identifier'+'\t'+'Sample'+'\t'+'lane'+'\n')
    for f in files:
        project, identifier, sample, lane, r1, o1 = f.split('_')
        pertinent = [f, identifier, sample, lane]
        out_f.write('\t'.join(pertinent)+'\n')
    out_f.close()
 1/9: f
1/10: files
1/11:
    files = [f for f in os.listdir(data_dir) if f.startswith('RNA')]
    files.sort()
    project = files[0].split('_')[0]
    out_f = open(project + '_metadata.txt','w')
    out_f.write('SampleID'+'\t'+'Identifier'+'\t'+'Sample'+'\t'+'lane'+'\n')
    for f in files:
        project, identifier, sample, lane, r1, o1 = f.split('_')
        pertinent = [f, identifier, sample, lane]
        out_f.write('\t'.join(pertinent)+'\n')
    out_f.close()
1/12: f
1/13: f.split('_')
1/14:
    files = [f for f in os.listdir(data_dir) if f.startswith('RNA')]
    files.sort()
    project = files[0].split('_')[0]
    out_f = open(project + '_metadata.txt','w')
    out_f.write('SampleID'+'\t'+'Identifier'+'\t'+'Sample'+'\t'+'lane'+'\n')
    for f in files:
        #project, identifier, sample, lane, r1, o1 = f.split('_')
        project, identifier, sample, r1= f.split('_')
        #pertinent = [f, identifier, sample, lane]
        pertinent = [f, identifier, sample]
        out_f.write('\t'.join(pertinent)+'\n')
    out_f.close()
1/15: gtfFile = "/home/exacloud/lustre1/BioCoders/DataResources/Genomes/mm10/release-87/gtf/Mus_musculus.GRCm38.87.gtf"
1/16: gtf_read_dir = gtfFile.split('/')[:-1].join('/')
1/17: gtf_read_dir = gtfFile.split('/')[:-1].join()
1/18: gtf_read_dir = gtfFile.split('/')[:-1]
1/19: gtf_read_dir
1/20:     gtf_read_dir = join('/',gtfFile.split('/')[1:-1])
1/21: join
1/22: gtf_read_dir = gtfFile.split('/')[1:-1]
1/23: gtf_read_dir.join
1/24: join
1/25: ?join
1/26: '/'.join(gtf_read_dir)
1/27:     gtf_read_dir = .join('/',gtfFile.split('/')[:-1])
1/28: gtf_read_dir = .join('/',gtfFile.split('/')[:-1])
1/29: gtf_read_dir = '/'.join(gtfFile.split('/')[:-1])
1/30: gtf_read_dir
1/31:
    code ="""require(data.table)
    require(NMF)
    require(affy)
    require(limma)
    require(AnnotationDbi)
    require(biomaRt)
    source("{code_dir}/AbundanceFunctions/BiasReduce.R")
    source("{code_dir}/AbundanceFunctions/ExtractTransformLoad.R")
    source("{code_dir}/AbundanceFunctions/DifferentialAnalysis.R")
    source("{code_dir}/AbundanceFunctions/NonVisualOutput.R")
    source("{code_dir}/GenomicsFunctions/ReadAndParse.R")
    source("{code_dir}/AssociationFunctions/gs.wrapper.R")
    source("{code_dir}/AssociationFunctions/PathwayAnalysis.R")
    source("{code_dir}/BcorePlotting/SummaryPlots.R")
    source("{code_dir}/BcorePlotting/MultipleTestingCorrection.R")
    source("{code_dir}/BcorePlotting/ClusteringPlots.R")


    setwd({read_dir})

    # constants
    # max data rows for hclust
    hclust.limit = 2^16

    # quantile of data distribution reqd in one group's worth of data if too many rows for hclust()
    hc.frac.cut = 0.75;
    SJ.counts.na.frac = 0.25;
    # max fraction of samples not having detected a splice junction for the splice
    # junction to be retained in raw data


    # regression parameters
    na.lim = 0 # max NAs per row tolerated by lm() at least in some cases
    do.not.regress = "alograw" # control norm not to be used for regression stats
    # plotting colors
    colors.rgb = c(rgb(0,0,0),rgb(0.1,0.1,1),rgb(0,.7,.7),rgb(0,.7,0),rgb(.7,1,0),rgb(.7,0,.7))
    md.file = {meta_file}
    md.orientation = "byRow" # sampleIDs are in @ row. alt:byCol (IDs in @ col)
    md.IDcol = "SampleID" # reqd if md.orientation is byRow; byCol==headers are IDs

    # gene annotation
    taxID = {taxID}
    gene2ENSfile = "/home/exacloud/lustre1/BioCoders/DataResources/AnnotationSources/ncbi/gene2ensembl.gz"
    gene2ENS.col = c("taxID","EntrezID","Gene","RefSeqTranscript","EnsemblTranscript","RefSeqProtein","EnsemblProtein")
    gtfFile = "{gtfFile}"
    gtf.feature = "{gtf_feature}"
    gtf.orig.col = c("gene_id","gene_name","gene_biotype")
    gtf.col = c("Gene","Symbol","biotype")
    proj.title = "{project_title}"
    gtf.Rdir = "{gtf_read_dir}"

    readdir = "{read_dir}"
    readpattern = "{read_pattern}"
    useme.cols = "{useme_cols}"
    label.from.colname = "{label_from_colname}"
    samps = dir(path=readdir, pattern=readpattern)
    samp.labels = gsub(label.from.colname,'\\1', samps)

    annCol.names = 'group"
    annCol.lmBy = "{lmBy}"
    annCol.label = "{label_from_colname}"

    # read in STAR alignments
    STAR.data = read_STAR(useme.cols=useme.cols,label.from.colname=label.from.colname,annCol.label=annCol.label,annCol.names=annCol.names,annCol.normBy=NULL,annCol.lmBy=annCol.lmBy,readpattern=readpattern,unstranded.col = list(gene.counts = c(1:4),SJ.counts = c(1:3,7)))

    # filter out SJs with too many NAs
    if(any( names(STAR.data$LoM.raw)=="SJ.counts" & exists("SJ.counts.na.frac") )){
      STAR.data$SJ.counts.orig = STAR.data$LoM.raw$SJ.counts
      for(tag in names(STAR.data$LoM.raw$SJ.counts) ){
        STAR.data$LoM.raw$SJ.counts[[tag]] =
          STAR.data$LoM.raw$SJ.counts[[tag]][
            rowSums(is.na(STAR.data$LoM.raw$SJ.counts[[tag]]))
          <= (SJ.counts.na.frac*ncol(STAR.data$LoM.raw$SJ.counts[[tag]])), ]
      }
    }

    gtf = readENSgtf(filename=gtfFile)
    genes.gtf = gtf[feature==gtf.feature, mget(gtf.orig.col)]
    names(genes.gtf) = gtf.col
    setkeyv(genes.gtf,gtf.col[1])

    ##### parse of NCBI's EntrezID to Ensembl translation
    Entrez2Ensembl = fread(paste("zgrep",paste0("-E '^",taxID,"'"),gene2ENSfile))
    names(Entrez2Ensembl) = gene2ENS.col
    setkeyv(Entrez2Ensembl,gene2ENS.col[3])

    ##### add EntrezIDs to genes.gtf
    tmp = Entrez2Ensembl[,mget(gene2ENS.col[2:3])]; tmp=tmp[!duplicated(tmp),]
    genes.gtf = merge(genes.gtf,tmp,all.x=TRUE)
    rm(tmp)

    myreads = STAR.data$myreads
    LoM.norms = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(LoM.norms) = names(STAR.data$LoM.raw)
    for(tag in names(STAR.data$LoM.raw) ){
      if( length(STAR.data$LoM.raw[[tag]])>1 ){
        LoM.norms[[tag]] = normMatrix(tag=tag,raw.mat=STAR.data$LoM.raw[[tag]][[myreads]])
      } else {
        LoM.norms[[tag]] = normMatrix(tag=tag,raw.mat=STAR.data$LoM.raw[[tag]][[1]])
      }
    }


    ## custom: reannotate from metadata
    ## run if necessary metadata is in file and not also in FASTQ file names
    md.dt = fread({meta_file})
    # map annotation to read matrix
    if( md.orientation == "byCol" ){ # samples are one per column
      idx2 = match( colnames(STAR.data$LoM.raw[[1]][[myreads]]), names(md.dt) )
    } else { # samples are one per row
      idx2 = match( colnames(STAR.data$LoM.raw[[1]][[myreads]]), md.dt[,get(md.IDcol)] )
    }
    idx1 = which(!is.na(idx2)); idx2 = idx2[idx1]
    if( sum(idx1==1:ncol(STAR.data$LoM.raw[[1]][[myreads]]))==ncol(STAR.data$LoM.raw[[1]][[myreads]]) ){
      if( md.orientation == "byCol" ){ # samples are one per column
        annCol = NULL
        namecol = setdiff( 1:ncol(md.dt), idx2 )
        md.factors = as.character(md.dt[,namecol,with=F])
        for(k in 1:nrow(md.dt) ){
          annCol[[ md.dt[k,get(names(md.dt)[namecol])] ]][idx1] =
          as.vector( md.dt[ k, mget(names(md.dt)[idx2]) ] )
        }
      } else { # samples are one per row
        annCol = NULL
        namecol = setdiff( names(md.dt), md.IDcol )
        md.factors = as.character(namecol)
        for( k in setdiff(names(md.dt),md.IDcol) ){
          annCol[[ k ]][idx1] =
          as.vector( md.dt[ idx2, get(k) ] )
        }
      }
    } else {
      stop(paste("Some samples have no annotation in",md.file))
    }


    mytypes = names(LoM.norms)
    mynorms = names(LoM.norms[[1]])

    grpBy = annCol [[annCol.lmBy]]
    annCol.plotme = c(annCol.lmBy)
    clim.pct=0.96
    histbins=20
    for(i in 1:length(mytypes) ){
      for(j in 1:length(mynorms)){
        # set up matrices and config for plotting
        if( length(STAR.data$LoM.raw[[ mytypes[i] ]])>1 ){
          rawmat = STAR.data$LoM.raw[[ mytypes[i] ]][[myreads]]
        } else {
          rawmat = STAR.data$LoM.raw[[ mytypes[i] ]][[1]]
        }
        normmat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(rawmat)=samp.labels; colnames(normmat)=samp.labels
        plotdata = list(plotdir='.',plotbase=paste(mynorms[j],mytypes[i],sep='.'),plottitle=proj.title)
        rowmask = rowSums(rawmat>1,na.rm=T) > (ncol(rawmat)/length(unique(grpBy)) ) & rowSums(is.na(normmat)) < (ncol(normmat)/length(unique(grpBy)))
        if(sum(rowmask)>hclust.limit){ #hack! build optimization!
          rowmask = rowSums(rawmat>=quantile(rawmat,probs=hc.frac.cut,na.rm=T),na.rm=T) > (ncol(rawmat)/length(unique(grpBy)) ) & rowSums(is.na(normmat)) < (ncol(normmat)/length(unique(grpBy)) )
        }
        ans = summary.plots(rawmat=log2(rawmat +1), normmat=normmat, mynorm=mynorms[j], samp.labels=samp.labels, samp.classes=grpBy, plotdata=plotdata,plot2file=TRUE,histbins=histbins, colorspec=colors.rgb)
        ans = qc.clusters(rawmat=log2(rawmat[rowmask,] +1), normmat=normmat[rowmask,], attribs=annCol[annCol.plotme], oneclass=annCol.lmBy, colorspec=colors.rgb, plotdata=plotdata, plot2file=TRUE, clim.pct=clim.pct)
      }
    }

    regress_lsls = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(regress_lsls) = names(STAR.data$LoM.raw)
    contr_ls = list("Condition"=list(baseline="{baseline}",contr.FUN="contr.treatment"))
    # set baseline for regression in parameter(s) of interest
    # contr.treatment generates regression coefficients that are like (adjusted mean) ratios of other groups to the baseline group.
    # contr.sum generates coefficients that are like (adjusted mean) ratios to average all for all but the mandadory ommitted treatment group (because there is always one fewer independent pairwise comparison than there are pairs).

    lm_expr = "y ~ {lmBy}"
    rowmask_ls = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(rowmask_ls) = names(STAR.data$LoM.raw)
    for(i in 1:length(mytypes) ){
      for(j in which(!mynorms %in% do.not.regress) ){
        # set up data matrices
        if( length(STAR.data$LoM.raw[[ mytypes[i] ]])>1 ){
          rawmat = STAR.data$LoM.raw[[ mytypes[i] ]][[myreads]]
        } else {
          rawmat = STAR.data$LoM.raw[[ mytypes[i] ]][[1]]
        }
        tmp = unlist(lapply(contr_ls,function(x){x$baseline}))
        tmp=paste(names(tmp),tmp,sep=".")

        plotdata = list(plotdir='./',plotbase=paste(mynorms[j],mytypes[i],'vs',tmp,sep='.'),plottitle=proj.title)
        normmat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(rawmat)=samp.labels; colnames(normmat)=samp.labels

        # prepare rowmask for heatmap/MDS (remove non-expr or low expr>hclustlim)
        rowmask = rowSums(rawmat>1,na.rm=T) > (ncol(rawmat)/length(unique(grpBy)) ) & rowSums(is.na(normmat)) <= na.lim
        rowmask_ls[[ mytypes[i] ]][[ mynorms[j] ]] = rowmask # save for later

        # regression

        regress_lsls[[mytypes[i]]][[mynorms[j]]] = regressMatrix(normmat[rowmask,], expt.design=annCol[annCol.lmBy],
        lm_expression=lm_expr, contr_list = contr_ls, plot2file = TRUE, plotdata = plotdata)
      }
    }

    topn=500 # number with which to play
    for( i in 1:length(mytypes) ){
      for( j in which(!mynorms %in% do.not.regress) ){
        ans = regress_lsls[[mytypes[i]]][[mynorms[j]]]$q_list
        cat(mytypes[i],mynorms[j],"1-p0:",signif(unlist(lapply(2:length(ans),function(x){1-ans[[x]]$pi0})),2),"\n")
        cat(mytypes[i],mynorms[j],"qcut:",signif(unlist(lapply(2:length(ans),function(x){quantile(ans[[x]]$qvalues,probs=topn/length(ans[[x]]$qvalues))})),3),
        "returns ~",topn,"out of",length(ans[[2]]$qvalues),"\n")
    }}

    # Plots for each design factor (default) or for factors specified in facSel
    #   1) Histogram of p values that were included in the design
    #      Look for a peak on the left, and no peaks in the middle or on the right
    #   2) qvalue's default plots, with full qvalue range c(0,1) plotted
    #      Look for descent to pi0 in the top left tuning plot with good asymptote
    #      The slow/steep rise in q-values in remaining plots depends on resolving power of data


    lmBy = annCol.lmBy
    histbins=20
    for( i in 1:length(mytypes) ){
      for( j in which(!mynorms %in% do.not.regress) ){
        normmat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(normmat)=samp.labels
        reg_ls = regress_lsls[[mytypes[i]]][[mynorms[j]]]
        mymask= rowmask_ls[[mytypes[i]]][[mynorms[j]]]
        # pull out regression design and set up plot config
        tmp = unlist(lapply(contr_ls,function(x){x$baseline}))
        tmp=paste(names(tmp),tmp,sep=".")
        plotdata = list(plotdir='./',plotbase=paste(mynorms[j],mytypes[i],tmp,sep='.'),plottitle=proj.title)
        for(fac in names(reg_ls$q_list)[grep(lmBy,names(reg_ls$q_list))] ){
          pdata = plotdata
          pdata$plotbase = paste(plotdata$plotbase,make.names(fac),sep='.')
          ans = qcQvalues(norm_x=normmat[mymask,], pvalue_v=reg_ls$p_mat[,fac], obj_qvalue=reg_ls$q_list[[fac]], attribs=annCol[lmBy],
          oneclass=lmBy, plotdata=pdata, colorspec=colors.rgb, histbins=histbins, plot2file=TRUE)
        }
      }
    }

    # Build of ratios to baselines for desired design factors
    # Also build masks selecting genes based on q-values per factor
    #  and follow-up considerations such as expression level and fold change
    # Loop over q-value cuts to assist with final cut selection.
    # Plot heatmaps and MDS plots based on selected genes and designed ratios


    ngene_v = c(200,500,1000) # q-value cuts by number; can also cut by q-value
    ratioby_ls = list("Condition"=contr_ls$Condition$baseline)
    ratio_fold = 1.3
    intensity_fold = 2

    cut_ls = list(q_combine="OR", rcut_fold=ratio_fold, icut_fold=intensity_fold)
    # settings for heatmaps
    annCol.plotme = annCol.lmBy # heatmap tracks
    clustrowmin = 10 # min data rows for heatmap and MDS plots
    # save ratios and selections
    select_lsmk = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(select_lsmk) = names(STAR.data$LoM.raw)
    ratio_lsmat = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(ratio_lsmat) = names(STAR.data$LoM.raw)
    for( i in 1:length(mytypes) ){
      for( j in which(!mynorms %in% do.not.regress) ){
        normmat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(normmat)=samp.labels
        if( grepl('SJ',mytypes[i]) ){ #not the most robust way to find SJs
          # map gene IDs back to data matrix using mapping built earlier
          # this enables include_ID to select rows
          idx2 = match( rownames(STAR.data$LoM.raw[[ mytypes[i] ]][[1]]), mySJ_dt[,get(pos.col)] )
          idx1 = which(!is.na(idx2)); idx2 = idx2[idx1]
          rownames(normmat)[idx1] = mySJ_dt[idx2,get(gtf.col[1])]
        }
        mymask= rowmask_ls[[mytypes[i]]][[mynorms[j]]]
        reg_ls = regress_lsls[[mytypes[i]]][[mynorms[j]]]$q_list
        reg_ls = reg_ls[!grepl('Intercept',names(reg_ls))]
        plotdata = list(plotdir='./',plotbase=paste(mynorms[j],sub('counts','ratios',mytypes[i]),'minratio',paste0(ratio_fold,'x'),'minexpr',round(min(normmat,na.rm=T)+log2(intensity_fold),1),sep='.'),plottitle=proj.title)
        for( ngenes in ngene_v ) {
          # calculate and plot ratios
          cut_ls$qcut = ngenes
          pdata = plotdata
          pdata$plotbase = paste(plotdata$plotbase,'ratio.vs',ratioby_ls$strain,ngenes,sep='.')
          # this function makes the ratios and cuts
          ans = designRatios(normmat[mymask,], q_list=reg_ls, attribs=annCol[annCol.plotme], ratioby_ls=ratioby_ls, cut_ls=cut_ls)
          # this function creates the heatmap and MDS plot
          if( sum(ans$rowmask)>clustrowmin ){
            ans2= plotRatios( ratiomat=ans$ratiomat, attribs=annCol[annCol.plotme], oneclass=lmBy, plotdata=pdata, colorspec=colors.rgb, rowmask=ans$rowmask, plot2file=TRUE)
          }
          # save selections
          select_lsmk[[mytypes[i]]][[mynorms[j]]][paste(fac,ngenes,sep=":")] = ans['rowmask']
        }
        # save ratiomat (not dependent on ngene cut) after ngene loop
        ratio_lsmat[[mytypes[i]]][[mynorms[j]]] = ans$ratiomat
      }
    }

    dir.create(file.path(getwd(),'PairwiseScatter'),showWarnings=FALSE)
    for( i in 1:length(mytypes) ){
      for( j in 1:length(mynorms) ){
        scatter_mat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(normmat)=samp.labels
        plotdata = list(plotdir='./PairwiseScatter',plotbase=paste(mynorms[j],mytypes[i],'Scatter',lmBy,sep=':'),plottitle=proj.title)
        scatterplot(scatter_mat, attribs=annCol[[lmBy]],plotdata=plotdata,plot2file = TRUE)
      }
    }


    ratio_mat = ratio_lsmat[[{path_type}]][[{path_norms}]]
    reg_ls = regress_lsls[[{path_type}]][[{path_norms}]]
    ID = rownames(ratio_mat)

    ensembl=useMart("ensembl")
    listDatasets(ensembl)
    mouse = useMart("ensembl", dataset = "{mart_dataset}")
    human = useMart("ensembl", dataset = "hsapiens_gene_ensembl")

    hsa_entrezID = getLDS(attributes = "ensembl_gene_id",mart = mouse,  attributesL = "entrezgene", martL = human)
    Entrez.ID = character(length(ID))
    Entrez.ID[] = NA
    idx2 = match(ID,hsa_entrezID$Gene.ID)
    idx1 = which(!is.na(idx2)); idx2= idx2[idx1]
    Entrez.ID[idx1] = hsa_entrezID$EntrezGene.ID[idx2];
    Entrez.ID = as.numeric(Entrez.ID)

    ###Human gene symbols
    hsa_symbol = getLDS(attributes = "ensembl_gene_id",mart = mouse,  attributesL = "hgnc_symbol", martL = human)
    Anno.Symbol = character(length(ID))
    Anno.Symbol[] = NA
    idx2 = match(ID,hsa_symbol$Gene.ID)
    idx1 = which(!is.na(idx2)); idx2= idx2[idx1]
    Anno.Symbol[idx1] = unlist(hsa_symbol$HGNC.symbol[idx2]);
    # assemble IDs for annotation
    backgroundset = as.data.table(cbind(ID, Entrez.ID, Anno.Symbol))
    # assemble signatures


    reg_ls = regress_lsls$gene.counts${path_norms}
    fac = names(reg_ls$q_list)[grep(lmBy,names(reg_ls$q_list))]
    rowmask = select_lsmk[[{path_type}]][[{path_norms}]][[paste(fac,ngenes,sep=":")]]
    sig_gmt = NULL
    betas = reg_ls$b_mat[,fac]
    for (k in names(masks)){
        sig_gmt = NULL
        rowmask = select_lsmk[[mytypes[i]]][[mynorms[j]]][[k]]
        sig_gmt[['all']] =  ID[rowmask]
        fileSettings = list(directory='{read_dir}',baseFilename=paste(k,'with_Uni',sep='_')
        path_ls = ags.wrapper(setlist=sig_gmt, backgroundset, include.identifiers=TRUE, anno.uni=TRUE,
                              fileSettings = fileSettings,
                              functiondir = "{code_dir}/AssociationFunctions/PathwayAnalysis.R",
                              resourcedir = "/home/exacloud/lustre1/BioCoders/DataResources/AnnotationSources/old_association_resources",
                              return.OM=TRUE, ecut=0.05, ocut=5)
        fileSettings = list(directory='{read_dir}',baseFilename=paste(k,'with_out_Uni',sep='_')
        pathi_ls = ags.wrapper(setlist=sig_gmt, backgroundset, include.identifiers=TRUE, anno.uni=FALSE,
                       fileSettings = fileSettings,
                       functiondir = "{code_dir}/AssociationFunctions/PathwayAnalysis.R",
                       resourcedir = "/home/exacloud/lustre1/BioCoders/DataResources/AnnotationSources/old_association_resources",
                       return.OM=TRUE, ecut=0.05, ocut=5)
    }
    gtf.Rdir = "{gtf_read_dir}"
    out_norm_mat = LoM.norms$gene.counts$loess[rowmask_ls${path_type}${path_norms},]
    out_table = outputTable(normmat= out_norm_mat, gtf.file = "{gtfFile}",ratiomat = ratio_lsmat${path_type}${path_norms}, q_list=reg_ls$q_list)
    write.csv(out_table, row.names = FALSE, file=paste({project_title},{path_norms},'Normed_with_Ratio_and_Abundance.csv',sep='_'),quote=FALSE)
    """
1/32: code
1/33: read_dir ="/home/exacloud/lustre1/BioCoders/ProjectCollaborations/DPerunderai_RNA161116DP/results/STAR/"
1/34: meta_file = "/home/users/estabroj/RNA161116DP_metadata.txt"
1/35: code_o
1/36: code_dir = "/home/exacloud/lustre1/BioCoders/ProjectCollaborations/DPerunderai_RNA161116DP/code/"
1/37: taxID = 10090
1/38: gtfFile = "/home/exacloud/lustre1/BioCoders/DataResources/Genomes/mm10/release-87/gtf/Mus_musculus.GRCm38.87.gtf"
1/39: project_title = "DPerunderai_auto_generate"
1/40: baseline ="WT"
1/41:
    gtf_read_dir = '/'.join(gtfFile.split('/')[:-1])
    out_f = open(project_title+'_analysis.R','w')
1/42: gtf_read_dir
1/43:
    code ="""
    require(data.table)
    require(NMF)
    require(affy)
    require(limma)
    require(AnnotationDbi)
    require(biomaRt)
    source("{code_dir}/AbundanceFunctions/BiasReduce.R")
    source("{code_dir}/AbundanceFunctions/ExtractTransformLoad.R")
    source("{code_dir}/AbundanceFunctions/DifferentialAnalysis.R")
    source("{code_dir}/AbundanceFunctions/NonVisualOutput.R")
    source("{code_dir}/GenomicsFunctions/ReadAndParse.R")
    source("{code_dir}/AssociationFunctions/gs.wrapper.R")
    source("{code_dir}/AssociationFunctions/PathwayAnalysis.R")
    source("{code_dir}/BcorePlotting/SummaryPlots.R")
    source("{code_dir}/BcorePlotting/MultipleTestingCorrection.R")
    source("{code_dir}/BcorePlotting/ClusteringPlots.R")


    setwd({read_dir})

    # constants
    # max data rows for hclust
    hclust.limit = 2^16

    # quantile of data distribution reqd in one group's worth of data if too many rows for hclust()
    hc.frac.cut = 0.75;
    SJ.counts.na.frac = 0.25;
    # max fraction of samples not having detected a splice junction for the splice
    # junction to be retained in raw data


    # regression parameters
    na.lim = 0 # max NAs per row tolerated by lm() at least in some cases
    do.not.regress = "alograw" # control norm not to be used for regression stats
    # plotting colors
    colors.rgb = c(rgb(0,0,0),rgb(0.1,0.1,1),rgb(0,.7,.7),rgb(0,.7,0),rgb(.7,1,0),rgb(.7,0,.7))
    md.file = {meta_file}
    md.orientation = "byRow" # sampleIDs are in @ row. alt:byCol (IDs in @ col)
    md.IDcol = "SampleID" # reqd if md.orientation is byRow; byCol==headers are IDs

    # gene annotation
    taxID = {taxID}
    gene2ENSfile = "/home/exacloud/lustre1/BioCoders/DataResources/AnnotationSources/ncbi/gene2ensembl.gz"
    gene2ENS.col = c("taxID","EntrezID","Gene","RefSeqTranscript","EnsemblTranscript","RefSeqProtein","EnsemblProtein")
    gtfFile = "{gtfFile}"
    gtf.feature = "{gtf_feature}"
    gtf.orig.col = c("gene_id","gene_name","gene_biotype")
    gtf.col = c("Gene","Symbol","biotype")
    proj.title = "{project_title}"
    gtf.Rdir = "{gtf_read_dir}"

    readdir = "{read_dir}"
    readpattern = "{read_pattern}"
    useme.cols = "{useme_cols}"
    label.from.colname = "{label_from_colname}"
    samps = dir(path=readdir, pattern=readpattern)
    samp.labels = gsub(label.from.colname,'\\1', samps)

    annCol.names = 'group"
    annCol.lmBy = "{lmBy}"
    annCol.label = "{label_from_colname}"

    # read in STAR alignments
    STAR.data = read_STAR(useme.cols=useme.cols,label.from.colname=label.from.colname,annCol.label=annCol.label,annCol.names=annCol.names,annCol.normBy=NULL,annCol.lmBy=annCol.lmBy,readpattern=readpattern,unstranded.col = list(gene.counts = c(1:4),SJ.counts = c(1:3,7)))

    # filter out SJs with too many NAs
    if(any( names(STAR.data$LoM.raw)=="SJ.counts" & exists("SJ.counts.na.frac") )){
      STAR.data$SJ.counts.orig = STAR.data$LoM.raw$SJ.counts
      for(tag in names(STAR.data$LoM.raw$SJ.counts) ){
        STAR.data$LoM.raw$SJ.counts[[tag]] =
          STAR.data$LoM.raw$SJ.counts[[tag]][
            rowSums(is.na(STAR.data$LoM.raw$SJ.counts[[tag]]))
          <= (SJ.counts.na.frac*ncol(STAR.data$LoM.raw$SJ.counts[[tag]])), ]
      }
    }

    gtf = readENSgtf(filename=gtfFile)
    genes.gtf = gtf[feature==gtf.feature, mget(gtf.orig.col)]
    names(genes.gtf) = gtf.col
    setkeyv(genes.gtf,gtf.col[1])

    ##### parse of NCBI's EntrezID to Ensembl translation
    Entrez2Ensembl = fread(paste("zgrep",paste0("-E '^",taxID,"'"),gene2ENSfile))
    names(Entrez2Ensembl) = gene2ENS.col
    setkeyv(Entrez2Ensembl,gene2ENS.col[3])

    ##### add EntrezIDs to genes.gtf
    tmp = Entrez2Ensembl[,mget(gene2ENS.col[2:3])]; tmp=tmp[!duplicated(tmp),]
    genes.gtf = merge(genes.gtf,tmp,all.x=TRUE)
    rm(tmp)

    myreads = STAR.data$myreads
    LoM.norms = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(LoM.norms) = names(STAR.data$LoM.raw)
    for(tag in names(STAR.data$LoM.raw) ){
      if( length(STAR.data$LoM.raw[[tag]])>1 ){
        LoM.norms[[tag]] = normMatrix(tag=tag,raw.mat=STAR.data$LoM.raw[[tag]][[myreads]])
      } else {
        LoM.norms[[tag]] = normMatrix(tag=tag,raw.mat=STAR.data$LoM.raw[[tag]][[1]])
      }
    }


    ## custom: reannotate from metadata
    ## run if necessary metadata is in file and not also in FASTQ file names
    md.dt = fread({meta_file})
    # map annotation to read matrix
    if( md.orientation == "byCol" ){ # samples are one per column
      idx2 = match( colnames(STAR.data$LoM.raw[[1]][[myreads]]), names(md.dt) )
    } else { # samples are one per row
      idx2 = match( colnames(STAR.data$LoM.raw[[1]][[myreads]]), md.dt[,get(md.IDcol)] )
    }
    idx1 = which(!is.na(idx2)); idx2 = idx2[idx1]
    if( sum(idx1==1:ncol(STAR.data$LoM.raw[[1]][[myreads]]))==ncol(STAR.data$LoM.raw[[1]][[myreads]]) ){
      if( md.orientation == "byCol" ){ # samples are one per column
        annCol = NULL
        namecol = setdiff( 1:ncol(md.dt), idx2 )
        md.factors = as.character(md.dt[,namecol,with=F])
        for(k in 1:nrow(md.dt) ){
          annCol[[ md.dt[k,get(names(md.dt)[namecol])] ]][idx1] =
          as.vector( md.dt[ k, mget(names(md.dt)[idx2]) ] )
        }
      } else { # samples are one per row
        annCol = NULL
        namecol = setdiff( names(md.dt), md.IDcol )
        md.factors = as.character(namecol)
        for( k in setdiff(names(md.dt),md.IDcol) ){
          annCol[[ k ]][idx1] =
          as.vector( md.dt[ idx2, get(k) ] )
        }
      }
    } else {
      stop(paste("Some samples have no annotation in",md.file))
    }


    mytypes = names(LoM.norms)
    mynorms = names(LoM.norms[[1]])

    grpBy = annCol [[annCol.lmBy]]
    annCol.plotme = c(annCol.lmBy)
    clim.pct=0.96
    histbins=20
    for(i in 1:length(mytypes) ){
      for(j in 1:length(mynorms)){
        # set up matrices and config for plotting
        if( length(STAR.data$LoM.raw[[ mytypes[i] ]])>1 ){
          rawmat = STAR.data$LoM.raw[[ mytypes[i] ]][[myreads]]
        } else {
          rawmat = STAR.data$LoM.raw[[ mytypes[i] ]][[1]]
        }
        normmat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(rawmat)=samp.labels; colnames(normmat)=samp.labels
        plotdata = list(plotdir='.',plotbase=paste(mynorms[j],mytypes[i],sep='.'),plottitle=proj.title)
        rowmask = rowSums(rawmat>1,na.rm=T) > (ncol(rawmat)/length(unique(grpBy)) ) & rowSums(is.na(normmat)) < (ncol(normmat)/length(unique(grpBy)))
        if(sum(rowmask)>hclust.limit){ #hack! build optimization!
          rowmask = rowSums(rawmat>=quantile(rawmat,probs=hc.frac.cut,na.rm=T),na.rm=T) > (ncol(rawmat)/length(unique(grpBy)) ) & rowSums(is.na(normmat)) < (ncol(normmat)/length(unique(grpBy)) )
        }
        ans = summary.plots(rawmat=log2(rawmat +1), normmat=normmat, mynorm=mynorms[j], samp.labels=samp.labels, samp.classes=grpBy, plotdata=plotdata,plot2file=TRUE,histbins=histbins, colorspec=colors.rgb)
        ans = qc.clusters(rawmat=log2(rawmat[rowmask,] +1), normmat=normmat[rowmask,], attribs=annCol[annCol.plotme], oneclass=annCol.lmBy, colorspec=colors.rgb, plotdata=plotdata, plot2file=TRUE, clim.pct=clim.pct)
      }
    }

    regress_lsls = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(regress_lsls) = names(STAR.data$LoM.raw)
    contr_ls = list("Condition"=list(baseline="{baseline}",contr.FUN="contr.treatment"))
    # set baseline for regression in parameter(s) of interest
    # contr.treatment generates regression coefficients that are like (adjusted mean) ratios of other groups to the baseline group.
    # contr.sum generates coefficients that are like (adjusted mean) ratios to average all for all but the mandadory ommitted treatment group (because there is always one fewer independent pairwise comparison than there are pairs).

    lm_expr = "y ~ {lmBy}"
    rowmask_ls = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(rowmask_ls) = names(STAR.data$LoM.raw)
    for(i in 1:length(mytypes) ){
      for(j in which(!mynorms %in% do.not.regress) ){
        # set up data matrices
        if( length(STAR.data$LoM.raw[[ mytypes[i] ]])>1 ){
          rawmat = STAR.data$LoM.raw[[ mytypes[i] ]][[myreads]]
        } else {
          rawmat = STAR.data$LoM.raw[[ mytypes[i] ]][[1]]
        }
        tmp = unlist(lapply(contr_ls,function(x){x$baseline}))
        tmp=paste(names(tmp),tmp,sep=".")

        plotdata = list(plotdir='./',plotbase=paste(mynorms[j],mytypes[i],'vs',tmp,sep='.'),plottitle=proj.title)
        normmat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(rawmat)=samp.labels; colnames(normmat)=samp.labels

        # prepare rowmask for heatmap/MDS (remove non-expr or low expr>hclustlim)
        rowmask = rowSums(rawmat>1,na.rm=T) > (ncol(rawmat)/length(unique(grpBy)) ) & rowSums(is.na(normmat)) <= na.lim
        rowmask_ls[[ mytypes[i] ]][[ mynorms[j] ]] = rowmask # save for later

        # regression

        regress_lsls[[mytypes[i]]][[mynorms[j]]] = regressMatrix(normmat[rowmask,], expt.design=annCol[annCol.lmBy],
        lm_expression=lm_expr, contr_list = contr_ls, plot2file = TRUE, plotdata = plotdata)
      }
    }

    topn=500 # number with which to play
    for( i in 1:length(mytypes) ){
      for( j in which(!mynorms %in% do.not.regress) ){
        ans = regress_lsls[[mytypes[i]]][[mynorms[j]]]$q_list
        cat(mytypes[i],mynorms[j],"1-p0:",signif(unlist(lapply(2:length(ans),function(x){1-ans[[x]]$pi0})),2),"\n")
        cat(mytypes[i],mynorms[j],"qcut:",signif(unlist(lapply(2:length(ans),function(x){quantile(ans[[x]]$qvalues,probs=topn/length(ans[[x]]$qvalues))})),3),
        "returns ~",topn,"out of",length(ans[[2]]$qvalues),"\n")
    }}

    # Plots for each design factor (default) or for factors specified in facSel
    #   1) Histogram of p values that were included in the design
    #      Look for a peak on the left, and no peaks in the middle or on the right
    #   2) qvalue's default plots, with full qvalue range c(0,1) plotted
    #      Look for descent to pi0 in the top left tuning plot with good asymptote
    #      The slow/steep rise in q-values in remaining plots depends on resolving power of data


    lmBy = annCol.lmBy
    histbins=20
    for( i in 1:length(mytypes) ){
      for( j in which(!mynorms %in% do.not.regress) ){
        normmat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(normmat)=samp.labels
        reg_ls = regress_lsls[[mytypes[i]]][[mynorms[j]]]
        mymask= rowmask_ls[[mytypes[i]]][[mynorms[j]]]
        # pull out regression design and set up plot config
        tmp = unlist(lapply(contr_ls,function(x){x$baseline}))
        tmp=paste(names(tmp),tmp,sep=".")
        plotdata = list(plotdir='./',plotbase=paste(mynorms[j],mytypes[i],tmp,sep='.'),plottitle=proj.title)
        for(fac in names(reg_ls$q_list)[grep(lmBy,names(reg_ls$q_list))] ){
          pdata = plotdata
          pdata$plotbase = paste(plotdata$plotbase,make.names(fac),sep='.')
          ans = qcQvalues(norm_x=normmat[mymask,], pvalue_v=reg_ls$p_mat[,fac], obj_qvalue=reg_ls$q_list[[fac]], attribs=annCol[lmBy],
          oneclass=lmBy, plotdata=pdata, colorspec=colors.rgb, histbins=histbins, plot2file=TRUE)
        }
      }
    }

    # Build of ratios to baselines for desired design factors
    # Also build masks selecting genes based on q-values per factor
    #  and follow-up considerations such as expression level and fold change
    # Loop over q-value cuts to assist with final cut selection.
    # Plot heatmaps and MDS plots based on selected genes and designed ratios


    ngene_v = c(200,500,1000) # q-value cuts by number; can also cut by q-value
    ratioby_ls = list("Condition"=contr_ls$Condition$baseline)
    ratio_fold = 1.3
    intensity_fold = 2

    cut_ls = list(q_combine="OR", rcut_fold=ratio_fold, icut_fold=intensity_fold)
    # settings for heatmaps
    annCol.plotme = annCol.lmBy # heatmap tracks
    clustrowmin = 10 # min data rows for heatmap and MDS plots
    # save ratios and selections
    select_lsmk = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(select_lsmk) = names(STAR.data$LoM.raw)
    ratio_lsmat = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(ratio_lsmat) = names(STAR.data$LoM.raw)
    for( i in 1:length(mytypes) ){
      for( j in which(!mynorms %in% do.not.regress) ){
        normmat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(normmat)=samp.labels
        if( grepl('SJ',mytypes[i]) ){ #not the most robust way to find SJs
          # map gene IDs back to data matrix using mapping built earlier
          # this enables include_ID to select rows
          idx2 = match( rownames(STAR.data$LoM.raw[[ mytypes[i] ]][[1]]), mySJ_dt[,get(pos.col)] )
          idx1 = which(!is.na(idx2)); idx2 = idx2[idx1]
          rownames(normmat)[idx1] = mySJ_dt[idx2,get(gtf.col[1])]
        }
        mymask= rowmask_ls[[mytypes[i]]][[mynorms[j]]]
        reg_ls = regress_lsls[[mytypes[i]]][[mynorms[j]]]$q_list
        reg_ls = reg_ls[!grepl('Intercept',names(reg_ls))]
        plotdata = list(plotdir='./',plotbase=paste(mynorms[j],sub('counts','ratios',mytypes[i]),'minratio',paste0(ratio_fold,'x'),'minexpr',round(min(normmat,na.rm=T)+log2(intensity_fold),1),sep='.'),plottitle=proj.title)
        for( ngenes in ngene_v ) {
          # calculate and plot ratios
          cut_ls$qcut = ngenes
          pdata = plotdata
          pdata$plotbase = paste(plotdata$plotbase,'ratio.vs',ratioby_ls$strain,ngenes,sep='.')
          # this function makes the ratios and cuts
          ans = designRatios(normmat[mymask,], q_list=reg_ls, attribs=annCol[annCol.plotme], ratioby_ls=ratioby_ls, cut_ls=cut_ls)
          # this function creates the heatmap and MDS plot
          if( sum(ans$rowmask)>clustrowmin ){
            ans2= plotRatios( ratiomat=ans$ratiomat, attribs=annCol[annCol.plotme], oneclass=lmBy, plotdata=pdata, colorspec=colors.rgb, rowmask=ans$rowmask, plot2file=TRUE)
          }
          # save selections
          select_lsmk[[mytypes[i]]][[mynorms[j]]][paste(fac,ngenes,sep=":")] = ans['rowmask']
        }
        # save ratiomat (not dependent on ngene cut) after ngene loop
        ratio_lsmat[[mytypes[i]]][[mynorms[j]]] = ans$ratiomat
      }
    }

    dir.create(file.path(getwd(),'PairwiseScatter'),showWarnings=FALSE)
    for( i in 1:length(mytypes) ){
      for( j in 1:length(mynorms) ){
        scatter_mat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(normmat)=samp.labels
        plotdata = list(plotdir='./PairwiseScatter',plotbase=paste(mynorms[j],mytypes[i],'Scatter',lmBy,sep=':'),plottitle=proj.title)
        scatterplot(scatter_mat, attribs=annCol[[lmBy]],plotdata=plotdata,plot2file = TRUE)
      }
    }


    ratio_mat = ratio_lsmat[[{path_type}]][[{path_norms}]]
    reg_ls = regress_lsls[[{path_type}]][[{path_norms}]]
    ID = rownames(ratio_mat)

    ensembl=useMart("ensembl")
    listDatasets(ensembl)
    mouse = useMart("ensembl", dataset = "{mart_dataset}")
    human = useMart("ensembl", dataset = "hsapiens_gene_ensembl")

    hsa_entrezID = getLDS(attributes = "ensembl_gene_id",mart = mouse,  attributesL = "entrezgene", martL = human)
    Entrez.ID = character(length(ID))
    Entrez.ID[] = NA
    idx2 = match(ID,hsa_entrezID$Gene.ID)
    idx1 = which(!is.na(idx2)); idx2= idx2[idx1]
    Entrez.ID[idx1] = hsa_entrezID$EntrezGene.ID[idx2];
    Entrez.ID = as.numeric(Entrez.ID)

    ###Human gene symbols
    hsa_symbol = getLDS(attributes = "ensembl_gene_id",mart = mouse,  attributesL = "hgnc_symbol", martL = human)
    Anno.Symbol = character(length(ID))
    Anno.Symbol[] = NA
    idx2 = match(ID,hsa_symbol$Gene.ID)
    idx1 = which(!is.na(idx2)); idx2= idx2[idx1]
    Anno.Symbol[idx1] = unlist(hsa_symbol$HGNC.symbol[idx2]);
    # assemble IDs for annotation
    backgroundset = as.data.table(cbind(ID, Entrez.ID, Anno.Symbol))
    # assemble signatures


    reg_ls = regress_lsls$gene.counts${path_norms}
    fac = names(reg_ls$q_list)[grep(lmBy,names(reg_ls$q_list))]
    rowmask = select_lsmk[[{path_type}]][[{path_norms}]][[paste(fac,ngenes,sep=":")]]
    sig_gmt = NULL
    betas = reg_ls$b_mat[,fac]
    for (k in names(masks)){
        sig_gmt = NULL
        rowmask = select_lsmk[[mytypes[i]]][[mynorms[j]]][[k]]
        sig_gmt[['all']] =  ID[rowmask]
        fileSettings = list(directory='{read_dir}',baseFilename=paste(k,'with_Uni',sep='_')
        path_ls = ags.wrapper(setlist=sig_gmt, backgroundset, include.identifiers=TRUE, anno.uni=TRUE,
                              fileSettings = fileSettings,
                              functiondir = "{code_dir}/AssociationFunctions/PathwayAnalysis.R",
                              resourcedir = "/home/exacloud/lustre1/BioCoders/DataResources/AnnotationSources/old_association_resources",
                              return.OM=TRUE, ecut=0.05, ocut=5)
        fileSettings = list(directory='{read_dir}',baseFilename=paste(k,'with_out_Uni',sep='_')
        pathi_ls = ags.wrapper(setlist=sig_gmt, backgroundset, include.identifiers=TRUE, anno.uni=FALSE,
                       fileSettings = fileSettings,
                       functiondir = "{code_dir}/AssociationFunctions/PathwayAnalysis.R",
                       resourcedir = "/home/exacloud/lustre1/BioCoders/DataResources/AnnotationSources/old_association_resources",
                       return.OM=TRUE, ecut=0.05, ocut=5)
    }
    gtf.Rdir = "{gtf_read_dir}"
    out_norm_mat = LoM.norms$gene.counts$loess[rowmask_ls${path_type}${path_norms},]
    out_table = outputTable(normmat= out_norm_mat, gtf.file = "{gtfFile}",ratiomat = ratio_lsmat${path_type}${path_norms}, q_list=reg_ls$q_list)
    write.csv(out_table, row.names = FALSE, file=paste({project_title},{path_norms},'Normed_with_Ratio_and_Abundance.csv',sep='_'),quote=FALSE)
    """
1/44:     code_context = {"code_dir":code_dir, "meta_file":meta_file, "taxID":taxID, "gtfFile":gtfFile, "gtf_feature":gtf_feature, "project_title":project_title, "gtf_read_dir":gtf_read_dir,"read_dir":read_dir, "read_pattern":read_pattern, "useme_cols":useme_cols, "lmBy":lmBy, "baseline":baseline, "path_type":path_type, "path_norms":path_norms, "mart_dataset":mart_dataset, "label_from_colname":label_from_colname}
1/45: mart_dataset = "mmusculus_gene_ensembl"
1/46: lmBy = "Identifier"
1/47: gtf_feature = "gene"
1/48: read_pattern="^RNA1"
1/49: useme_cols="RNA1"
1/50: label_from_colname = "^.*?RNA[^_]*?_([^_]+)_.*"
1/51: path_type = "gene.counts"
1/52: path_norms = "loess"
1/53: code_context = {"code_dir":code_dir, "meta_file":meta_file, "taxID":taxID, "gtfFile":gtfFile, "gtf_feature":gtf_feature, "project_title":project_title, "gtf_read_dir":gtf_read_dir,"read_dir":read_dir, "read_pattern":read_pattern, "useme_cols":useme_cols, "lmBy":lmBy, "baseline":baseline, "path_type":path_type, "path_norms":path_norms, "mart_dataset":mart_dataset, "label_from_colname":label_from_colname}
1/54: out_f.write(code.format(**code_context))
1/55: code_context
1/56: code
1/57: print(code)
1/58:
    code ="""
    require(data.table)
    require(NMF)
    require(affy)
    require(limma)
    require(AnnotationDbi)
    require(biomaRt)
    source("{code_dir}/AbundanceFunctions/BiasReduce.R")
    source("{code_dir}/AbundanceFunctions/ExtractTransformLoad.R")
    source("{code_dir}/AbundanceFunctions/DifferentialAnalysis.R")
    source("{code_dir}/AbundanceFunctions/NonVisualOutput.R")
    source("{code_dir}/GenomicsFunctions/ReadAndParse.R")
    source("{code_dir}/AssociationFunctions/gs.wrapper.R")
    source("{code_dir}/AssociationFunctions/PathwayAnalysis.R")
    source("{code_dir}/BcorePlotting/SummaryPlots.R")
    source("{code_dir}/BcorePlotting/MultipleTestingCorrection.R")
    source("{code_dir}/BcorePlotting/ClusteringPlots.R")


    setwd({read_dir})

    # constants
    # max data rows for hclust
    hclust.limit = 2^16

    # quantile of data distribution reqd in one group's worth of data if too many rows for hclust()
    hc.frac.cut = 0.75;
    SJ.counts.na.frac = 0.25;
    # max fraction of samples not having detected a splice junction for the splice
    # junction to be retained in raw data


    # regression parameters
    na.lim = 0 # max NAs per row tolerated by lm() at least in some cases
    do.not.regress = "alograw" # control norm not to be used for regression stats
    # plotting colors
    colors.rgb = c(rgb(0,0,0),rgb(0.1,0.1,1),rgb(0,.7,.7),rgb(0,.7,0),rgb(.7,1,0),rgb(.7,0,.7))
    md.file = {meta_file}
    md.orientation = "byRow" # sampleIDs are in @ row. alt:byCol (IDs in @ col)
    md.IDcol = "SampleID" # reqd if md.orientation is byRow; byCol==headers are IDs

    # gene annotation
    taxID = {taxID}
    gene2ENSfile = "/home/exacloud/lustre1/BioCoders/DataResources/AnnotationSources/ncbi/gene2ensembl.gz"
    gene2ENS.col = c("taxID","EntrezID","Gene","RefSeqTranscript","EnsemblTranscript","RefSeqProtein","EnsemblProtein")
    gtfFile = "{gtfFile}"
    gtf.feature = "{gtf_feature}"
    gtf.orig.col = c("gene_id","gene_name","gene_biotype")
    gtf.col = c("Gene","Symbol","biotype")
    proj.title = "{project_title}"
    gtf.Rdir = "{gtf_read_dir}"

    readdir = "{read_dir}"
    readpattern = "{read_pattern}"
    useme.cols = "{useme_cols}"
    label.from.colname = "{label_from_colname}"
    samps = dir(path=readdir, pattern=readpattern)
    samp.labels = gsub(label.from.colname,'\\1', samps)

    annCol.names = 'group"
    annCol.lmBy = "{lmBy}"
    annCol.label = "{label_from_colname}"

    # read in STAR alignments
    STAR.data = read_STAR(useme.cols=useme.cols,label.from.colname=label.from.colname,annCol.label=annCol.label,annCol.names=annCol.names,annCol.normBy=NULL,annCol.lmBy=annCol.lmBy,readpattern=readpattern,unstranded.col = list(gene.counts = c(1:4),SJ.counts = c(1:3,7)))

    # filter out SJs with too many NAs
    if(any( names(STAR.data$LoM.raw)=="SJ.counts" & exists("SJ.counts.na.frac") )){
      STAR.data$SJ.counts.orig = STAR.data$LoM.raw$SJ.counts
      for(tag in names(STAR.data$LoM.raw$SJ.counts) ){
        STAR.data$LoM.raw$SJ.counts[[tag]] = STAR.data$LoM.raw$SJ.counts[[tag]][rowSums(is.na(STAR.data$LoM.raw$SJ.counts[[tag]]))<= (SJ.counts.na.frac*ncol(STAR.data$LoM.raw$SJ.counts[[tag]])), ]
      }
    }

    gtf = readENSgtf(filename=gtfFile)
    genes.gtf = gtf[feature==gtf.feature, mget(gtf.orig.col)]
    names(genes.gtf) = gtf.col
    setkeyv(genes.gtf,gtf.col[1])

    ##### parse of NCBI's EntrezID to Ensembl translation
    Entrez2Ensembl = fread(paste("zgrep",paste0("-E '^",taxID,"'"),gene2ENSfile))
    names(Entrez2Ensembl) = gene2ENS.col
    setkeyv(Entrez2Ensembl,gene2ENS.col[3])

    ##### add EntrezIDs to genes.gtf
    tmp = Entrez2Ensembl[,mget(gene2ENS.col[2:3])]; tmp=tmp[!duplicated(tmp),]
    genes.gtf = merge(genes.gtf,tmp,all.x=TRUE)
    rm(tmp)

    myreads = STAR.data$myreads
    LoM.norms = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(LoM.norms) = names(STAR.data$LoM.raw)
    for(tag in names(STAR.data$LoM.raw) ){
      if( length(STAR.data$LoM.raw[[tag]])>1 ){
        LoM.norms[[tag]] = normMatrix(tag=tag,raw.mat=STAR.data$LoM.raw[[tag]][[myreads]])
      } else {
        LoM.norms[[tag]] = normMatrix(tag=tag,raw.mat=STAR.data$LoM.raw[[tag]][[1]])
      }
    }


    ## custom: reannotate from metadata
    ## run if necessary metadata is in file and not also in FASTQ file names
    md.dt = fread({meta_file})
    # map annotation to read matrix
    if( md.orientation == "byCol" ){ # samples are one per column
      idx2 = match( colnames(STAR.data$LoM.raw[[1]][[myreads]]), names(md.dt) )
    } else { # samples are one per row
      idx2 = match( colnames(STAR.data$LoM.raw[[1]][[myreads]]), md.dt[,get(md.IDcol)] )
    }
    idx1 = which(!is.na(idx2)); idx2 = idx2[idx1]
    if( sum(idx1==1:ncol(STAR.data$LoM.raw[[1]][[myreads]]))==ncol(STAR.data$LoM.raw[[1]][[myreads]]) ){
      if( md.orientation == "byCol" ){ # samples are one per column
        annCol = NULL
        namecol = setdiff( 1:ncol(md.dt), idx2 )
        md.factors = as.character(md.dt[,namecol,with=F])
        for(k in 1:nrow(md.dt) ){
          annCol[[ md.dt[k,get(names(md.dt)[namecol])] ]][idx1] =
          as.vector( md.dt[ k, mget(names(md.dt)[idx2]) ] )
        }
      } else { # samples are one per row
        annCol = NULL
        namecol = setdiff( names(md.dt), md.IDcol )
        md.factors = as.character(namecol)
        for( k in setdiff(names(md.dt),md.IDcol) ){
          annCol[[ k ]][idx1] =
          as.vector( md.dt[ idx2, get(k) ] )
        }
      }
    } else {
      stop(paste("Some samples have no annotation in",md.file))
    }


    mytypes = names(LoM.norms)
    mynorms = names(LoM.norms[[1]])

    grpBy = annCol [[annCol.lmBy]]
    annCol.plotme = c(annCol.lmBy)
    clim.pct=0.96
    histbins=20
    for(i in 1:length(mytypes) ){
      for(j in 1:length(mynorms)){
        # set up matrices and config for plotting
        if( length(STAR.data$LoM.raw[[ mytypes[i] ]])>1 ){
          rawmat = STAR.data$LoM.raw[[ mytypes[i] ]][[myreads]]
        } else {
          rawmat = STAR.data$LoM.raw[[ mytypes[i] ]][[1]]
        }
        normmat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(rawmat)=samp.labels; colnames(normmat)=samp.labels
        plotdata = list(plotdir='.',plotbase=paste(mynorms[j],mytypes[i],sep='.'),plottitle=proj.title)
        rowmask = rowSums(rawmat>1,na.rm=T) > (ncol(rawmat)/length(unique(grpBy)) ) & rowSums(is.na(normmat)) < (ncol(normmat)/length(unique(grpBy)))
        if(sum(rowmask)>hclust.limit){ #hack! build optimization!
          rowmask = rowSums(rawmat>=quantile(rawmat,probs=hc.frac.cut,na.rm=T),na.rm=T) > (ncol(rawmat)/length(unique(grpBy)) ) & rowSums(is.na(normmat)) < (ncol(normmat)/length(unique(grpBy)) )
        }
        ans = summary.plots(rawmat=log2(rawmat +1), normmat=normmat, mynorm=mynorms[j], samp.labels=samp.labels, samp.classes=grpBy, plotdata=plotdata,plot2file=TRUE,histbins=histbins, colorspec=colors.rgb)
        ans = qc.clusters(rawmat=log2(rawmat[rowmask,] +1), normmat=normmat[rowmask,], attribs=annCol[annCol.plotme], oneclass=annCol.lmBy, colorspec=colors.rgb, plotdata=plotdata, plot2file=TRUE, clim.pct=clim.pct)
      }
    }

    regress_lsls = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(regress_lsls) = names(STAR.data$LoM.raw)
    contr_ls = list("Condition"=list(baseline="{baseline}",contr.FUN="contr.treatment"))
    # set baseline for regression in parameter(s) of interest
    # contr.treatment generates regression coefficients that are like (adjusted mean) ratios of other groups to the baseline group.
    # contr.sum generates coefficients that are like (adjusted mean) ratios to average all for all but the mandadory ommitted treatment group (because there is always one fewer independent pairwise comparison than there are pairs).

    lm_expr = "y ~ {lmBy}"
    rowmask_ls = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(rowmask_ls) = names(STAR.data$LoM.raw)
    for(i in 1:length(mytypes) ){
      for(j in which(!mynorms %in% do.not.regress) ){
        # set up data matrices
        if( length(STAR.data$LoM.raw[[ mytypes[i] ]])>1 ){
          rawmat = STAR.data$LoM.raw[[ mytypes[i] ]][[myreads]]
        } else {
          rawmat = STAR.data$LoM.raw[[ mytypes[i] ]][[1]]
        }
        tmp = unlist(lapply(contr_ls,function(x){x$baseline}))
        tmp=paste(names(tmp),tmp,sep=".")

        plotdata = list(plotdir='./',plotbase=paste(mynorms[j],mytypes[i],'vs',tmp,sep='.'),plottitle=proj.title)
        normmat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(rawmat)=samp.labels; colnames(normmat)=samp.labels

        # prepare rowmask for heatmap/MDS (remove non-expr or low expr>hclustlim)
        rowmask = rowSums(rawmat>1,na.rm=T) > (ncol(rawmat)/length(unique(grpBy)) ) & rowSums(is.na(normmat)) <= na.lim
        rowmask_ls[[ mytypes[i] ]][[ mynorms[j] ]] = rowmask # save for later

        # regression

        regress_lsls[[mytypes[i]]][[mynorms[j]]] = regressMatrix(normmat[rowmask,], expt.design=annCol[annCol.lmBy],
        lm_expression=lm_expr, contr_list = contr_ls, plot2file = TRUE, plotdata = plotdata)
      }
    }

    topn=500 # number with which to play
    for( i in 1:length(mytypes) ){
      for( j in which(!mynorms %in% do.not.regress) ){
        ans = regress_lsls[[mytypes[i]]][[mynorms[j]]]$q_list
        cat(mytypes[i],mynorms[j],"1-p0:",signif(unlist(lapply(2:length(ans),function(x){1-ans[[x]]$pi0})),2),"\n")
        cat(mytypes[i],mynorms[j],"qcut:",signif(unlist(lapply(2:length(ans),function(x){quantile(ans[[x]]$qvalues,probs=topn/length(ans[[x]]$qvalues))})),3),
        "returns ~",topn,"out of",length(ans[[2]]$qvalues),"\n")
    }}

    # Plots for each design factor (default) or for factors specified in facSel
    #   1) Histogram of p values that were included in the design
    #      Look for a peak on the left, and no peaks in the middle or on the right
    #   2) qvalue's default plots, with full qvalue range c(0,1) plotted
    #      Look for descent to pi0 in the top left tuning plot with good asymptote
    #      The slow/steep rise in q-values in remaining plots depends on resolving power of data


    lmBy = annCol.lmBy
    histbins=20
    for( i in 1:length(mytypes) ){
      for( j in which(!mynorms %in% do.not.regress) ){
        normmat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(normmat)=samp.labels
        reg_ls = regress_lsls[[mytypes[i]]][[mynorms[j]]]
        mymask= rowmask_ls[[mytypes[i]]][[mynorms[j]]]
        # pull out regression design and set up plot config
        tmp = unlist(lapply(contr_ls,function(x){x$baseline}))
        tmp=paste(names(tmp),tmp,sep=".")
        plotdata = list(plotdir='./',plotbase=paste(mynorms[j],mytypes[i],tmp,sep='.'),plottitle=proj.title)
        for(fac in names(reg_ls$q_list)[grep(lmBy,names(reg_ls$q_list))] ){
          pdata = plotdata
          pdata$plotbase = paste(plotdata$plotbase,make.names(fac),sep='.')
          ans = qcQvalues(norm_x=normmat[mymask,], pvalue_v=reg_ls$p_mat[,fac], obj_qvalue=reg_ls$q_list[[fac]], attribs=annCol[lmBy],
          oneclass=lmBy, plotdata=pdata, colorspec=colors.rgb, histbins=histbins, plot2file=TRUE)
        }
      }
    }

    # Build of ratios to baselines for desired design factors
    # Also build masks selecting genes based on q-values per factor
    #  and follow-up considerations such as expression level and fold change
    # Loop over q-value cuts to assist with final cut selection.
    # Plot heatmaps and MDS plots based on selected genes and designed ratios


    ngene_v = c(200,500,1000) # q-value cuts by number; can also cut by q-value
    ratioby_ls = list("Condition"=contr_ls$Condition$baseline)
    ratio_fold = 1.3
    intensity_fold = 2

    cut_ls = list(q_combine="OR", rcut_fold=ratio_fold, icut_fold=intensity_fold)
    # settings for heatmaps
    annCol.plotme = annCol.lmBy # heatmap tracks
    clustrowmin = 10 # min data rows for heatmap and MDS plots
    # save ratios and selections
    select_lsmk = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(select_lsmk) = names(STAR.data$LoM.raw)
    ratio_lsmat = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(ratio_lsmat) = names(STAR.data$LoM.raw)
    for( i in 1:length(mytypes) ){
      for( j in which(!mynorms %in% do.not.regress) ){
        normmat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(normmat)=samp.labels
        if( grepl('SJ',mytypes[i]) ){ #not the most robust way to find SJs
          # map gene IDs back to data matrix using mapping built earlier
          # this enables include_ID to select rows
          idx2 = match( rownames(STAR.data$LoM.raw[[ mytypes[i] ]][[1]]), mySJ_dt[,get(pos.col)] )
          idx1 = which(!is.na(idx2)); idx2 = idx2[idx1]
          rownames(normmat)[idx1] = mySJ_dt[idx2,get(gtf.col[1])]
        }
        mymask= rowmask_ls[[mytypes[i]]][[mynorms[j]]]
        reg_ls = regress_lsls[[mytypes[i]]][[mynorms[j]]]$q_list
        reg_ls = reg_ls[!grepl('Intercept',names(reg_ls))]
        plotdata = list(plotdir='./',plotbase=paste(mynorms[j],sub('counts','ratios',mytypes[i]),'minratio',paste0(ratio_fold,'x'),'minexpr',round(min(normmat,na.rm=T)+log2(intensity_fold),1),sep='.'),plottitle=proj.title)
        for( ngenes in ngene_v ) {
          # calculate and plot ratios
          cut_ls$qcut = ngenes
          pdata = plotdata
          pdata$plotbase = paste(plotdata$plotbase,'ratio.vs',ratioby_ls$strain,ngenes,sep='.')
          # this function makes the ratios and cuts
          ans = designRatios(normmat[mymask,], q_list=reg_ls, attribs=annCol[annCol.plotme], ratioby_ls=ratioby_ls, cut_ls=cut_ls)
          # this function creates the heatmap and MDS plot
          if( sum(ans$rowmask)>clustrowmin ){
            ans2= plotRatios( ratiomat=ans$ratiomat, attribs=annCol[annCol.plotme], oneclass=lmBy, plotdata=pdata, colorspec=colors.rgb, rowmask=ans$rowmask, plot2file=TRUE)
          }
          # save selections
          select_lsmk[[mytypes[i]]][[mynorms[j]]][paste(fac,ngenes,sep=":")] = ans['rowmask']
        }
        # save ratiomat (not dependent on ngene cut) after ngene loop
        ratio_lsmat[[mytypes[i]]][[mynorms[j]]] = ans$ratiomat
      }
    }

    dir.create(file.path(getwd(),'PairwiseScatter'),showWarnings=FALSE)
    for( i in 1:length(mytypes) ){
      for( j in 1:length(mynorms) ){
        scatter_mat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(normmat)=samp.labels
        plotdata = list(plotdir='./PairwiseScatter',plotbase=paste(mynorms[j],mytypes[i],'Scatter',lmBy,sep=':'),plottitle=proj.title)
        scatterplot(scatter_mat, attribs=annCol[[lmBy]],plotdata=plotdata,plot2file = TRUE)
      }
    }


    ratio_mat = ratio_lsmat[[{path_type}]][[{path_norms}]]
    reg_ls = regress_lsls[[{path_type}]][[{path_norms}]]
    ID = rownames(ratio_mat)

    ensembl=useMart("ensembl")
    listDatasets(ensembl)
    mouse = useMart("ensembl", dataset = "{mart_dataset}")
    human = useMart("ensembl", dataset = "hsapiens_gene_ensembl")

    hsa_entrezID = getLDS(attributes = "ensembl_gene_id",mart = mouse,  attributesL = "entrezgene", martL = human)
    Entrez.ID = character(length(ID))
    Entrez.ID[] = NA
    idx2 = match(ID,hsa_entrezID$Gene.ID)
    idx1 = which(!is.na(idx2)); idx2= idx2[idx1]
    Entrez.ID[idx1] = hsa_entrezID$EntrezGene.ID[idx2];
    Entrez.ID = as.numeric(Entrez.ID)

    ###Human gene symbols
    hsa_symbol = getLDS(attributes = "ensembl_gene_id",mart = mouse,  attributesL = "hgnc_symbol", martL = human)
    Anno.Symbol = character(length(ID))
    Anno.Symbol[] = NA
    idx2 = match(ID,hsa_symbol$Gene.ID)
    idx1 = which(!is.na(idx2)); idx2= idx2[idx1]
    Anno.Symbol[idx1] = unlist(hsa_symbol$HGNC.symbol[idx2]);
    # assemble IDs for annotation
    backgroundset = as.data.table(cbind(ID, Entrez.ID, Anno.Symbol))
    # assemble signatures


    reg_ls = regress_lsls$gene.counts${path_norms}
    fac = names(reg_ls$q_list)[grep(lmBy,names(reg_ls$q_list))]
    rowmask = select_lsmk[[{path_type}]][[{path_norms}]][[paste(fac,ngenes,sep=":")]]
    sig_gmt = NULL
    betas = reg_ls$b_mat[,fac]
    for (k in names(masks)){
        sig_gmt = NULL
        rowmask = select_lsmk[[mytypes[i]]][[mynorms[j]]][[k]]
        sig_gmt[['all']] =  ID[rowmask]
        fileSettings = list(directory='{read_dir}',baseFilename=paste(k,'with_Uni',sep='_')
        path_ls = ags.wrapper(setlist=sig_gmt, backgroundset, include.identifiers=TRUE, anno.uni=TRUE,
                              fileSettings = fileSettings,
                              functiondir = "{code_dir}/AssociationFunctions/PathwayAnalysis.R",
                              resourcedir = "/home/exacloud/lustre1/BioCoders/DataResources/AnnotationSources/old_association_resources",
                              return.OM=TRUE, ecut=0.05, ocut=5)
        fileSettings = list(directory='{read_dir}',baseFilename=paste(k,'with_out_Uni',sep='_')
        pathi_ls = ags.wrapper(setlist=sig_gmt, backgroundset, include.identifiers=TRUE, anno.uni=FALSE,
                       fileSettings = fileSettings,
                       functiondir = "{code_dir}/AssociationFunctions/PathwayAnalysis.R",
                       resourcedir = "/home/exacloud/lustre1/BioCoders/DataResources/AnnotationSources/old_association_resources",
                       return.OM=TRUE, ecut=0.05, ocut=5)
    }
    gtf.Rdir = "{gtf_read_dir}"
    out_norm_mat = LoM.norms$gene.counts$loess[rowmask_ls${path_type}${path_norms},]
    out_table = outputTable(normmat= out_norm_mat, gtf.file = "{gtfFile}",ratiomat = ratio_lsmat${path_type}${path_norms}, q_list=reg_ls$q_list)
    write.csv(out_table, row.names = FALSE, file=paste({project_title},{path_norms},'Normed_with_Ratio_and_Abundance.csv',sep='_'),quote=FALSE)
    """
1/59: out_f.write(code.format(**code_context))
1/60: code
1/61: print code
 2/1: %paste
 2/2:
    code ="""
    require(data.table)
    require(NMF)
    require(affy)
    require(limma)
    require(AnnotationDbi)
    require(biomaRt)
    source("{code_dir}/AbundanceFunctions/BiasReduce.R")
    source("{code_dir}/AbundanceFunctions/ExtractTransformLoad.R")
    source("{code_dir}/AbundanceFunctions/DifferentialAnalysis.R")
    source("{code_dir}/AbundanceFunctions/NonVisualOutput.R")
    source("{code_dir}/GenomicsFunctions/ReadAndParse.R")
    source("{code_dir}/AssociationFunctions/gs.wrapper.R")
    source("{code_dir}/AssociationFunctions/PathwayAnalysis.R")
    source("{code_dir}/BcorePlotting/SummaryPlots.R")
    source("{code_dir}/BcorePlotting/MultipleTestingCorrection.R")
    source("{code_dir}/BcorePlotting/ClusteringPlots.R")


    setwd({read_dir})

    # constants
    # max data rows for hclust
    hclust.limit = 2^16

    # quantile of data distribution reqd in one group's worth of data if too many rows for hclust()
    hc.frac.cut = 0.75;
    SJ.counts.na.frac = 0.25;
    # max fraction of samples not having detected a splice junction for the splice
    # junction to be retained in raw data


    # regression parameters
    na.lim = 0 # max NAs per row tolerated by lm() at least in some cases
    do.not.regress = "alograw" # control norm not to be used for regression stats
    # plotting colors
    colors.rgb = c(rgb(0,0,0),rgb(0.1,0.1,1),rgb(0,.7,.7),rgb(0,.7,0),rgb(.7,1,0),rgb(.7,0,.7))
    md.file = {meta_file}
    md.orientation = "byRow" # sampleIDs are in @ row. alt:byCol (IDs in @ col)
    md.IDcol = "SampleID" # reqd if md.orientation is byRow; byCol==headers are IDs

    # gene annotation
    taxID = {taxID}
    gene2ENSfile = "/home/exacloud/lustre1/BioCoders/DataResources/AnnotationSources/ncbi/gene2ensembl.gz"
    gene2ENS.col = c("taxID","EntrezID","Gene","RefSeqTranscript","EnsemblTranscript","RefSeqProtein","EnsemblProtein")
    gtfFile = "{gtfFile}"
    gtf.feature = "{gtf_feature}"
    gtf.orig.col = c("gene_id","gene_name","gene_biotype")
    gtf.col = c("Gene","Symbol","biotype")
    proj.title = "{project_title}"
    gtf.Rdir = "{gtf_read_dir}"

    readdir = "{read_dir}"
    readpattern = "{read_pattern}"
    useme.cols = "{useme_cols}"
    label.from.colname = "{label_from_colname}"
    samps = dir(path=readdir, pattern=readpattern)
    samp.labels = gsub(label.from.colname,'\\1', samps)

    annCol.names = 'group"
    annCol.lmBy = "{lmBy}"
    annCol.label = "{label_from_colname}"

    # read in STAR alignments
    STAR.data = read_STAR(useme.cols=useme.cols,label.from.colname=label.from.colname,annCol.label=annCol.label,annCol.names=annCol.names,annCol.normBy=NULL,annCol.lmBy=annCol.lmBy,readpattern=readpattern,unstranded.col = list(gene.counts = c(1:4),SJ.counts = c(1:3,7)))

    # filter out SJs with too many NAs
    if(any( names(STAR.data$LoM.raw)=="SJ.counts" & exists("SJ.counts.na.frac") )){
      STAR.data$SJ.counts.orig = STAR.data$LoM.raw$SJ.counts
      for(tag in names(STAR.data$LoM.raw$SJ.counts) ){
        STAR.data$LoM.raw$SJ.counts[[tag]] = STAR.data$LoM.raw$SJ.counts[[tag]][rowSums(is.na(STAR.data$LoM.raw$SJ.counts[[tag]]))<= (SJ.counts.na.frac*ncol(STAR.data$LoM.raw$SJ.counts[[tag]])), ]
      }
    }

    gtf = readENSgtf(filename=gtfFile)
    genes.gtf = gtf[feature==gtf.feature, mget(gtf.orig.col)]
    names(genes.gtf) = gtf.col
    setkeyv(genes.gtf,gtf.col[1])

    ##### parse of NCBI's EntrezID to Ensembl translation
    Entrez2Ensembl = fread(paste("zgrep",paste0("-E '^",taxID,"'"),gene2ENSfile))
    names(Entrez2Ensembl) = gene2ENS.col
    setkeyv(Entrez2Ensembl,gene2ENS.col[3])

    ##### add EntrezIDs to genes.gtf
    tmp = Entrez2Ensembl[,mget(gene2ENS.col[2:3])]; tmp=tmp[!duplicated(tmp),]
    genes.gtf = merge(genes.gtf,tmp,all.x=TRUE)
    rm(tmp)

    myreads = STAR.data$myreads
    LoM.norms = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(LoM.norms) = names(STAR.data$LoM.raw)
    for(tag in names(STAR.data$LoM.raw) ){
      if( length(STAR.data$LoM.raw[[tag]])>1 ){
        LoM.norms[[tag]] = normMatrix(tag=tag,raw.mat=STAR.data$LoM.raw[[tag]][[myreads]])
      } else {
        LoM.norms[[tag]] = normMatrix(tag=tag,raw.mat=STAR.data$LoM.raw[[tag]][[1]])
      }
    }


    ## custom: reannotate from metadata
    ## run if necessary metadata is in file and not also in FASTQ file names
    md.dt = fread({meta_file})
    # map annotation to read matrix
    if( md.orientation == "byCol" ){ # samples are one per column
      idx2 = match( colnames(STAR.data$LoM.raw[[1]][[myreads]]), names(md.dt) )
    } else { # samples are one per row
      idx2 = match( colnames(STAR.data$LoM.raw[[1]][[myreads]]), md.dt[,get(md.IDcol)] )
    }
    idx1 = which(!is.na(idx2)); idx2 = idx2[idx1]
    if( sum(idx1==1:ncol(STAR.data$LoM.raw[[1]][[myreads]]))==ncol(STAR.data$LoM.raw[[1]][[myreads]]) ){
      if( md.orientation == "byCol" ){ # samples are one per column
        annCol = NULL
        namecol = setdiff( 1:ncol(md.dt), idx2 )
        md.factors = as.character(md.dt[,namecol,with=F])
        for(k in 1:nrow(md.dt) ){
          annCol[[ md.dt[k,get(names(md.dt)[namecol])] ]][idx1] =
          as.vector( md.dt[ k, mget(names(md.dt)[idx2]) ] )
        }
      } else { # samples are one per row
        annCol = NULL
        namecol = setdiff( names(md.dt), md.IDcol )
        md.factors = as.character(namecol)
        for( k in setdiff(names(md.dt),md.IDcol) ){
          annCol[[ k ]][idx1] =
          as.vector( md.dt[ idx2, get(k) ] )
        }
      }
    } else {
      stop(paste("Some samples have no annotation in",md.file))
    }


    mytypes = names(LoM.norms)
    mynorms = names(LoM.norms[[1]])

    grpBy = annCol [[annCol.lmBy]]
    annCol.plotme = c(annCol.lmBy)
    clim.pct=0.96
    histbins=20
    for(i in 1:length(mytypes) ){
      for(j in 1:length(mynorms)){
        # set up matrices and config for plotting
        if( length(STAR.data$LoM.raw[[ mytypes[i] ]])>1 ){
          rawmat = STAR.data$LoM.raw[[ mytypes[i] ]][[myreads]]
        } else {
          rawmat = STAR.data$LoM.raw[[ mytypes[i] ]][[1]]
        }
        normmat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(rawmat)=samp.labels; colnames(normmat)=samp.labels
        plotdata = list(plotdir='.',plotbase=paste(mynorms[j],mytypes[i],sep='.'),plottitle=proj.title)
        rowmask = rowSums(rawmat>1,na.rm=T) > (ncol(rawmat)/length(unique(grpBy)) ) & rowSums(is.na(normmat)) < (ncol(normmat)/length(unique(grpBy)))
        if(sum(rowmask)>hclust.limit){ #hack! build optimization!
          rowmask = rowSums(rawmat>=quantile(rawmat,probs=hc.frac.cut,na.rm=T),na.rm=T) > (ncol(rawmat)/length(unique(grpBy)) ) & rowSums(is.na(normmat)) < (ncol(normmat)/length(unique(grpBy)) )
        }
        ans = summary.plots(rawmat=log2(rawmat +1), normmat=normmat, mynorm=mynorms[j], samp.labels=samp.labels, samp.classes=grpBy, plotdata=plotdata,plot2file=TRUE,histbins=histbins, colorspec=colors.rgb)
        ans = qc.clusters(rawmat=log2(rawmat[rowmask,] +1), normmat=normmat[rowmask,], attribs=annCol[annCol.plotme], oneclass=annCol.lmBy, colorspec=colors.rgb, plotdata=plotdata, plot2file=TRUE, clim.pct=clim.pct)
      }
    }

    regress_lsls = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(regress_lsls) = names(STAR.data$LoM.raw)
    contr_ls = list("Condition"=list(baseline="{baseline}",contr.FUN="contr.treatment"))
    # set baseline for regression in parameter(s) of interest
    # contr.treatment generates regression coefficients that are like (adjusted mean) ratios of other groups to the baseline group.
    # contr.sum generates coefficients that are like (adjusted mean) ratios to average all for all but the mandadory ommitted treatment group (because there is always one fewer independent pairwise comparison than there are pairs).

    lm_expr = "y ~ {lmBy}"
    rowmask_ls = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(rowmask_ls) = names(STAR.data$LoM.raw)
    for(i in 1:length(mytypes) ){
      for(j in which(!mynorms %in% do.not.regress) ){
        # set up data matrices
        if( length(STAR.data$LoM.raw[[ mytypes[i] ]])>1 ){
          rawmat = STAR.data$LoM.raw[[ mytypes[i] ]][[myreads]]
        } else {
          rawmat = STAR.data$LoM.raw[[ mytypes[i] ]][[1]]
        }
        tmp = unlist(lapply(contr_ls,function(x){x$baseline}))
        tmp=paste(names(tmp),tmp,sep=".")

        plotdata = list(plotdir='./',plotbase=paste(mynorms[j],mytypes[i],'vs',tmp,sep='.'),plottitle=proj.title)
        normmat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(rawmat)=samp.labels; colnames(normmat)=samp.labels

        # prepare rowmask for heatmap/MDS (remove non-expr or low expr>hclustlim)
        rowmask = rowSums(rawmat>1,na.rm=T) > (ncol(rawmat)/length(unique(grpBy)) ) & rowSums(is.na(normmat)) <= na.lim
        rowmask_ls[[ mytypes[i] ]][[ mynorms[j] ]] = rowmask # save for later

        # regression

        regress_lsls[[mytypes[i]]][[mynorms[j]]] = regressMatrix(normmat[rowmask,], expt.design=annCol[annCol.lmBy],
        lm_expression=lm_expr, contr_list = contr_ls, plot2file = TRUE, plotdata = plotdata)
      }
    }

    topn=500 # number with which to play
    for( i in 1:length(mytypes) ){
      for( j in which(!mynorms %in% do.not.regress) ){
        ans = regress_lsls[[mytypes[i]]][[mynorms[j]]]$q_list
        cat(mytypes[i],mynorms[j],"1-p0:",signif(unlist(lapply(2:length(ans),function(x){1-ans[[x]]$pi0})),2),"\n")
        cat(mytypes[i],mynorms[j],"qcut:",signif(unlist(lapply(2:length(ans),function(x){quantile(ans[[x]]$qvalues,probs=topn/length(ans[[x]]$qvalues))})),3),
        "returns ~",topn,"out of",length(ans[[2]]$qvalues),"\n")
    }}

    # Plots for each design factor (default) or for factors specified in facSel
    #   1) Histogram of p values that were included in the design
    #      Look for a peak on the left, and no peaks in the middle or on the right
    #   2) qvalue's default plots, with full qvalue range c(0,1) plotted
    #      Look for descent to pi0 in the top left tuning plot with good asymptote
    #      The slow/steep rise in q-values in remaining plots depends on resolving power of data


    lmBy = annCol.lmBy
    histbins=20
    for( i in 1:length(mytypes) ){
      for( j in which(!mynorms %in% do.not.regress) ){
        normmat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(normmat)=samp.labels
        reg_ls = regress_lsls[[mytypes[i]]][[mynorms[j]]]
        mymask= rowmask_ls[[mytypes[i]]][[mynorms[j]]]
        # pull out regression design and set up plot config
        tmp = unlist(lapply(contr_ls,function(x){x$baseline}))
        tmp=paste(names(tmp),tmp,sep=".")
        plotdata = list(plotdir='./',plotbase=paste(mynorms[j],mytypes[i],tmp,sep='.'),plottitle=proj.title)
        for(fac in names(reg_ls$q_list)[grep(lmBy,names(reg_ls$q_list))] ){
          pdata = plotdata
          pdata$plotbase = paste(plotdata$plotbase,make.names(fac),sep='.')
          ans = qcQvalues(norm_x=normmat[mymask,], pvalue_v=reg_ls$p_mat[,fac], obj_qvalue=reg_ls$q_list[[fac]], attribs=annCol[lmBy],
          oneclass=lmBy, plotdata=pdata, colorspec=colors.rgb, histbins=histbins, plot2file=TRUE)
        }
      }
    }

    # Build of ratios to baselines for desired design factors
    # Also build masks selecting genes based on q-values per factor
    #  and follow-up considerations such as expression level and fold change
    # Loop over q-value cuts to assist with final cut selection.
    # Plot heatmaps and MDS plots based on selected genes and designed ratios


    ngene_v = c(200,500,1000) # q-value cuts by number; can also cut by q-value
    ratioby_ls = list("Condition"=contr_ls$Condition$baseline)
    ratio_fold = 1.3
    intensity_fold = 2

    cut_ls = list(q_combine="OR", rcut_fold=ratio_fold, icut_fold=intensity_fold)
    # settings for heatmaps
    annCol.plotme = annCol.lmBy # heatmap tracks
    clustrowmin = 10 # min data rows for heatmap and MDS plots
    # save ratios and selections
    select_lsmk = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(select_lsmk) = names(STAR.data$LoM.raw)
    ratio_lsmat = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(ratio_lsmat) = names(STAR.data$LoM.raw)
    for( i in 1:length(mytypes) ){
      for( j in which(!mynorms %in% do.not.regress) ){
        normmat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(normmat)=samp.labels
        if( grepl('SJ',mytypes[i]) ){ #not the most robust way to find SJs
          # map gene IDs back to data matrix using mapping built earlier
          # this enables include_ID to select rows
          idx2 = match( rownames(STAR.data$LoM.raw[[ mytypes[i] ]][[1]]), mySJ_dt[,get(pos.col)] )
          idx1 = which(!is.na(idx2)); idx2 = idx2[idx1]
          rownames(normmat)[idx1] = mySJ_dt[idx2,get(gtf.col[1])]
        }
        mymask= rowmask_ls[[mytypes[i]]][[mynorms[j]]]
        reg_ls = regress_lsls[[mytypes[i]]][[mynorms[j]]]$q_list
        reg_ls = reg_ls[!grepl('Intercept',names(reg_ls))]
        plotdata = list(plotdir='./',plotbase=paste(mynorms[j],sub('counts','ratios',mytypes[i]),'minratio',paste0(ratio_fold,'x'),'minexpr',round(min(normmat,na.rm=T)+log2(intensity_fold),1),sep='.'),plottitle=proj.title)
        for( ngenes in ngene_v ) {
          # calculate and plot ratios
          cut_ls$qcut = ngenes
          pdata = plotdata
          pdata$plotbase = paste(plotdata$plotbase,'ratio.vs',ratioby_ls$strain,ngenes,sep='.')
          # this function makes the ratios and cuts
          ans = designRatios(normmat[mymask,], q_list=reg_ls, attribs=annCol[annCol.plotme], ratioby_ls=ratioby_ls, cut_ls=cut_ls)
          # this function creates the heatmap and MDS plot
          if( sum(ans$rowmask)>clustrowmin ){
            ans2= plotRatios( ratiomat=ans$ratiomat, attribs=annCol[annCol.plotme], oneclass=lmBy, plotdata=pdata, colorspec=colors.rgb, rowmask=ans$rowmask, plot2file=TRUE)
          }
          # save selections
          select_lsmk[[mytypes[i]]][[mynorms[j]]][paste(fac,ngenes,sep=":")] = ans['rowmask']
        }
        # save ratiomat (not dependent on ngene cut) after ngene loop
        ratio_lsmat[[mytypes[i]]][[mynorms[j]]] = ans$ratiomat
      }
    }

    dir.create(file.path(getwd(),'PairwiseScatter'),showWarnings=FALSE)
    for( i in 1:length(mytypes) ){
      for( j in 1:length(mynorms) ){
        scatter_mat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(normmat)=samp.labels
        plotdata = list(plotdir='./PairwiseScatter',plotbase=paste(mynorms[j],mytypes[i],'Scatter',lmBy,sep=':'),plottitle=proj.title)
        scatterplot(scatter_mat, attribs=annCol[[lmBy]],plotdata=plotdata,plot2file = TRUE)
      }
    }


    ratio_mat = ratio_lsmat[[{path_type}]][[{path_norms}]]
    reg_ls = regress_lsls[[{path_type}]][[{path_norms}]]
    ID = rownames(ratio_mat)

    ensembl=useMart("ensembl")
    listDatasets(ensembl)
    mouse = useMart("ensembl", dataset = "{mart_dataset}")
    human = useMart("ensembl", dataset = "hsapiens_gene_ensembl")

    hsa_entrezID = getLDS(attributes = "ensembl_gene_id",mart = mouse,  attributesL = "entrezgene", martL = human)
    Entrez.ID = character(length(ID))
    Entrez.ID[] = NA
    idx2 = match(ID,hsa_entrezID$Gene.ID)
    idx1 = which(!is.na(idx2)); idx2= idx2[idx1]
    Entrez.ID[idx1] = hsa_entrezID$EntrezGene.ID[idx2];
    Entrez.ID = as.numeric(Entrez.ID)

    ###Human gene symbols
    hsa_symbol = getLDS(attributes = "ensembl_gene_id",mart = mouse,  attributesL = "hgnc_symbol", martL = human)
    Anno.Symbol = character(length(ID))
    Anno.Symbol[] = NA
    idx2 = match(ID,hsa_symbol$Gene.ID)
    idx1 = which(!is.na(idx2)); idx2= idx2[idx1]
    Anno.Symbol[idx1] = unlist(hsa_symbol$HGNC.symbol[idx2]);
    # assemble IDs for annotation
    backgroundset = as.data.table(cbind(ID, Entrez.ID, Anno.Symbol))
    # assemble signatures


    reg_ls = regress_lsls$gene.counts${path_norms}
    fac = names(reg_ls$q_list)[grep(lmBy,names(reg_ls$q_list))]
    rowmask = select_lsmk[[{path_type}]][[{path_norms}]][[paste(fac,ngenes,sep=":")]]
    sig_gmt = NULL
    betas = reg_ls$b_mat[,fac]
    for (k in names(masks)){
        sig_gmt = NULL
        rowmask = select_lsmk[[mytypes[i]]][[mynorms[j]]][[k]]
        sig_gmt[['all']] =  ID[rowmask]
        fileSettings = list(directory='{read_dir}',baseFilename=paste(k,'with_Uni',sep='_')
        path_ls = ags.wrapper(setlist=sig_gmt, backgroundset, include.identifiers=TRUE, anno.uni=TRUE,
                              fileSettings = fileSettings,
                              functiondir = "{code_dir}/AssociationFunctions/PathwayAnalysis.R",
                              resourcedir = "/home/exacloud/lustre1/BioCoders/DataResources/AnnotationSources/old_association_resources",
                              return.OM=TRUE, ecut=0.05, ocut=5)
        fileSettings = list(directory='{read_dir}',baseFilename=paste(k,'with_out_Uni',sep='_')
        pathi_ls = ags.wrapper(setlist=sig_gmt, backgroundset, include.identifiers=TRUE, anno.uni=FALSE,
                       fileSettings = fileSettings,
                       functiondir = "{code_dir}/AssociationFunctions/PathwayAnalysis.R",
                       resourcedir = "/home/exacloud/lustre1/BioCoders/DataResources/AnnotationSources/old_association_resources",
                       return.OM=TRUE, ecut=0.05, ocut=5)
    }
    gtf.Rdir = "{gtf_read_dir}"
    out_norm_mat = LoM.norms$gene.counts$loess[rowmask_ls${path_type}${path_norms},]
    out_table = outputTable(normmat= out_norm_mat, gtf.file = "{gtfFile}",ratiomat = ratio_lsmat${path_type}${path_norms}, q_list=reg_ls$q_list)
    write.csv(out_table, row.names = FALSE, file=paste({project_title},{path_norms},'Normed_with_Ratio_and_Abundance.csv',sep='_'),quote=FALSE)
    """
 2/3: import textwrap
 2/4: textwrap.fill(code)
 2/5: print textwrap.fill(code)
 2/6: print textwrap.dedent(code).strip()
 2/7: test =  textwrap.dedent(code).strip()
 2/8: code_context ={"code_dir":"TESTING/DICTIONARY/COERSION"}
 2/9: test.format(**code_context)
2/10: code_dir = "/home/exacloud/lustre1/BioCoders/ProjectCollaborations/DPerunderai_RNA161116DP/code/"
2/11: read_dir = "/home/exacloud/lustre1/BioCoders/ProjectCollaborations/DPerunderai_RNA161116DP/results/STAR/"
2/12: taxID = 10090
2/13: gtfFile = "/home/exacloud/lustre1/BioCoders/DataResources/Genomes/mm10/release-87/gtf/Mus_musculus.GRCm38.87.gtf"
2/14: project_title = "DPerunderai_auto_generate"
2/15: baseline = "WT"
2/16: %paste
2/17:
mart_dataset = "mmusculus_gene_ensembl",
                              lmBy = "Identifier", gtf_feature = "gene", read_pattern="^RNA1", useme_cols="RNA1",
                              label_from_colname = "^.*?RNA[^_]*?_([^_]+)_.*", path_type = "gene.counts",
                              path_norms = "loess"
2/18: mart_dataset = "mmusculus_gene_ensembl"
2/19: lmBy = "Identifier"
2/20: gtf_feature = "gene"
2/21: read_pattern="^RNA1"
2/22: useme_cols="RNA1"
2/23: label_from_colname = "^.*?RNA[^_]*?_([^_]+)_.*"
2/24: path_type = "gene.counts"
2/25: path_norms = "loess"
2/26: %paste
2/27:
    gtf_read_dir = '/'.join(gtfFile.split('/')[:-1])
    out_f = open(project_title+'_analysis.R','w')
    code ="""
    require(data.table)
    require(NMF)
    require(affy)
    require(limma)
    require(AnnotationDbi)
    require(biomaRt)
    source("{code_dir}/AbundanceFunctions/BiasReduce.R")
    source("{code_dir}/AbundanceFunctions/ExtractTransformLoad.R")
    source("{code_dir}/AbundanceFunctions/DifferentialAnalysis.R")
    source("{code_dir}/AbundanceFunctions/NonVisualOutput.R")
    source("{code_dir}/GenomicsFunctions/ReadAndParse.R")
    source("{code_dir}/AssociationFunctions/gs.wrapper.R")
    source("{code_dir}/AssociationFunctions/PathwayAnalysis.R")
    source("{code_dir}/BcorePlotting/SummaryPlots.R")
    source("{code_dir}/BcorePlotting/MultipleTestingCorrection.R")
    source("{code_dir}/BcorePlotting/ClusteringPlots.R")


    setwd({read_dir})

    # constants
    # max data rows for hclust
    hclust.limit = 2^16

    # quantile of data distribution reqd in one group's worth of data if too many rows for hclust()
    hc.frac.cut = 0.75;
    SJ.counts.na.frac = 0.25;
    # max fraction of samples not having detected a splice junction for the splice
    # junction to be retained in raw data


    # regression parameters
    na.lim = 0 # max NAs per row tolerated by lm() at least in some cases
    do.not.regress = "alograw" # control norm not to be used for regression stats
    # plotting colors
    colors.rgb = c(rgb(0,0,0),rgb(0.1,0.1,1),rgb(0,.7,.7),rgb(0,.7,0),rgb(.7,1,0),rgb(.7,0,.7))
    md.file = {meta_file}
    md.orientation = "byRow" # sampleIDs are in @ row. alt:byCol (IDs in @ col)
    md.IDcol = "SampleID" # reqd if md.orientation is byRow; byCol==headers are IDs

    # gene annotation
    taxID = {taxID}
    gene2ENSfile = "/home/exacloud/lustre1/BioCoders/DataResources/AnnotationSources/ncbi/gene2ensembl.gz"
    gene2ENS.col = c("taxID","EntrezID","Gene","RefSeqTranscript","EnsemblTranscript","RefSeqProtein","EnsemblProtein")
    gtfFile = "{gtfFile}"
    gtf.feature = "{gtf_feature}"
    gtf.orig.col = c("gene_id","gene_name","gene_biotype")
    gtf.col = c("Gene","Symbol","biotype")
    proj.title = "{project_title}"
    gtf.Rdir = "{gtf_read_dir}"

    readdir = "{read_dir}"
    readpattern = "{read_pattern}"
    useme.cols = "{useme_cols}"
    label.from.colname = "{label_from_colname}"
    samps = dir(path=readdir, pattern=readpattern)
    samp.labels = gsub(label.from.colname,'\\1', samps)

    annCol.names = 'group"
    annCol.lmBy = "{lmBy}"
    annCol.label = "{label_from_colname}"

    # read in STAR alignments
    STAR.data = read_STAR(useme.cols=useme.cols,label.from.colname=label.from.colname,annCol.label=annCol.label,annCol.names=annCol.names,annCol.normBy=NULL,annCol.lmBy=annCol.lmBy,readpattern=readpattern,unstranded.col = list(gene.counts = c(1:4),SJ.counts = c(1:3,7)))

    # filter out SJs with too many NAs
    if(any( names(STAR.data$LoM.raw)=="SJ.counts" & exists("SJ.counts.na.frac") )){
      STAR.data$SJ.counts.orig = STAR.data$LoM.raw$SJ.counts
      for(tag in names(STAR.data$LoM.raw$SJ.counts) ){
        STAR.data$LoM.raw$SJ.counts[[tag]] = STAR.data$LoM.raw$SJ.counts[[tag]][rowSums(is.na(STAR.data$LoM.raw$SJ.counts[[tag]]))<= (SJ.counts.na.frac*ncol(STAR.data$LoM.raw$SJ.counts[[tag]])), ]
      }
    }

    gtf = readENSgtf(filename=gtfFile)
    genes.gtf = gtf[feature==gtf.feature, mget(gtf.orig.col)]
    names(genes.gtf) = gtf.col
    setkeyv(genes.gtf,gtf.col[1])

    ##### parse of NCBI's EntrezID to Ensembl translation
    Entrez2Ensembl = fread(paste("zgrep",paste0("-E '^",taxID,"'"),gene2ENSfile))
    names(Entrez2Ensembl) = gene2ENS.col
    setkeyv(Entrez2Ensembl,gene2ENS.col[3])

    ##### add EntrezIDs to genes.gtf
    tmp = Entrez2Ensembl[,mget(gene2ENS.col[2:3])]; tmp=tmp[!duplicated(tmp),]
    genes.gtf = merge(genes.gtf,tmp,all.x=TRUE)
    rm(tmp)

    myreads = STAR.data$myreads
    LoM.norms = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(LoM.norms) = names(STAR.data$LoM.raw)
    for(tag in names(STAR.data$LoM.raw) ){
      if( length(STAR.data$LoM.raw[[tag]])>1 ){
        LoM.norms[[tag]] = normMatrix(tag=tag,raw.mat=STAR.data$LoM.raw[[tag]][[myreads]])
      } else {
        LoM.norms[[tag]] = normMatrix(tag=tag,raw.mat=STAR.data$LoM.raw[[tag]][[1]])
      }
    }


    ## custom: reannotate from metadata
    ## run if necessary metadata is in file and not also in FASTQ file names
    md.dt = fread({meta_file})
    # map annotation to read matrix
    if( md.orientation == "byCol" ){ # samples are one per column
      idx2 = match( colnames(STAR.data$LoM.raw[[1]][[myreads]]), names(md.dt) )
    } else { # samples are one per row
      idx2 = match( colnames(STAR.data$LoM.raw[[1]][[myreads]]), md.dt[,get(md.IDcol)] )
    }
    idx1 = which(!is.na(idx2)); idx2 = idx2[idx1]
    if( sum(idx1==1:ncol(STAR.data$LoM.raw[[1]][[myreads]]))==ncol(STAR.data$LoM.raw[[1]][[myreads]]) ){
      if( md.orientation == "byCol" ){ # samples are one per column
        annCol = NULL
        namecol = setdiff( 1:ncol(md.dt), idx2 )
        md.factors = as.character(md.dt[,namecol,with=F])
        for(k in 1:nrow(md.dt) ){
          annCol[[ md.dt[k,get(names(md.dt)[namecol])] ]][idx1] =
          as.vector( md.dt[ k, mget(names(md.dt)[idx2]) ] )
        }
      } else { # samples are one per row
        annCol = NULL
        namecol = setdiff( names(md.dt), md.IDcol )
        md.factors = as.character(namecol)
        for( k in setdiff(names(md.dt),md.IDcol) ){
          annCol[[ k ]][idx1] =
          as.vector( md.dt[ idx2, get(k) ] )
        }
      }
    } else {
      stop(paste("Some samples have no annotation in",md.file))
    }


    mytypes = names(LoM.norms)
    mynorms = names(LoM.norms[[1]])

    grpBy = annCol [[annCol.lmBy]]
    annCol.plotme = c(annCol.lmBy)
    clim.pct=0.96
    histbins=20
    for(i in 1:length(mytypes) ){
      for(j in 1:length(mynorms)){
        # set up matrices and config for plotting
        if( length(STAR.data$LoM.raw[[ mytypes[i] ]])>1 ){
          rawmat = STAR.data$LoM.raw[[ mytypes[i] ]][[myreads]]
        } else {
          rawmat = STAR.data$LoM.raw[[ mytypes[i] ]][[1]]
        }
        normmat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(rawmat)=samp.labels; colnames(normmat)=samp.labels
        plotdata = list(plotdir='.',plotbase=paste(mynorms[j],mytypes[i],sep='.'),plottitle=proj.title)
        rowmask = rowSums(rawmat>1,na.rm=T) > (ncol(rawmat)/length(unique(grpBy)) ) & rowSums(is.na(normmat)) < (ncol(normmat)/length(unique(grpBy)))
        if(sum(rowmask)>hclust.limit){ #hack! build optimization!
          rowmask = rowSums(rawmat>=quantile(rawmat,probs=hc.frac.cut,na.rm=T),na.rm=T) > (ncol(rawmat)/length(unique(grpBy)) ) & rowSums(is.na(normmat)) < (ncol(normmat)/length(unique(grpBy)) )
        }
        ans = summary.plots(rawmat=log2(rawmat +1), normmat=normmat, mynorm=mynorms[j], samp.labels=samp.labels, samp.classes=grpBy, plotdata=plotdata,plot2file=TRUE,histbins=histbins, colorspec=colors.rgb)
        ans = qc.clusters(rawmat=log2(rawmat[rowmask,] +1), normmat=normmat[rowmask,], attribs=annCol[annCol.plotme], oneclass=annCol.lmBy, colorspec=colors.rgb, plotdata=plotdata, plot2file=TRUE, clim.pct=clim.pct)
      }
    }

    regress_lsls = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(regress_lsls) = names(STAR.data$LoM.raw)
    contr_ls = list("Condition"=list(baseline="{baseline}",contr.FUN="contr.treatment"))
    # set baseline for regression in parameter(s) of interest
    # contr.treatment generates regression coefficients that are like (adjusted mean) ratios of other groups to the baseline group.
    # contr.sum generates coefficients that are like (adjusted mean) ratios to average all for all but the mandadory ommitted treatment group (because there is always one fewer independent pairwise comparison than there are pairs).

    lm_expr = "y ~ {lmBy}"
    rowmask_ls = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(rowmask_ls) = names(STAR.data$LoM.raw)
    for(i in 1:length(mytypes) ){
      for(j in which(!mynorms %in% do.not.regress) ){
        # set up data matrices
        if( length(STAR.data$LoM.raw[[ mytypes[i] ]])>1 ){
          rawmat = STAR.data$LoM.raw[[ mytypes[i] ]][[myreads]]
        } else {
          rawmat = STAR.data$LoM.raw[[ mytypes[i] ]][[1]]
        }
        tmp = unlist(lapply(contr_ls,function(x){x$baseline}))
        tmp=paste(names(tmp),tmp,sep=".")

        plotdata = list(plotdir='./',plotbase=paste(mynorms[j],mytypes[i],'vs',tmp,sep='.'),plottitle=proj.title)
        normmat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(rawmat)=samp.labels; colnames(normmat)=samp.labels

        # prepare rowmask for heatmap/MDS (remove non-expr or low expr>hclustlim)
        rowmask = rowSums(rawmat>1,na.rm=T) > (ncol(rawmat)/length(unique(grpBy)) ) & rowSums(is.na(normmat)) <= na.lim
        rowmask_ls[[ mytypes[i] ]][[ mynorms[j] ]] = rowmask # save for later

        # regression

        regress_lsls[[mytypes[i]]][[mynorms[j]]] = regressMatrix(normmat[rowmask,], expt.design=annCol[annCol.lmBy],
        lm_expression=lm_expr, contr_list = contr_ls, plot2file = TRUE, plotdata = plotdata)
      }
    }

    topn=500 # number with which to play
    for( i in 1:length(mytypes) ){
      for( j in which(!mynorms %in% do.not.regress) ){
        ans = regress_lsls[[mytypes[i]]][[mynorms[j]]]$q_list
        cat(mytypes[i],mynorms[j],"1-p0:",signif(unlist(lapply(2:length(ans),function(x){1-ans[[x]]$pi0})),2),"\n")
        cat(mytypes[i],mynorms[j],"qcut:",signif(unlist(lapply(2:length(ans),function(x){quantile(ans[[x]]$qvalues,probs=topn/length(ans[[x]]$qvalues))})),3),
        "returns ~",topn,"out of",length(ans[[2]]$qvalues),"\n")
    }}

    # Plots for each design factor (default) or for factors specified in facSel
    #   1) Histogram of p values that were included in the design
    #      Look for a peak on the left, and no peaks in the middle or on the right
    #   2) qvalue's default plots, with full qvalue range c(0,1) plotted
    #      Look for descent to pi0 in the top left tuning plot with good asymptote
    #      The slow/steep rise in q-values in remaining plots depends on resolving power of data


    lmBy = annCol.lmBy
    histbins=20
    for( i in 1:length(mytypes) ){
      for( j in which(!mynorms %in% do.not.regress) ){
        normmat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(normmat)=samp.labels
        reg_ls = regress_lsls[[mytypes[i]]][[mynorms[j]]]
        mymask= rowmask_ls[[mytypes[i]]][[mynorms[j]]]
        # pull out regression design and set up plot config
        tmp = unlist(lapply(contr_ls,function(x){x$baseline}))
        tmp=paste(names(tmp),tmp,sep=".")
        plotdata = list(plotdir='./',plotbase=paste(mynorms[j],mytypes[i],tmp,sep='.'),plottitle=proj.title)
        for(fac in names(reg_ls$q_list)[grep(lmBy,names(reg_ls$q_list))] ){
          pdata = plotdata
          pdata$plotbase = paste(plotdata$plotbase,make.names(fac),sep='.')
          ans = qcQvalues(norm_x=normmat[mymask,], pvalue_v=reg_ls$p_mat[,fac], obj_qvalue=reg_ls$q_list[[fac]], attribs=annCol[lmBy],
          oneclass=lmBy, plotdata=pdata, colorspec=colors.rgb, histbins=histbins, plot2file=TRUE)
        }
      }
    }

    # Build of ratios to baselines for desired design factors
    # Also build masks selecting genes based on q-values per factor
    #  and follow-up considerations such as expression level and fold change
    # Loop over q-value cuts to assist with final cut selection.
    # Plot heatmaps and MDS plots based on selected genes and designed ratios


    ngene_v = c(200,500,1000) # q-value cuts by number; can also cut by q-value
    ratioby_ls = list("Condition"=contr_ls$Condition$baseline)
    ratio_fold = 1.3
    intensity_fold = 2

    cut_ls = list(q_combine="OR", rcut_fold=ratio_fold, icut_fold=intensity_fold)
    # settings for heatmaps
    annCol.plotme = annCol.lmBy # heatmap tracks
    clustrowmin = 10 # min data rows for heatmap and MDS plots
    # save ratios and selections
    select_lsmk = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(select_lsmk) = names(STAR.data$LoM.raw)
    ratio_lsmat = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(ratio_lsmat) = names(STAR.data$LoM.raw)
    for( i in 1:length(mytypes) ){
      for( j in which(!mynorms %in% do.not.regress) ){
        normmat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(normmat)=samp.labels
        if( grepl('SJ',mytypes[i]) ){ #not the most robust way to find SJs
          # map gene IDs back to data matrix using mapping built earlier
          # this enables include_ID to select rows
          idx2 = match( rownames(STAR.data$LoM.raw[[ mytypes[i] ]][[1]]), mySJ_dt[,get(pos.col)] )
          idx1 = which(!is.na(idx2)); idx2 = idx2[idx1]
          rownames(normmat)[idx1] = mySJ_dt[idx2,get(gtf.col[1])]
        }
        mymask= rowmask_ls[[mytypes[i]]][[mynorms[j]]]
        reg_ls = regress_lsls[[mytypes[i]]][[mynorms[j]]]$q_list
        reg_ls = reg_ls[!grepl('Intercept',names(reg_ls))]
        plotdata = list(plotdir='./',plotbase=paste(mynorms[j],sub('counts','ratios',mytypes[i]),'minratio',paste0(ratio_fold,'x'),'minexpr',round(min(normmat,na.rm=T)+log2(intensity_fold),1),sep='.'),plottitle=proj.title)
        for( ngenes in ngene_v ) {
          # calculate and plot ratios
          cut_ls$qcut = ngenes
          pdata = plotdata
          pdata$plotbase = paste(plotdata$plotbase,'ratio.vs',ratioby_ls$strain,ngenes,sep='.')
          # this function makes the ratios and cuts
          ans = designRatios(normmat[mymask,], q_list=reg_ls, attribs=annCol[annCol.plotme], ratioby_ls=ratioby_ls, cut_ls=cut_ls)
          # this function creates the heatmap and MDS plot
          if( sum(ans$rowmask)>clustrowmin ){
            ans2= plotRatios( ratiomat=ans$ratiomat, attribs=annCol[annCol.plotme], oneclass=lmBy, plotdata=pdata, colorspec=colors.rgb, rowmask=ans$rowmask, plot2file=TRUE)
          }
          # save selections
          select_lsmk[[mytypes[i]]][[mynorms[j]]][paste(fac,ngenes,sep=":")] = ans['rowmask']
        }
        # save ratiomat (not dependent on ngene cut) after ngene loop
        ratio_lsmat[[mytypes[i]]][[mynorms[j]]] = ans$ratiomat
      }
    }

    dir.create(file.path(getwd(),'PairwiseScatter'),showWarnings=FALSE)
    for( i in 1:length(mytypes) ){
      for( j in 1:length(mynorms) ){
        scatter_mat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(normmat)=samp.labels
        plotdata = list(plotdir='./PairwiseScatter',plotbase=paste(mynorms[j],mytypes[i],'Scatter',lmBy,sep=':'),plottitle=proj.title)
        scatterplot(scatter_mat, attribs=annCol[[lmBy]],plotdata=plotdata,plot2file = TRUE)
      }
    }


    ratio_mat = ratio_lsmat[[{path_type}]][[{path_norms}]]
    reg_ls = regress_lsls[[{path_type}]][[{path_norms}]]
    ID = rownames(ratio_mat)

    ensembl=useMart("ensembl")
    listDatasets(ensembl)
    mouse = useMart("ensembl", dataset = "{mart_dataset}")
    human = useMart("ensembl", dataset = "hsapiens_gene_ensembl")

    hsa_entrezID = getLDS(attributes = "ensembl_gene_id",mart = mouse,  attributesL = "entrezgene", martL = human)
    Entrez.ID = character(length(ID))
    Entrez.ID[] = NA
    idx2 = match(ID,hsa_entrezID$Gene.ID)
    idx1 = which(!is.na(idx2)); idx2= idx2[idx1]
    Entrez.ID[idx1] = hsa_entrezID$EntrezGene.ID[idx2];
    Entrez.ID = as.numeric(Entrez.ID)

    ###Human gene symbols
    hsa_symbol = getLDS(attributes = "ensembl_gene_id",mart = mouse,  attributesL = "hgnc_symbol", martL = human)
    Anno.Symbol = character(length(ID))
    Anno.Symbol[] = NA
    idx2 = match(ID,hsa_symbol$Gene.ID)
    idx1 = which(!is.na(idx2)); idx2= idx2[idx1]
    Anno.Symbol[idx1] = unlist(hsa_symbol$HGNC.symbol[idx2]);
    # assemble IDs for annotation
    backgroundset = as.data.table(cbind(ID, Entrez.ID, Anno.Symbol))
    # assemble signatures


    reg_ls = regress_lsls$gene.counts${path_norms}
    fac = names(reg_ls$q_list)[grep(lmBy,names(reg_ls$q_list))]
    rowmask = select_lsmk[[{path_type}]][[{path_norms}]][[paste(fac,ngenes,sep=":")]]
    sig_gmt = NULL
    betas = reg_ls$b_mat[,fac]
    for (k in names(masks)){
        sig_gmt = NULL
        rowmask = select_lsmk[[mytypes[i]]][[mynorms[j]]][[k]]
        sig_gmt[['all']] =  ID[rowmask]
        fileSettings = list(directory='{read_dir}',baseFilename=paste(k,'with_Uni',sep='_')
        path_ls = ags.wrapper(setlist=sig_gmt, backgroundset, include.identifiers=TRUE, anno.uni=TRUE,
                              fileSettings = fileSettings,
                              functiondir = "{code_dir}/AssociationFunctions/PathwayAnalysis.R",
                              resourcedir = "/home/exacloud/lustre1/BioCoders/DataResources/AnnotationSources/old_association_resources",
                              return.OM=TRUE, ecut=0.05, ocut=5)
        fileSettings = list(directory='{read_dir}',baseFilename=paste(k,'with_out_Uni',sep='_')
        pathi_ls = ags.wrapper(setlist=sig_gmt, backgroundset, include.identifiers=TRUE, anno.uni=FALSE,
                       fileSettings = fileSettings,
                       functiondir = "{code_dir}/AssociationFunctions/PathwayAnalysis.R",
                       resourcedir = "/home/exacloud/lustre1/BioCoders/DataResources/AnnotationSources/old_association_resources",
                       return.OM=TRUE, ecut=0.05, ocut=5)
    }
    gtf.Rdir = "{gtf_read_dir}"
    out_norm_mat = LoM.norms$gene.counts$loess[rowmask_ls${path_type}${path_norms},]
    out_table = outputTable(normmat= out_norm_mat, gtf.file = "{gtfFile}",ratiomat = ratio_lsmat${path_type}${path_norms}, q_list=reg_ls$q_list)
    write.csv(out_table, row.names = FALSE, file=paste({project_title},{path_norms},'Normed_with_Ratio_and_Abundance.csv',sep='_'),quote=FALSE)
    """
    reformatted_code = textwrap.dedent(code).strip()
    code_context = {"code_dir":code_dir, "meta_file":meta_file, "taxID":taxID, "gtfFile":gtfFile, "gtf_feature":gtf_feature, "project_title":project_title, "gtf_read_dir":gtf_read_dir,"read_dir":read_dir, "read_pattern":read_pattern, "useme_cols":useme_cols, "lmBy":lmBy, "baseline":baseline, "path_type":path_type, "path_norms":path_norms, "mart_dataset":mart_dataset, "label_from_colname":label_from_colname}
    out_f.write(reformatted_code.format(**code_context))
    out_f.close()
2/28: meta_file = "/home/users/estabroj/RNA161116DP_metadata.txt"
2/29:
    gtf_read_dir = '/'.join(gtfFile.split('/')[:-1])
    out_f = open(project_title+'_analysis.R','w')
    code ="""
    require(data.table)
    require(NMF)
    require(affy)
    require(limma)
    require(AnnotationDbi)
    require(biomaRt)
    source("{code_dir}/AbundanceFunctions/BiasReduce.R")
    source("{code_dir}/AbundanceFunctions/ExtractTransformLoad.R")
    source("{code_dir}/AbundanceFunctions/DifferentialAnalysis.R")
    source("{code_dir}/AbundanceFunctions/NonVisualOutput.R")
    source("{code_dir}/GenomicsFunctions/ReadAndParse.R")
    source("{code_dir}/AssociationFunctions/gs.wrapper.R")
    source("{code_dir}/AssociationFunctions/PathwayAnalysis.R")
    source("{code_dir}/BcorePlotting/SummaryPlots.R")
    source("{code_dir}/BcorePlotting/MultipleTestingCorrection.R")
    source("{code_dir}/BcorePlotting/ClusteringPlots.R")


    setwd({read_dir})

    # constants
    # max data rows for hclust
    hclust.limit = 2^16

    # quantile of data distribution reqd in one group's worth of data if too many rows for hclust()
    hc.frac.cut = 0.75;
    SJ.counts.na.frac = 0.25;
    # max fraction of samples not having detected a splice junction for the splice
    # junction to be retained in raw data


    # regression parameters
    na.lim = 0 # max NAs per row tolerated by lm() at least in some cases
    do.not.regress = "alograw" # control norm not to be used for regression stats
    # plotting colors
    colors.rgb = c(rgb(0,0,0),rgb(0.1,0.1,1),rgb(0,.7,.7),rgb(0,.7,0),rgb(.7,1,0),rgb(.7,0,.7))
    md.file = {meta_file}
    md.orientation = "byRow" # sampleIDs are in @ row. alt:byCol (IDs in @ col)
    md.IDcol = "SampleID" # reqd if md.orientation is byRow; byCol==headers are IDs

    # gene annotation
    taxID = {taxID}
    gene2ENSfile = "/home/exacloud/lustre1/BioCoders/DataResources/AnnotationSources/ncbi/gene2ensembl.gz"
    gene2ENS.col = c("taxID","EntrezID","Gene","RefSeqTranscript","EnsemblTranscript","RefSeqProtein","EnsemblProtein")
    gtfFile = "{gtfFile}"
    gtf.feature = "{gtf_feature}"
    gtf.orig.col = c("gene_id","gene_name","gene_biotype")
    gtf.col = c("Gene","Symbol","biotype")
    proj.title = "{project_title}"
    gtf.Rdir = "{gtf_read_dir}"

    readdir = "{read_dir}"
    readpattern = "{read_pattern}"
    useme.cols = "{useme_cols}"
    label.from.colname = "{label_from_colname}"
    samps = dir(path=readdir, pattern=readpattern)
    samp.labels = gsub(label.from.colname,'\\1', samps)

    annCol.names = 'group"
    annCol.lmBy = "{lmBy}"
    annCol.label = "{label_from_colname}"

    # read in STAR alignments
    STAR.data = read_STAR(useme.cols=useme.cols,label.from.colname=label.from.colname,annCol.label=annCol.label,annCol.names=annCol.names,annCol.normBy=NULL,annCol.lmBy=annCol.lmBy,readpattern=readpattern,unstranded.col = list(gene.counts = c(1:4),SJ.counts = c(1:3,7)))

    # filter out SJs with too many NAs
    if(any( names(STAR.data$LoM.raw)=="SJ.counts" & exists("SJ.counts.na.frac") )){
      STAR.data$SJ.counts.orig = STAR.data$LoM.raw$SJ.counts
      for(tag in names(STAR.data$LoM.raw$SJ.counts) ){
        STAR.data$LoM.raw$SJ.counts[[tag]] = STAR.data$LoM.raw$SJ.counts[[tag]][rowSums(is.na(STAR.data$LoM.raw$SJ.counts[[tag]]))<= (SJ.counts.na.frac*ncol(STAR.data$LoM.raw$SJ.counts[[tag]])), ]
      }
    }

    gtf = readENSgtf(filename=gtfFile)
    genes.gtf = gtf[feature==gtf.feature, mget(gtf.orig.col)]
    names(genes.gtf) = gtf.col
    setkeyv(genes.gtf,gtf.col[1])

    ##### parse of NCBI's EntrezID to Ensembl translation
    Entrez2Ensembl = fread(paste("zgrep",paste0("-E '^",taxID,"'"),gene2ENSfile))
    names(Entrez2Ensembl) = gene2ENS.col
    setkeyv(Entrez2Ensembl,gene2ENS.col[3])

    ##### add EntrezIDs to genes.gtf
    tmp = Entrez2Ensembl[,mget(gene2ENS.col[2:3])]; tmp=tmp[!duplicated(tmp),]
    genes.gtf = merge(genes.gtf,tmp,all.x=TRUE)
    rm(tmp)

    myreads = STAR.data$myreads
    LoM.norms = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(LoM.norms) = names(STAR.data$LoM.raw)
    for(tag in names(STAR.data$LoM.raw) ){
      if( length(STAR.data$LoM.raw[[tag]])>1 ){
        LoM.norms[[tag]] = normMatrix(tag=tag,raw.mat=STAR.data$LoM.raw[[tag]][[myreads]])
      } else {
        LoM.norms[[tag]] = normMatrix(tag=tag,raw.mat=STAR.data$LoM.raw[[tag]][[1]])
      }
    }


    ## custom: reannotate from metadata
    ## run if necessary metadata is in file and not also in FASTQ file names
    md.dt = fread({meta_file})
    # map annotation to read matrix
    if( md.orientation == "byCol" ){ # samples are one per column
      idx2 = match( colnames(STAR.data$LoM.raw[[1]][[myreads]]), names(md.dt) )
    } else { # samples are one per row
      idx2 = match( colnames(STAR.data$LoM.raw[[1]][[myreads]]), md.dt[,get(md.IDcol)] )
    }
    idx1 = which(!is.na(idx2)); idx2 = idx2[idx1]
    if( sum(idx1==1:ncol(STAR.data$LoM.raw[[1]][[myreads]]))==ncol(STAR.data$LoM.raw[[1]][[myreads]]) ){
      if( md.orientation == "byCol" ){ # samples are one per column
        annCol = NULL
        namecol = setdiff( 1:ncol(md.dt), idx2 )
        md.factors = as.character(md.dt[,namecol,with=F])
        for(k in 1:nrow(md.dt) ){
          annCol[[ md.dt[k,get(names(md.dt)[namecol])] ]][idx1] =
          as.vector( md.dt[ k, mget(names(md.dt)[idx2]) ] )
        }
      } else { # samples are one per row
        annCol = NULL
        namecol = setdiff( names(md.dt), md.IDcol )
        md.factors = as.character(namecol)
        for( k in setdiff(names(md.dt),md.IDcol) ){
          annCol[[ k ]][idx1] =
          as.vector( md.dt[ idx2, get(k) ] )
        }
      }
    } else {
      stop(paste("Some samples have no annotation in",md.file))
    }


    mytypes = names(LoM.norms)
    mynorms = names(LoM.norms[[1]])

    grpBy = annCol [[annCol.lmBy]]
    annCol.plotme = c(annCol.lmBy)
    clim.pct=0.96
    histbins=20
    for(i in 1:length(mytypes) ){
      for(j in 1:length(mynorms)){
        # set up matrices and config for plotting
        if( length(STAR.data$LoM.raw[[ mytypes[i] ]])>1 ){
          rawmat = STAR.data$LoM.raw[[ mytypes[i] ]][[myreads]]
        } else {
          rawmat = STAR.data$LoM.raw[[ mytypes[i] ]][[1]]
        }
        normmat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(rawmat)=samp.labels; colnames(normmat)=samp.labels
        plotdata = list(plotdir='.',plotbase=paste(mynorms[j],mytypes[i],sep='.'),plottitle=proj.title)
        rowmask = rowSums(rawmat>1,na.rm=T) > (ncol(rawmat)/length(unique(grpBy)) ) & rowSums(is.na(normmat)) < (ncol(normmat)/length(unique(grpBy)))
        if(sum(rowmask)>hclust.limit){ #hack! build optimization!
          rowmask = rowSums(rawmat>=quantile(rawmat,probs=hc.frac.cut,na.rm=T),na.rm=T) > (ncol(rawmat)/length(unique(grpBy)) ) & rowSums(is.na(normmat)) < (ncol(normmat)/length(unique(grpBy)) )
        }
        ans = summary.plots(rawmat=log2(rawmat +1), normmat=normmat, mynorm=mynorms[j], samp.labels=samp.labels, samp.classes=grpBy, plotdata=plotdata,plot2file=TRUE,histbins=histbins, colorspec=colors.rgb)
        ans = qc.clusters(rawmat=log2(rawmat[rowmask,] +1), normmat=normmat[rowmask,], attribs=annCol[annCol.plotme], oneclass=annCol.lmBy, colorspec=colors.rgb, plotdata=plotdata, plot2file=TRUE, clim.pct=clim.pct)
      }
    }

    regress_lsls = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(regress_lsls) = names(STAR.data$LoM.raw)
    contr_ls = list("Condition"=list(baseline="{baseline}",contr.FUN="contr.treatment"))
    # set baseline for regression in parameter(s) of interest
    # contr.treatment generates regression coefficients that are like (adjusted mean) ratios of other groups to the baseline group.
    # contr.sum generates coefficients that are like (adjusted mean) ratios to average all for all but the mandadory ommitted treatment group (because there is always one fewer independent pairwise comparison than there are pairs).

    lm_expr = "y ~ {lmBy}"
    rowmask_ls = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(rowmask_ls) = names(STAR.data$LoM.raw)
    for(i in 1:length(mytypes) ){
      for(j in which(!mynorms %in% do.not.regress) ){
        # set up data matrices
        if( length(STAR.data$LoM.raw[[ mytypes[i] ]])>1 ){
          rawmat = STAR.data$LoM.raw[[ mytypes[i] ]][[myreads]]
        } else {
          rawmat = STAR.data$LoM.raw[[ mytypes[i] ]][[1]]
        }
        tmp = unlist(lapply(contr_ls,function(x){x$baseline}))
        tmp=paste(names(tmp),tmp,sep=".")

        plotdata = list(plotdir='./',plotbase=paste(mynorms[j],mytypes[i],'vs',tmp,sep='.'),plottitle=proj.title)
        normmat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(rawmat)=samp.labels; colnames(normmat)=samp.labels

        # prepare rowmask for heatmap/MDS (remove non-expr or low expr>hclustlim)
        rowmask = rowSums(rawmat>1,na.rm=T) > (ncol(rawmat)/length(unique(grpBy)) ) & rowSums(is.na(normmat)) <= na.lim
        rowmask_ls[[ mytypes[i] ]][[ mynorms[j] ]] = rowmask # save for later

        # regression

        regress_lsls[[mytypes[i]]][[mynorms[j]]] = regressMatrix(normmat[rowmask,], expt.design=annCol[annCol.lmBy],
        lm_expression=lm_expr, contr_list = contr_ls, plot2file = TRUE, plotdata = plotdata)
      }
    }

    topn=500 # number with which to play
    for( i in 1:length(mytypes) ){
      for( j in which(!mynorms %in% do.not.regress) ){
        ans = regress_lsls[[mytypes[i]]][[mynorms[j]]]$q_list
        cat(mytypes[i],mynorms[j],"1-p0:",signif(unlist(lapply(2:length(ans),function(x){1-ans[[x]]$pi0})),2),"\n")
        cat(mytypes[i],mynorms[j],"qcut:",signif(unlist(lapply(2:length(ans),function(x){quantile(ans[[x]]$qvalues,probs=topn/length(ans[[x]]$qvalues))})),3),
        "returns ~",topn,"out of",length(ans[[2]]$qvalues),"\n")
    }}

    # Plots for each design factor (default) or for factors specified in facSel
    #   1) Histogram of p values that were included in the design
    #      Look for a peak on the left, and no peaks in the middle or on the right
    #   2) qvalue's default plots, with full qvalue range c(0,1) plotted
    #      Look for descent to pi0 in the top left tuning plot with good asymptote
    #      The slow/steep rise in q-values in remaining plots depends on resolving power of data


    lmBy = annCol.lmBy
    histbins=20
    for( i in 1:length(mytypes) ){
      for( j in which(!mynorms %in% do.not.regress) ){
        normmat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(normmat)=samp.labels
        reg_ls = regress_lsls[[mytypes[i]]][[mynorms[j]]]
        mymask= rowmask_ls[[mytypes[i]]][[mynorms[j]]]
        # pull out regression design and set up plot config
        tmp = unlist(lapply(contr_ls,function(x){x$baseline}))
        tmp=paste(names(tmp),tmp,sep=".")
        plotdata = list(plotdir='./',plotbase=paste(mynorms[j],mytypes[i],tmp,sep='.'),plottitle=proj.title)
        for(fac in names(reg_ls$q_list)[grep(lmBy,names(reg_ls$q_list))] ){
          pdata = plotdata
          pdata$plotbase = paste(plotdata$plotbase,make.names(fac),sep='.')
          ans = qcQvalues(norm_x=normmat[mymask,], pvalue_v=reg_ls$p_mat[,fac], obj_qvalue=reg_ls$q_list[[fac]], attribs=annCol[lmBy],
          oneclass=lmBy, plotdata=pdata, colorspec=colors.rgb, histbins=histbins, plot2file=TRUE)
        }
      }
    }

    # Build of ratios to baselines for desired design factors
    # Also build masks selecting genes based on q-values per factor
    #  and follow-up considerations such as expression level and fold change
    # Loop over q-value cuts to assist with final cut selection.
    # Plot heatmaps and MDS plots based on selected genes and designed ratios


    ngene_v = c(200,500,1000) # q-value cuts by number; can also cut by q-value
    ratioby_ls = list("Condition"=contr_ls$Condition$baseline)
    ratio_fold = 1.3
    intensity_fold = 2

    cut_ls = list(q_combine="OR", rcut_fold=ratio_fold, icut_fold=intensity_fold)
    # settings for heatmaps
    annCol.plotme = annCol.lmBy # heatmap tracks
    clustrowmin = 10 # min data rows for heatmap and MDS plots
    # save ratios and selections
    select_lsmk = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(select_lsmk) = names(STAR.data$LoM.raw)
    ratio_lsmat = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(ratio_lsmat) = names(STAR.data$LoM.raw)
    for( i in 1:length(mytypes) ){
      for( j in which(!mynorms %in% do.not.regress) ){
        normmat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(normmat)=samp.labels
        if( grepl('SJ',mytypes[i]) ){ #not the most robust way to find SJs
          # map gene IDs back to data matrix using mapping built earlier
          # this enables include_ID to select rows
          idx2 = match( rownames(STAR.data$LoM.raw[[ mytypes[i] ]][[1]]), mySJ_dt[,get(pos.col)] )
          idx1 = which(!is.na(idx2)); idx2 = idx2[idx1]
          rownames(normmat)[idx1] = mySJ_dt[idx2,get(gtf.col[1])]
        }
        mymask= rowmask_ls[[mytypes[i]]][[mynorms[j]]]
        reg_ls = regress_lsls[[mytypes[i]]][[mynorms[j]]]$q_list
        reg_ls = reg_ls[!grepl('Intercept',names(reg_ls))]
        plotdata = list(plotdir='./',plotbase=paste(mynorms[j],sub('counts','ratios',mytypes[i]),'minratio',paste0(ratio_fold,'x'),'minexpr',round(min(normmat,na.rm=T)+log2(intensity_fold),1),sep='.'),plottitle=proj.title)
        for( ngenes in ngene_v ) {
          # calculate and plot ratios
          cut_ls$qcut = ngenes
          pdata = plotdata
          pdata$plotbase = paste(plotdata$plotbase,'ratio.vs',ratioby_ls$strain,ngenes,sep='.')
          # this function makes the ratios and cuts
          ans = designRatios(normmat[mymask,], q_list=reg_ls, attribs=annCol[annCol.plotme], ratioby_ls=ratioby_ls, cut_ls=cut_ls)
          # this function creates the heatmap and MDS plot
          if( sum(ans$rowmask)>clustrowmin ){
            ans2= plotRatios( ratiomat=ans$ratiomat, attribs=annCol[annCol.plotme], oneclass=lmBy, plotdata=pdata, colorspec=colors.rgb, rowmask=ans$rowmask, plot2file=TRUE)
          }
          # save selections
          select_lsmk[[mytypes[i]]][[mynorms[j]]][paste(fac,ngenes,sep=":")] = ans['rowmask']
        }
        # save ratiomat (not dependent on ngene cut) after ngene loop
        ratio_lsmat[[mytypes[i]]][[mynorms[j]]] = ans$ratiomat
      }
    }

    dir.create(file.path(getwd(),'PairwiseScatter'),showWarnings=FALSE)
    for( i in 1:length(mytypes) ){
      for( j in 1:length(mynorms) ){
        scatter_mat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(normmat)=samp.labels
        plotdata = list(plotdir='./PairwiseScatter',plotbase=paste(mynorms[j],mytypes[i],'Scatter',lmBy,sep=':'),plottitle=proj.title)
        scatterplot(scatter_mat, attribs=annCol[[lmBy]],plotdata=plotdata,plot2file = TRUE)
      }
    }


    ratio_mat = ratio_lsmat[[{path_type}]][[{path_norms}]]
    reg_ls = regress_lsls[[{path_type}]][[{path_norms}]]
    ID = rownames(ratio_mat)

    ensembl=useMart("ensembl")
    listDatasets(ensembl)
    mouse = useMart("ensembl", dataset = "{mart_dataset}")
    human = useMart("ensembl", dataset = "hsapiens_gene_ensembl")

    hsa_entrezID = getLDS(attributes = "ensembl_gene_id",mart = mouse,  attributesL = "entrezgene", martL = human)
    Entrez.ID = character(length(ID))
    Entrez.ID[] = NA
    idx2 = match(ID,hsa_entrezID$Gene.ID)
    idx1 = which(!is.na(idx2)); idx2= idx2[idx1]
    Entrez.ID[idx1] = hsa_entrezID$EntrezGene.ID[idx2];
    Entrez.ID = as.numeric(Entrez.ID)

    ###Human gene symbols
    hsa_symbol = getLDS(attributes = "ensembl_gene_id",mart = mouse,  attributesL = "hgnc_symbol", martL = human)
    Anno.Symbol = character(length(ID))
    Anno.Symbol[] = NA
    idx2 = match(ID,hsa_symbol$Gene.ID)
    idx1 = which(!is.na(idx2)); idx2= idx2[idx1]
    Anno.Symbol[idx1] = unlist(hsa_symbol$HGNC.symbol[idx2]);
    # assemble IDs for annotation
    backgroundset = as.data.table(cbind(ID, Entrez.ID, Anno.Symbol))
    # assemble signatures


    reg_ls = regress_lsls$gene.counts${path_norms}
    fac = names(reg_ls$q_list)[grep(lmBy,names(reg_ls$q_list))]
    rowmask = select_lsmk[[{path_type}]][[{path_norms}]][[paste(fac,ngenes,sep=":")]]
    sig_gmt = NULL
    betas = reg_ls$b_mat[,fac]
    for (k in names(masks)){
        sig_gmt = NULL
        rowmask = select_lsmk[[mytypes[i]]][[mynorms[j]]][[k]]
        sig_gmt[['all']] =  ID[rowmask]
        fileSettings = list(directory='{read_dir}',baseFilename=paste(k,'with_Uni',sep='_')
        path_ls = ags.wrapper(setlist=sig_gmt, backgroundset, include.identifiers=TRUE, anno.uni=TRUE,
                              fileSettings = fileSettings,
                              functiondir = "{code_dir}/AssociationFunctions/PathwayAnalysis.R",
                              resourcedir = "/home/exacloud/lustre1/BioCoders/DataResources/AnnotationSources/old_association_resources",
                              return.OM=TRUE, ecut=0.05, ocut=5)
        fileSettings = list(directory='{read_dir}',baseFilename=paste(k,'with_out_Uni',sep='_')
        pathi_ls = ags.wrapper(setlist=sig_gmt, backgroundset, include.identifiers=TRUE, anno.uni=FALSE,
                       fileSettings = fileSettings,
                       functiondir = "{code_dir}/AssociationFunctions/PathwayAnalysis.R",
                       resourcedir = "/home/exacloud/lustre1/BioCoders/DataResources/AnnotationSources/old_association_resources",
                       return.OM=TRUE, ecut=0.05, ocut=5)
    }
    gtf.Rdir = "{gtf_read_dir}"
    out_norm_mat = LoM.norms$gene.counts$loess[rowmask_ls${path_type}${path_norms},]
    out_table = outputTable(normmat= out_norm_mat, gtf.file = "{gtfFile}",ratiomat = ratio_lsmat${path_type}${path_norms}, q_list=reg_ls$q_list)
    write.csv(out_table, row.names = FALSE, file=paste({project_title},{path_norms},'Normed_with_Ratio_and_Abundance.csv',sep='_'),quote=FALSE)
    """
    reformatted_code = textwrap.dedent(code).strip()
    code_context = {"code_dir":code_dir, "meta_file":meta_file, "taxID":taxID, "gtfFile":gtfFile, "gtf_feature":gtf_feature, "project_title":project_title, "gtf_read_dir":gtf_read_dir,"read_dir":read_dir, "read_pattern":read_pattern, "useme_cols":useme_cols, "lmBy":lmBy, "baseline":baseline, "path_type":path_type, "path_norms":path_norms, "mart_dataset":mart_dataset, "label_from_colname":label_from_colname}
    out_f.write(reformatted_code.format(**code_context))
    out_f.close()
2/30: reformatted_code
2/31: print reformatted_code
2/32: textwrap.fill(reformatted_code)
2/33: print textwrap.fill(reformatted_code)
2/34: print textwrap.fill(reformatted_code, width=200)
2/35: print reformatted_code
2/36: test = reformatted_code.format(**code_context)
2/37: test
2/38:
    code ="""
    require(data.table)
    require(NMF)
    require(affy)
    require(limma)
    require(AnnotationDbi)
    require(biomaRt)
    source("{code_dir}/AbundanceFunctions/BiasReduce.R")
    source("{code_dir}/AbundanceFunctions/ExtractTransformLoad.R")
    source("{code_dir}/AbundanceFunctions/DifferentialAnalysis.R")
    source("{code_dir}/AbundanceFunctions/NonVisualOutput.R")
    source("{code_dir}/GenomicsFunctions/ReadAndParse.R")
    source("{code_dir}/AssociationFunctions/gs.wrapper.R")
    source("{code_dir}/AssociationFunctions/PathwayAnalysis.R")
    source("{code_dir}/BcorePlotting/SummaryPlots.R")
    source("{code_dir}/BcorePlotting/MultipleTestingCorrection.R")
    source("{code_dir}/BcorePlotting/ClusteringPlots.R")


    setwd({read_dir})

    # constants
    # max data rows for hclust
    hclust.limit = 2^16

    # quantile of data distribution reqd in one group's worth of data if too many rows for hclust()
    hc.frac.cut = 0.75;
    SJ.counts.na.frac = 0.25;
    # max fraction of samples not having detected a splice junction for the splice
    # junction to be retained in raw data


    # regression parameters
    na.lim = 0 # max NAs per row tolerated by lm() at least in some cases
    do.not.regress = "alograw" # control norm not to be used for regression stats
    # plotting colors
    colors.rgb = c(rgb(0,0,0),rgb(0.1,0.1,1),rgb(0,.7,.7),rgb(0,.7,0),rgb(.7,1,0),rgb(.7,0,.7))
    md.file = {meta_file}
    md.orientation = "byRow" # sampleIDs are in @ row. alt:byCol (IDs in @ col)
    md.IDcol = "SampleID" # reqd if md.orientation is byRow; byCol==headers are IDs

    # gene annotation
    taxID = {taxID}
    gene2ENSfile = "/home/exacloud/lustre1/BioCoders/DataResources/AnnotationSources/ncbi/gene2ensembl.gz"
    gene2ENS.col = c("taxID","EntrezID","Gene","RefSeqTranscript","EnsemblTranscript","RefSeqProtein","EnsemblProtein")
    gtfFile = "{gtfFile}"
    gtf.feature = "{gtf_feature}"
    gtf.orig.col = c("gene_id","gene_name","gene_biotype")
    gtf.col = c("Gene","Symbol","biotype")
    proj.title = "{project_title}"
    gtf.Rdir = "{gtf_read_dir}"

    readdir = "{read_dir}"
    readpattern = "{read_pattern}"
    useme.cols = "{useme_cols}"
    label.from.colname = "{label_from_colname}"
    samps = dir(path=readdir, pattern=readpattern)
    samp.labels = gsub(label.from.colname,'\\1', samps)

    annCol.names = 'group"
    annCol.lmBy = "{lmBy}"
    annCol.label = "{label_from_colname}"

    # read in STAR alignments
    STAR.data = read_STAR(useme.cols=useme.cols,label.from.colname=label.from.colname,annCol.label=annCol.label,annCol.names=annCol.names,annCol.normBy=NULL,annCol.lmBy=annCol.lmBy, readpattern=readpattern, unstranded.col=list(gene.counts=c(1:4), SJ.counts=c(1:3,7)))

    # filter out SJs with too many NAs
    if(any( names(STAR.data$LoM.raw)=="SJ.counts" & exists("SJ.counts.na.frac") )){{
      STAR.data$SJ.counts.orig = STAR.data$LoM.raw$SJ.counts
      for(tag in names(STAR.data$LoM.raw$SJ.counts) ){{
        STAR.data$LoM.raw$SJ.counts[[tag]] = STAR.data$LoM.raw$SJ.counts[[tag]][rowSums(is.na(STAR.data$LoM.raw$SJ.counts[[tag]]))<= (SJ.counts.na.frac*ncol(STAR.data$LoM.raw$SJ.counts[[tag]])), ]
      }}
    }}

    gtf = readENSgtf(filename=gtfFile)
    genes.gtf = gtf[feature==gtf.feature, mget(gtf.orig.col)]
    names(genes.gtf) = gtf.col
    setkeyv(genes.gtf,gtf.col[1])

    ##### parse of NCBI's EntrezID to Ensembl translation
    Entrez2Ensembl = fread(paste("zgrep",paste0("-E '^",taxID,"'"),gene2ENSfile))
    names(Entrez2Ensembl) = gene2ENS.col
    setkeyv(Entrez2Ensembl,gene2ENS.col[3])

    ##### add EntrezIDs to genes.gtf
    tmp = Entrez2Ensembl[,mget(gene2ENS.col[2:3])]; tmp=tmp[!duplicated(tmp),]
    genes.gtf = merge(genes.gtf,tmp,all.x=TRUE)
    rm(tmp)

    myreads = STAR.data$myreads
    LoM.norms = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(LoM.norms) = names(STAR.data$LoM.raw)
    for(tag in names(STAR.data$LoM.raw) ){{
      if( length(STAR.data$LoM.raw[[tag]])>1 ){{
        LoM.norms[[tag]] = normMatrix(tag=tag,raw.mat=STAR.data$LoM.raw[[tag]][[myreads]])
      }} else {{
        LoM.norms[[tag]] = normMatrix(tag=tag,raw.mat=STAR.data$LoM.raw[[tag]][[1]])
      }}
    }}


    ## custom: reannotate from metadata
    ## run if necessary metadata is in file and not also in FASTQ file names
    md.dt = fread({meta_file}})
    # map annotation to read matrix
    if( md.orientation == "byCol" ){{ # samples are one per column
      idx2 = match( colnames(STAR.data$LoM.raw[[1]][[myreads]]), names(md.dt) )
    }} else {{ # samples are one per row
      idx2 = match( colnames(STAR.data$LoM.raw[[1]][[myreads]]), md.dt[,get(md.IDcol)] )
    }}
    idx1 = which(!is.na(idx2)); idx2 = idx2[idx1]
    if( sum(idx1==1:ncol(STAR.data$LoM.raw[[1]][[myreads]]))==ncol(STAR.data$LoM.raw[[1]][[myreads]]) ){{
      if( md.orientation == "byCol" ){{ # samples are one per column
        annCol = NULL
        namecol = setdiff( 1:ncol(md.dt), idx2 )
        md.factors = as.character(md.dt[,namecol,with=F])
        for(k in 1:nrow(md.dt) ){{
          annCol[[ md.dt[k,get(names(md.dt)[namecol])] ]][idx1] =
          as.vector( md.dt[ k, mget(names(md.dt)[idx2]) ] )
        }}
      }} else {{ # samples are one per row
        annCol = NULL
        namecol = setdiff( names(md.dt), md.IDcol )
        md.factors = as.character(namecol)
        for( k in setdiff(names(md.dt),md.IDcol) ){{
          annCol[[ k ]][idx1] =
          as.vector( md.dt[ idx2, get(k) ] )
        }}
      }}
    }} else {{
      stop(paste("Some samples have no annotation in",md.file))
    }}


    mytypes = names(LoM.norms)
    mynorms = names(LoM.norms[[1]])

    grpBy = annCol [[annCol.lmBy]]
    annCol.plotme = c(annCol.lmBy)
    clim.pct=0.96
    histbins=20
    for(i in 1:length(mytypes) ){{
      for(j in 1:length(mynorms)){{
        # set up matrices and config for plotting
        if( length(STAR.data$LoM.raw[[ mytypes[i] ]])>1 ){{
          rawmat = STAR.data$LoM.raw[[ mytypes[i] ]][[myreads]]
        }} else {{
          rawmat = STAR.data$LoM.raw[[ mytypes[i] ]][[1]]
        }}
        normmat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(rawmat)=samp.labels; colnames(normmat)=samp.labels
        plotdata = list(plotdir='.',plotbase=paste(mynorms[j],mytypes[i],sep='.'),plottitle=proj.title)
        rowmask = rowSums(rawmat>1,na.rm=T) > (ncol(rawmat)/length(unique(grpBy)) ) & rowSums(is.na(normmat)) < (ncol(normmat)/length(unique(grpBy)))
        if(sum(rowmask)>hclust.limit){{ #hack! build optimization!
          rowmask = rowSums(rawmat>=quantile(rawmat,probs=hc.frac.cut,na.rm=T),na.rm=T) > (ncol(rawmat)/length(unique(grpBy)) ) & rowSums(is.na(normmat)) < (ncol(normmat)/length(unique(grpBy)) )
        }}
        ans = summary.plots(rawmat=log2(rawmat +1), normmat=normmat, mynorm=mynorms[j], samp.labels=samp.labels, samp.classes=grpBy, plotdata=plotdata,plot2file=TRUE,histbins=histbins, colorspec=colors.rgb)
        ans = qc.clusters(rawmat=log2(rawmat[rowmask,] +1), normmat=normmat[rowmask,], attribs=annCol[annCol.plotme], oneclass=annCol.lmBy, colorspec=colors.rgb, plotdata=plotdata, plot2file=TRUE, clim.pct=clim.pct)
      }}
    }}

    regress_lsls = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(regress_lsls) = names(STAR.data$LoM.raw)
    contr_ls = list("Condition"=list(baseline="{baseline}",contr.FUN="contr.treatment"))
    # set baseline for regression in parameter(s) of interest
    # contr.treatment generates regression coefficients that are like (adjusted mean) ratios of other groups to the baseline group.
    # contr.sum generates coefficients that are like (adjusted mean) ratios to average all for all but the mandadory ommitted treatment group (because there is always one fewer independent pairwise comparison than there are pairs).

    lm_expr = "y ~ {lmBy}"
    rowmask_ls = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(rowmask_ls) = names(STAR.data$LoM.raw)
    for(i in 1:length(mytypes) ){{
      for(j in which(!mynorms %in% do.not.regress) ){{
        # set up data matrices
        if( length(STAR.data$LoM.raw[[ mytypes[i] ]])>1 ){{
          rawmat = STAR.data$LoM.raw[[ mytypes[i] ]][[myreads]]
        }} else {{
          rawmat = STAR.data$LoM.raw[[ mytypes[i] ]][[1]]
        }}
        tmp = unlist(lapply(contr_ls,function(x){{x$baseline}}))
        tmp=paste(names(tmp),tmp,sep=".")

        plotdata = list(plotdir='./',plotbase=paste(mynorms[j],mytypes[i],'vs',tmp,sep='.'),plottitle=proj.title)
        normmat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(rawmat)=samp.labels; colnames(normmat)=samp.labels

        # prepare rowmask for heatmap/MDS (remove non-expr or low expr>hclustlim)
        rowmask = rowSums(rawmat>1,na.rm=T) > (ncol(rawmat)/length(unique(grpBy)) ) & rowSums(is.na(normmat)) <= na.lim
        rowmask_ls[[ mytypes[i] ]][[ mynorms[j] ]] = rowmask # save for later

        # regression

        regress_lsls[[mytypes[i]]][[mynorms[j]]] = regressMatrix(normmat[rowmask,], expt.design=annCol[annCol.lmBy],
        lm_expression=lm_expr, contr_list = contr_ls, plot2file = TRUE, plotdata = plotdata)
      }}
    }}

    topn=500 # number with which to play
    for( i in 1:length(mytypes) ){{
      for( j in which(!mynorms %in% do.not.regress) ){{
        ans = regress_lsls[[mytypes[i]]][[mynorms[j]]]$q_list
        cat(mytypes[i],mynorms[j],"1-p0:",signif(unlist(lapply(2:length(ans),function(x){{1-ans[[x]]$pi0}})),2),"\n")
        cat(mytypes[i],mynorms[j],"qcut:",signif(unlist(lapply(2:length(ans),function(x){{quantile(ans[[x]]$qvalues,probs=topn/length(ans[[x]]$qvalues))}})),3),
        "returns ~",topn,"out of",length(ans[[2]]$qvalues),"\n")
    }}}}

    # Plots for each design factor (default) or for factors specified in facSel
    #   1) Histogram of p values that were included in the design
    #      Look for a peak on the left, and no peaks in the middle or on the right
    #   2) qvalue's default plots, with full qvalue range c(0,1) plotted
    #      Look for descent to pi0 in the top left tuning plot with good asymptote
    #      The slow/steep rise in q-values in remaining plots depends on resolving power of data


    lmBy = annCol.lmBy
    histbins=20
    for( i in 1:length(mytypes) ){{
      for( j in which(!mynorms %in% do.not.regress) ){{
        normmat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(normmat)=samp.labels
        reg_ls = regress_lsls[[mytypes[i]]][[mynorms[j]]]
        mymask= rowmask_ls[[mytypes[i]]][[mynorms[j]]]
        # pull out regression design and set up plot config
        tmp = unlist(lapply(contr_ls,function(x){{x$baseline}}))
        tmp=paste(names(tmp),tmp,sep=".")
        plotdata = list(plotdir='./',plotbase=paste(mynorms[j],mytypes[i],tmp,sep='.'),plottitle=proj.title)
        for(fac in names(reg_ls$q_list)[grep(lmBy,names(reg_ls$q_list))] ){{
          pdata = plotdata
          pdata$plotbase = paste(plotdata$plotbase,make.names(fac),sep='.')
          ans = qcQvalues(norm_x=normmat[mymask,], pvalue_v=reg_ls$p_mat[,fac], obj_qvalue=reg_ls$q_list[[fac]], attribs=annCol[lmBy],
          oneclass=lmBy, plotdata=pdata, colorspec=colors.rgb, histbins=histbins, plot2file=TRUE)
        }}
      }}
    }}

    # Build of ratios to baselines for desired design factors
    # Also build masks selecting genes based on q-values per factor
    #  and follow-up considerations such as expression level and fold change
    # Loop over q-value cuts to assist with final cut selection.
    # Plot heatmaps and MDS plots based on selected genes and designed ratios


    ngene_v = c(200,500,1000) # q-value cuts by number; can also cut by q-value
    ratioby_ls = list("Condition"=contr_ls$Condition$baseline)
    ratio_fold = 1.3
    intensity_fold = 2

    cut_ls = list(q_combine="OR", rcut_fold=ratio_fold, icut_fold=intensity_fold)
    # settings for heatmaps
    annCol.plotme = annCol.lmBy # heatmap tracks
    clustrowmin = 10 # min data rows for heatmap and MDS plots
    # save ratios and selections
    select_lsmk = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(select_lsmk) = names(STAR.data$LoM.raw)
    ratio_lsmat = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(ratio_lsmat) = names(STAR.data$LoM.raw)
    for( i in 1:length(mytypes) ){{
      for( j in which(!mynorms %in% do.not.regress) ){{
        normmat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(normmat)=samp.labels
        if( grepl('SJ',mytypes[i]) ){{ #not the most robust way to find SJs
          # map gene IDs back to data matrix using mapping built earlier
          # this enables include_ID to select rows
          idx2 = match( rownames(STAR.data$LoM.raw[[ mytypes[i] ]][[1]]), mySJ_dt[,get(pos.col)] )
          idx1 = which(!is.na(idx2)); idx2 = idx2[idx1]
          rownames(normmat)[idx1] = mySJ_dt[idx2,get(gtf.col[1])]
        }}
        mymask= rowmask_ls[[mytypes[i]]][[mynorms[j]]]
        reg_ls = regress_lsls[[mytypes[i]]][[mynorms[j]]]$q_list
        reg_ls = reg_ls[!grepl('Intercept',names(reg_ls))]
        plotdata = list(plotdir='./',plotbase=paste(mynorms[j],sub('counts','ratios',mytypes[i]),'minratio',paste0(ratio_fold,'x'),'minexpr',round(min(normmat,na.rm=T)+log2(intensity_fold),1),sep='.'),plottitle=proj.title)
        for( ngenes in ngene_v ) {{
          # calculate and plot ratios
          cut_ls$qcut = ngenes
          pdata = plotdata
          pdata$plotbase = paste(plotdata$plotbase,'ratio.vs',ratioby_ls$strain,ngenes,sep='.')
          # this function makes the ratios and cuts
          ans = designRatios(normmat[mymask,], q_list=reg_ls, attribs=annCol[annCol.plotme], ratioby_ls=ratioby_ls, cut_ls=cut_ls)
          # this function creates the heatmap and MDS plot
          if( sum(ans$rowmask)>clustrowmin ){{
            ans2= plotRatios( ratiomat=ans$ratiomat, attribs=annCol[annCol.plotme], oneclass=lmBy, plotdata=pdata, colorspec=colors.rgb, rowmask=ans$rowmask, plot2file=TRUE)
          }}
          # save selections
          select_lsmk[[mytypes[i]]][[mynorms[j]]][paste(fac,ngenes,sep=":")] = ans['rowmask']
        }}
        # save ratiomat (not dependent on ngene cut) after ngene loop
        ratio_lsmat[[mytypes[i]]][[mynorms[j]]] = ans$ratiomat
      }}
    }}

    dir.create(file.path(getwd(),'PairwiseScatter'),showWarnings=FALSE)
    for( i in 1:length(mytypes) ){{
      for( j in 1:length(mynorms) ){{
        scatter_mat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(normmat)=samp.labels
        plotdata = list(plotdir='./PairwiseScatter',plotbase=paste(mynorms[j],mytypes[i],'Scatter',lmBy,sep=':'),plottitle=proj.title)
        scatterplot(scatter_mat, attribs=annCol[[lmBy]],plotdata=plotdata,plot2file = TRUE)
      }}
    }}


    ratio_mat = ratio_lsmat[[{path_type}]][[{path_norms}]]
    reg_ls = regress_lsls[[{path_type}]][[{path_norms}]]
    ID = rownames(ratio_mat)

    ensembl=useMart("ensembl")
    listDatasets(ensembl)
    mouse = useMart("ensembl", dataset = "{mart_dataset}")
    human = useMart("ensembl", dataset = "hsapiens_gene_ensembl")

    hsa_entrezID = getLDS(attributes = "ensembl_gene_id",mart = mouse,  attributesL = "entrezgene", martL = human)
    Entrez.ID = character(length(ID))
    Entrez.ID[] = NA
    idx2 = match(ID,hsa_entrezID$Gene.ID)
    idx1 = which(!is.na(idx2)); idx2= idx2[idx1]
    Entrez.ID[idx1] = hsa_entrezID$EntrezGene.ID[idx2];
    Entrez.ID = as.numeric(Entrez.ID)

    ###Human gene symbols
    hsa_symbol = getLDS(attributes = "ensembl_gene_id",mart = mouse,  attributesL = "hgnc_symbol", martL = human)
    Anno.Symbol = character(length(ID))
    Anno.Symbol[] = NA
    idx2 = match(ID,hsa_symbol$Gene.ID)
    idx1 = which(!is.na(idx2)); idx2= idx2[idx1]
    Anno.Symbol[idx1] = unlist(hsa_symbol$HGNC.symbol[idx2]);
    # assemble IDs for annotation
    backgroundset = as.data.table(cbind(ID, Entrez.ID, Anno.Symbol))
    # assemble signatures


    reg_ls = regress_lsls$gene.counts${path_norms}
    fac = names(reg_ls$q_list)[grep(lmBy,names(reg_ls$q_list))]
    rowmask = select_lsmk[[{path_type}]][[{path_norms}]][[paste(fac,ngenes,sep=":")]]
    sig_gmt = NULL
    betas = reg_ls$b_mat[,fac]
    for (k in names(masks)){{
        sig_gmt = NULL
        rowmask = select_lsmk[[mytypes[i]]][[mynorms[j]]][[k]]
        sig_gmt[['all']] =  ID[rowmask]
        fileSettings = list(directory='{read_dir}',baseFilename=paste(k,'with_Uni',sep='_')
        path_ls = ags.wrapper(setlist=sig_gmt, backgroundset, include.identifiers=TRUE, anno.uni=TRUE,
                              fileSettings = fileSettings,
                              functiondir = "{code_dir}/AssociationFunctions/PathwayAnalysis.R",
                              resourcedir = "/home/exacloud/lustre1/BioCoders/DataResources/AnnotationSources/old_association_resources",
                              return.OM=TRUE, ecut=0.05, ocut=5)
        fileSettings = list(directory='{read_dir}',baseFilename=paste(k,'with_out_Uni',sep='_')
        pathi_ls = ags.wrapper(setlist=sig_gmt, backgroundset, include.identifiers=TRUE, anno.uni=FALSE,
                       fileSettings = fileSettings,
                       functiondir = "{code_dir}/AssociationFunctions/PathwayAnalysis.R",
                       resourcedir = "/home/exacloud/lustre1/BioCoders/DataResources/AnnotationSources/old_association_resources",
                       return.OM=TRUE, ecut=0.05, ocut=5)
    }}
    gtf.Rdir = "{gtf_read_dir}"
    out_norm_mat = LoM.norms$gene.counts$loess[rowmask_ls${path_type}${path_norms},]
    out_table = outputTable(normmat= out_norm_mat, gtf.file = "{gtfFile}",ratiomat = ratio_lsmat${path_type}${path_norms}, q_list=reg_ls$q_list)
    write.csv(out_table, row.names = FALSE, file=paste({project_title},{path_norms},'Normed_with_Ratio_and_Abundance.csv',sep='_'),quote=FALSE)
    """
    reformatted_code = textwrap.dedent(code).strip()
    code_context = {"code_dir":code_dir, "meta_file":meta_file, "taxID":taxID, "gtfFile":gtfFile, "gtf_feature":gtf_feature, "project_title":project_title, "gtf_read_dir":gtf_read_dir,"read_dir":read_dir, "read_pattern":read_pattern, "useme_cols":useme_cols, "lmBy":lmBy, "baseline":baseline, "path_type":path_type, "path_norms":path_norms, "mart_dataset":mart_dataset, "label_from_colname":label_from_colname}
    out_f.write(reformatted_code.format(**code_context))
    out_f.close()
2/39:
    code ="""
    require(data.table)
    require(NMF)
    require(affy)
    require(limma)
    require(AnnotationDbi)
    require(biomaRt)
    source("{code_dir}/AbundanceFunctions/BiasReduce.R")
    source("{code_dir}/AbundanceFunctions/ExtractTransformLoad.R")
    source("{code_dir}/AbundanceFunctions/DifferentialAnalysis.R")
    source("{code_dir}/AbundanceFunctions/NonVisualOutput.R")
    source("{code_dir}/GenomicsFunctions/ReadAndParse.R")
    source("{code_dir}/AssociationFunctions/gs.wrapper.R")
    source("{code_dir}/AssociationFunctions/PathwayAnalysis.R")
    source("{code_dir}/BcorePlotting/SummaryPlots.R")
    source("{code_dir}/BcorePlotting/MultipleTestingCorrection.R")
    source("{code_dir}/BcorePlotting/ClusteringPlots.R")


    setwd({read_dir})

    # constants
    # max data rows for hclust
    hclust.limit = 2^16

    # quantile of data distribution reqd in one group's worth of data if too many rows for hclust()
    hc.frac.cut = 0.75;
    SJ.counts.na.frac = 0.25;
    # max fraction of samples not having detected a splice junction for the splice
    # junction to be retained in raw data


    # regression parameters
    na.lim = 0 # max NAs per row tolerated by lm() at least in some cases
    do.not.regress = "alograw" # control norm not to be used for regression stats
    # plotting colors
    colors.rgb = c(rgb(0,0,0),rgb(0.1,0.1,1),rgb(0,.7,.7),rgb(0,.7,0),rgb(.7,1,0),rgb(.7,0,.7))
    md.file = {meta_file}
    md.orientation = "byRow" # sampleIDs are in @ row. alt:byCol (IDs in @ col)
    md.IDcol = "SampleID" # reqd if md.orientation is byRow; byCol==headers are IDs

    # gene annotation
    taxID = {taxID}
    gene2ENSfile = "/home/exacloud/lustre1/BioCoders/DataResources/AnnotationSources/ncbi/gene2ensembl.gz"
    gene2ENS.col = c("taxID","EntrezID","Gene","RefSeqTranscript","EnsemblTranscript","RefSeqProtein","EnsemblProtein")
    gtfFile = "{gtfFile}"
    gtf.feature = "{gtf_feature}"
    gtf.orig.col = c("gene_id","gene_name","gene_biotype")
    gtf.col = c("Gene","Symbol","biotype")
    proj.title = "{project_title}"
    gtf.Rdir = "{gtf_read_dir}"

    readdir = "{read_dir}"
    readpattern = "{read_pattern}"
    useme.cols = "{useme_cols}"
    label.from.colname = "{label_from_colname}"
    samps = dir(path=readdir, pattern=readpattern)
    samp.labels = gsub(label.from.colname,'\\1', samps)

    annCol.names = 'group"
    annCol.lmBy = "{lmBy}"
    annCol.label = "{label_from_colname}"

    # read in STAR alignments
    STAR.data = read_STAR(useme.cols=useme.cols,label.from.colname=label.from.colname,annCol.label=annCol.label,annCol.names=annCol.names,annCol.normBy=NULL,annCol.lmBy=annCol.lmBy, readpattern=readpattern, unstranded.col=list(gene.counts=c(1:4), SJ.counts=c(1:3,7)))

    # filter out SJs with too many NAs
    if(any( names(STAR.data$LoM.raw)=="SJ.counts" & exists("SJ.counts.na.frac") )){{
      STAR.data$SJ.counts.orig = STAR.data$LoM.raw$SJ.counts
      for(tag in names(STAR.data$LoM.raw$SJ.counts) ){{
        STAR.data$LoM.raw$SJ.counts[[tag]] = STAR.data$LoM.raw$SJ.counts[[tag]][rowSums(is.na(STAR.data$LoM.raw$SJ.counts[[tag]]))<= (SJ.counts.na.frac*ncol(STAR.data$LoM.raw$SJ.counts[[tag]])), ]
      }}
    }}

    gtf = readENSgtf(filename=gtfFile)
    genes.gtf = gtf[feature==gtf.feature, mget(gtf.orig.col)]
    names(genes.gtf) = gtf.col
    setkeyv(genes.gtf,gtf.col[1])

    ##### parse of NCBI's EntrezID to Ensembl translation
    Entrez2Ensembl = fread(paste("zgrep",paste0("-E '^",taxID,"'"),gene2ENSfile))
    names(Entrez2Ensembl) = gene2ENS.col
    setkeyv(Entrez2Ensembl,gene2ENS.col[3])

    ##### add EntrezIDs to genes.gtf
    tmp = Entrez2Ensembl[,mget(gene2ENS.col[2:3])]; tmp=tmp[!duplicated(tmp),]
    genes.gtf = merge(genes.gtf,tmp,all.x=TRUE)
    rm(tmp)

    myreads = STAR.data$myreads
    LoM.norms = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(LoM.norms) = names(STAR.data$LoM.raw)
    for(tag in names(STAR.data$LoM.raw) ){{
      if( length(STAR.data$LoM.raw[[tag]])>1 ){{
        LoM.norms[[tag]] = normMatrix(tag=tag,raw.mat=STAR.data$LoM.raw[[tag]][[myreads]])
      }} else {{
        LoM.norms[[tag]] = normMatrix(tag=tag,raw.mat=STAR.data$LoM.raw[[tag]][[1]])
      }}
    }}


    ## custom: reannotate from metadata
    ## run if necessary metadata is in file and not also in FASTQ file names
    md.dt = fread({meta_file})
    # map annotation to read matrix
    if( md.orientation == "byCol" ){{ # samples are one per column
      idx2 = match( colnames(STAR.data$LoM.raw[[1]][[myreads]]), names(md.dt) )
    }} else {{ # samples are one per row
      idx2 = match( colnames(STAR.data$LoM.raw[[1]][[myreads]]), md.dt[,get(md.IDcol)] )
    }}
    idx1 = which(!is.na(idx2)); idx2 = idx2[idx1]
    if( sum(idx1==1:ncol(STAR.data$LoM.raw[[1]][[myreads]]))==ncol(STAR.data$LoM.raw[[1]][[myreads]]) ){{
      if( md.orientation == "byCol" ){{ # samples are one per column
        annCol = NULL
        namecol = setdiff( 1:ncol(md.dt), idx2 )
        md.factors = as.character(md.dt[,namecol,with=F])
        for(k in 1:nrow(md.dt) ){{
          annCol[[ md.dt[k,get(names(md.dt)[namecol])] ]][idx1] =
          as.vector( md.dt[ k, mget(names(md.dt)[idx2]) ] )
        }}
      }} else {{ # samples are one per row
        annCol = NULL
        namecol = setdiff( names(md.dt), md.IDcol )
        md.factors = as.character(namecol)
        for( k in setdiff(names(md.dt),md.IDcol) ){{
          annCol[[ k ]][idx1] =
          as.vector( md.dt[ idx2, get(k) ] )
        }}
      }}
    }} else {{
      stop(paste("Some samples have no annotation in",md.file))
    }}


    mytypes = names(LoM.norms)
    mynorms = names(LoM.norms[[1]])

    grpBy = annCol [[annCol.lmBy]]
    annCol.plotme = c(annCol.lmBy)
    clim.pct=0.96
    histbins=20
    for(i in 1:length(mytypes) ){{
      for(j in 1:length(mynorms)){{
        # set up matrices and config for plotting
        if( length(STAR.data$LoM.raw[[ mytypes[i] ]])>1 ){{
          rawmat = STAR.data$LoM.raw[[ mytypes[i] ]][[myreads]]
        }} else {{
          rawmat = STAR.data$LoM.raw[[ mytypes[i] ]][[1]]
        }}
        normmat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(rawmat)=samp.labels; colnames(normmat)=samp.labels
        plotdata = list(plotdir='.',plotbase=paste(mynorms[j],mytypes[i],sep='.'),plottitle=proj.title)
        rowmask = rowSums(rawmat>1,na.rm=T) > (ncol(rawmat)/length(unique(grpBy)) ) & rowSums(is.na(normmat)) < (ncol(normmat)/length(unique(grpBy)))
        if(sum(rowmask)>hclust.limit){{ #hack! build optimization!
          rowmask = rowSums(rawmat>=quantile(rawmat,probs=hc.frac.cut,na.rm=T),na.rm=T) > (ncol(rawmat)/length(unique(grpBy)) ) & rowSums(is.na(normmat)) < (ncol(normmat)/length(unique(grpBy)) )
        }}
        ans = summary.plots(rawmat=log2(rawmat +1), normmat=normmat, mynorm=mynorms[j], samp.labels=samp.labels, samp.classes=grpBy, plotdata=plotdata,plot2file=TRUE,histbins=histbins, colorspec=colors.rgb)
        ans = qc.clusters(rawmat=log2(rawmat[rowmask,] +1), normmat=normmat[rowmask,], attribs=annCol[annCol.plotme], oneclass=annCol.lmBy, colorspec=colors.rgb, plotdata=plotdata, plot2file=TRUE, clim.pct=clim.pct)
      }}
    }}

    regress_lsls = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(regress_lsls) = names(STAR.data$LoM.raw)
    contr_ls = list("Condition"=list(baseline="{baseline}",contr.FUN="contr.treatment"))
    # set baseline for regression in parameter(s) of interest
    # contr.treatment generates regression coefficients that are like (adjusted mean) ratios of other groups to the baseline group.
    # contr.sum generates coefficients that are like (adjusted mean) ratios to average all for all but the mandadory ommitted treatment group (because there is always one fewer independent pairwise comparison than there are pairs).

    lm_expr = "y ~ {lmBy}"
    rowmask_ls = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(rowmask_ls) = names(STAR.data$LoM.raw)
    for(i in 1:length(mytypes) ){{
      for(j in which(!mynorms %in% do.not.regress) ){{
        # set up data matrices
        if( length(STAR.data$LoM.raw[[ mytypes[i] ]])>1 ){{
          rawmat = STAR.data$LoM.raw[[ mytypes[i] ]][[myreads]]
        }} else {{
          rawmat = STAR.data$LoM.raw[[ mytypes[i] ]][[1]]
        }}
        tmp = unlist(lapply(contr_ls,function(x){{x$baseline}}))
        tmp=paste(names(tmp),tmp,sep=".")

        plotdata = list(plotdir='./',plotbase=paste(mynorms[j],mytypes[i],'vs',tmp,sep='.'),plottitle=proj.title)
        normmat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(rawmat)=samp.labels; colnames(normmat)=samp.labels

        # prepare rowmask for heatmap/MDS (remove non-expr or low expr>hclustlim)
        rowmask = rowSums(rawmat>1,na.rm=T) > (ncol(rawmat)/length(unique(grpBy)) ) & rowSums(is.na(normmat)) <= na.lim
        rowmask_ls[[ mytypes[i] ]][[ mynorms[j] ]] = rowmask # save for later

        # regression

        regress_lsls[[mytypes[i]]][[mynorms[j]]] = regressMatrix(normmat[rowmask,], expt.design=annCol[annCol.lmBy],
        lm_expression=lm_expr, contr_list = contr_ls, plot2file = TRUE, plotdata = plotdata)
      }}
    }}

    topn=500 # number with which to play
    for( i in 1:length(mytypes) ){{
      for( j in which(!mynorms %in% do.not.regress) ){{
        ans = regress_lsls[[mytypes[i]]][[mynorms[j]]]$q_list
        cat(mytypes[i],mynorms[j],"1-p0:",signif(unlist(lapply(2:length(ans),function(x){{1-ans[[x]]$pi0}})),2),"\n")
        cat(mytypes[i],mynorms[j],"qcut:",signif(unlist(lapply(2:length(ans),function(x){{quantile(ans[[x]]$qvalues,probs=topn/length(ans[[x]]$qvalues))}})),3),
        "returns ~",topn,"out of",length(ans[[2]]$qvalues),"\n")
    }}}}

    # Plots for each design factor (default) or for factors specified in facSel
    #   1) Histogram of p values that were included in the design
    #      Look for a peak on the left, and no peaks in the middle or on the right
    #   2) qvalue's default plots, with full qvalue range c(0,1) plotted
    #      Look for descent to pi0 in the top left tuning plot with good asymptote
    #      The slow/steep rise in q-values in remaining plots depends on resolving power of data


    lmBy = annCol.lmBy
    histbins=20
    for( i in 1:length(mytypes) ){{
      for( j in which(!mynorms %in% do.not.regress) ){{
        normmat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(normmat)=samp.labels
        reg_ls = regress_lsls[[mytypes[i]]][[mynorms[j]]]
        mymask= rowmask_ls[[mytypes[i]]][[mynorms[j]]]
        # pull out regression design and set up plot config
        tmp = unlist(lapply(contr_ls,function(x){{x$baseline}}))
        tmp=paste(names(tmp),tmp,sep=".")
        plotdata = list(plotdir='./',plotbase=paste(mynorms[j],mytypes[i],tmp,sep='.'),plottitle=proj.title)
        for(fac in names(reg_ls$q_list)[grep(lmBy,names(reg_ls$q_list))] ){{
          pdata = plotdata
          pdata$plotbase = paste(plotdata$plotbase,make.names(fac),sep='.')
          ans = qcQvalues(norm_x=normmat[mymask,], pvalue_v=reg_ls$p_mat[,fac], obj_qvalue=reg_ls$q_list[[fac]], attribs=annCol[lmBy],
          oneclass=lmBy, plotdata=pdata, colorspec=colors.rgb, histbins=histbins, plot2file=TRUE)
        }}
      }}
    }}

    # Build of ratios to baselines for desired design factors
    # Also build masks selecting genes based on q-values per factor
    #  and follow-up considerations such as expression level and fold change
    # Loop over q-value cuts to assist with final cut selection.
    # Plot heatmaps and MDS plots based on selected genes and designed ratios


    ngene_v = c(200,500,1000) # q-value cuts by number; can also cut by q-value
    ratioby_ls = list("Condition"=contr_ls$Condition$baseline)
    ratio_fold = 1.3
    intensity_fold = 2

    cut_ls = list(q_combine="OR", rcut_fold=ratio_fold, icut_fold=intensity_fold)
    # settings for heatmaps
    annCol.plotme = annCol.lmBy # heatmap tracks
    clustrowmin = 10 # min data rows for heatmap and MDS plots
    # save ratios and selections
    select_lsmk = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(select_lsmk) = names(STAR.data$LoM.raw)
    ratio_lsmat = vector(mode='list',length=length(STAR.data$LoM.raw))
    names(ratio_lsmat) = names(STAR.data$LoM.raw)
    for( i in 1:length(mytypes) ){{
      for( j in which(!mynorms %in% do.not.regress) ){{
        normmat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(normmat)=samp.labels
        if( grepl('SJ',mytypes[i]) ){{ #not the most robust way to find SJs
          # map gene IDs back to data matrix using mapping built earlier
          # this enables include_ID to select rows
          idx2 = match( rownames(STAR.data$LoM.raw[[ mytypes[i] ]][[1]]), mySJ_dt[,get(pos.col)] )
          idx1 = which(!is.na(idx2)); idx2 = idx2[idx1]
          rownames(normmat)[idx1] = mySJ_dt[idx2,get(gtf.col[1])]
        }}
        mymask= rowmask_ls[[mytypes[i]]][[mynorms[j]]]
        reg_ls = regress_lsls[[mytypes[i]]][[mynorms[j]]]$q_list
        reg_ls = reg_ls[!grepl('Intercept',names(reg_ls))]
        plotdata = list(plotdir='./',plotbase=paste(mynorms[j],sub('counts','ratios',mytypes[i]),'minratio',paste0(ratio_fold,'x'),'minexpr',round(min(normmat,na.rm=T)+log2(intensity_fold),1),sep='.'),plottitle=proj.title)
        for( ngenes in ngene_v ) {{
          # calculate and plot ratios
          cut_ls$qcut = ngenes
          pdata = plotdata
          pdata$plotbase = paste(plotdata$plotbase,'ratio.vs',ratioby_ls$strain,ngenes,sep='.')
          # this function makes the ratios and cuts
          ans = designRatios(normmat[mymask,], q_list=reg_ls, attribs=annCol[annCol.plotme], ratioby_ls=ratioby_ls, cut_ls=cut_ls)
          # this function creates the heatmap and MDS plot
          if( sum(ans$rowmask)>clustrowmin ){{
            ans2= plotRatios( ratiomat=ans$ratiomat, attribs=annCol[annCol.plotme], oneclass=lmBy, plotdata=pdata, colorspec=colors.rgb, rowmask=ans$rowmask, plot2file=TRUE)
          }}
          # save selections
          select_lsmk[[mytypes[i]]][[mynorms[j]]][paste(fac,ngenes,sep=":")] = ans['rowmask']
        }}
        # save ratiomat (not dependent on ngene cut) after ngene loop
        ratio_lsmat[[mytypes[i]]][[mynorms[j]]] = ans$ratiomat
      }}
    }}

    dir.create(file.path(getwd(),'PairwiseScatter'),showWarnings=FALSE)
    for( i in 1:length(mytypes) ){{
      for( j in 1:length(mynorms) ){{
        scatter_mat= LoM.norms[[ mytypes[i] ]][[ mynorms[j] ]]
        colnames(normmat)=samp.labels
        plotdata = list(plotdir='./PairwiseScatter',plotbase=paste(mynorms[j],mytypes[i],'Scatter',lmBy,sep=':'),plottitle=proj.title)
        scatterplot(scatter_mat, attribs=annCol[[lmBy]],plotdata=plotdata,plot2file = TRUE)
      }}
    }}


    ratio_mat = ratio_lsmat[[{path_type}]][[{path_norms}]]
    reg_ls = regress_lsls[[{path_type}]][[{path_norms}]]
    ID = rownames(ratio_mat)

    ensembl=useMart("ensembl")
    listDatasets(ensembl)
    mouse = useMart("ensembl", dataset = "{mart_dataset}")
    human = useMart("ensembl", dataset = "hsapiens_gene_ensembl")

    hsa_entrezID = getLDS(attributes = "ensembl_gene_id",mart = mouse,  attributesL = "entrezgene", martL = human)
    Entrez.ID = character(length(ID))
    Entrez.ID[] = NA
    idx2 = match(ID,hsa_entrezID$Gene.ID)
    idx1 = which(!is.na(idx2)); idx2= idx2[idx1]
    Entrez.ID[idx1] = hsa_entrezID$EntrezGene.ID[idx2];
    Entrez.ID = as.numeric(Entrez.ID)

    ###Human gene symbols
    hsa_symbol = getLDS(attributes = "ensembl_gene_id",mart = mouse,  attributesL = "hgnc_symbol", martL = human)
    Anno.Symbol = character(length(ID))
    Anno.Symbol[] = NA
    idx2 = match(ID,hsa_symbol$Gene.ID)
    idx1 = which(!is.na(idx2)); idx2= idx2[idx1]
    Anno.Symbol[idx1] = unlist(hsa_symbol$HGNC.symbol[idx2]);
    # assemble IDs for annotation
    backgroundset = as.data.table(cbind(ID, Entrez.ID, Anno.Symbol))
    # assemble signatures


    reg_ls = regress_lsls$gene.counts${path_norms}
    fac = names(reg_ls$q_list)[grep(lmBy,names(reg_ls$q_list))]
    rowmask = select_lsmk[[{path_type}]][[{path_norms}]][[paste(fac,ngenes,sep=":")]]
    sig_gmt = NULL
    betas = reg_ls$b_mat[,fac]
    for (k in names(masks)){{
        sig_gmt = NULL
        rowmask = select_lsmk[[mytypes[i]]][[mynorms[j]]][[k]]
        sig_gmt[['all']] =  ID[rowmask]
        fileSettings = list(directory='{read_dir}',baseFilename=paste(k,'with_Uni',sep='_')
        path_ls = ags.wrapper(setlist=sig_gmt, backgroundset, include.identifiers=TRUE, anno.uni=TRUE,
                              fileSettings = fileSettings,
                              functiondir = "{code_dir}/AssociationFunctions/PathwayAnalysis.R",
                              resourcedir = "/home/exacloud/lustre1/BioCoders/DataResources/AnnotationSources/old_association_resources",
                              return.OM=TRUE, ecut=0.05, ocut=5)
        fileSettings = list(directory='{read_dir}',baseFilename=paste(k,'with_out_Uni',sep='_')
        pathi_ls = ags.wrapper(setlist=sig_gmt, backgroundset, include.identifiers=TRUE, anno.uni=FALSE,
                       fileSettings = fileSettings,
                       functiondir = "{code_dir}/AssociationFunctions/PathwayAnalysis.R",
                       resourcedir = "/home/exacloud/lustre1/BioCoders/DataResources/AnnotationSources/old_association_resources",
                       return.OM=TRUE, ecut=0.05, ocut=5)
    }}
    gtf.Rdir = "{gtf_read_dir}"
    out_norm_mat = LoM.norms$gene.counts$loess[rowmask_ls${path_type}${path_norms},]
    out_table = outputTable(normmat= out_norm_mat, gtf.file = "{gtfFile}",ratiomat = ratio_lsmat${path_type}${path_norms}, q_list=reg_ls$q_list)
    write.csv(out_table, row.names = FALSE, file=paste({project_title},{path_norms},'Normed_with_Ratio_and_Abundance.csv',sep='_'),quote=FALSE)
    """
    reformatted_code = textwrap.dedent(code).strip()
    code_context = {"code_dir":code_dir, "meta_file":meta_file, "taxID":taxID, "gtfFile":gtfFile, "gtf_feature":gtf_feature, "project_title":project_title, "gtf_read_dir":gtf_read_dir,"read_dir":read_dir, "read_pattern":read_pattern, "useme_cols":useme_cols, "lmBy":lmBy, "baseline":baseline, "path_type":path_type, "path_norms":path_norms, "mart_dataset":mart_dataset, "label_from_colname":label_from_colname}
    out_f.write(reformatted_code.format(**code_context))
    out_f.close()
2/40: sample_meta_data_list = ['test','color','dna','cool']
2/41: select_sample_meta_data = ['dna','cool']
2/42: idx = [sample_meta_data_list.index(i) for i in select_sample_meta_data]
2/43: idx
2/44: sample_meta_data_list[idx]
2/45: idx
2/46: sample_meta_data_list[2,3]
2/47: sample_meta_data_list[[2,3]]
2/48: sample_meta_data_list.index(idx)
2/49: import itemgetter
2/50: from operator import itemgetter
2/51: itemgetter(idx)(sample_meta_data_list)
2/52: itemgetter(*idx)(sample_meta_data_list)
2/53: itemgetter(idx)(sample_meta_data_list)
2/54: select_sample_meta_data = ['cool','dna']
2/55: idx = [sample_meta_data_list.index(i) for i in select_sample_meta_data]
2/56: idx
2/57: itemgetter(*idx)(sample_meta_data_list)
2/58: list(itemgetter(*idx)(sample_meta_data_list))
 3/1:
import os,sys
import textwrap
from operator import itemgetter
 3/2: data_dir = "/home/users/estabroj/projects/DPerunderai_RNA161116DP/results/STAR"
 3/3: sample_meta_data_list = ['Project','ID','Sample','Merged']
 3/4: select_meta_data_list = ['ID','Sample']
 3/5:
    sample_meta_data_list = list(sample_meta_data_list)
    select_meta_data_list = list(select_meta_data_list)
    try:
        intersect_check = set(sample_meta_data_list).issuperset(set(select_meta_data_list))
    except ValueError:
        print "[select_meta_data_list] values do not correspond with [sample_meta_data_list]", [i for i in select_meta_data_list if i not in sample_meta_data_list]
        sys.exit(1)
 3/6: intersect_check
 3/7:
        files = [f for f in os.listdir(data_dir) if f.startswith('RNA')]
        files.sort()
        project = files[0].split('_')[0]
        out_f = open(project + '_metadata.txt','w')
        out_f.write('\t'.join(sample_meta_data_list)+'\n')
        idx = [sample_meta_data_list.index(i) for i in select_meta_data_list]
        for f in files:
            metadata = f.split('_')
            pertinent = list(itemgetter(*idx)(metadata))
            out_f.write('\t'.join(pertinent)+'\n')
        out_f.close()
 3/8: idx
 3/9: select_meta_data_list = ['ID','Sample','Wombat']
3/10:
    sample_meta_data_list = list(sample_meta_data_list)
    select_meta_data_list = list(select_meta_data_list)
    try:
        intersect_check = set(sample_meta_data_list).issuperset(set(select_meta_data_list))
    except ValueError:
        print "[select_meta_data_list] values do not correspond with [sample_meta_data_list]", [i for i in select_meta_data_list if i not in sample_meta_data_list]
        sys.exit(1)
3/11: intersect_check
3/12:
    sample_meta_data_list = list(sample_meta_data_list)
    select_meta_data_list = list(select_meta_data_list)
    try:
        set(sample_meta_data_list).issuperset(set(select_meta_data_list))
    except ValueError:
        print "[select_meta_data_list] values do not correspond with [sample_meta_data_list]", [i for i in select_meta_data_list if i not in sample_meta_data_list]
3/13:
    sample_meta_data_list = list(sample_meta_data_list)
    select_meta_data_list = list(select_meta_data_list)
    try:
        intersect_check = set(sample_meta_data_list).issuperset(set(select_meta_data_list))

        if intersect_check:
            files = [f for f in os.listdir(data_dir) if f.startswith('RNA')]
            files.sort()
            project = files[0].split('_')[0]
            out_f = open(project + '_metadata.txt','w')
            out_f.write('\t'.join(select_meta_data_list)+'\n')
            idx = [sample_meta_data_list.index(i) for i in select_meta_data_list]
            for f in files:
                metadata = f.split('_')
                pertinent = list(itemgetter(*idx)(metadata))
                out_f.write('\t'.join(pertinent)+'\n')
            out_f.close()
    except ValueError:
        print "[select_meta_data_list] values do not correspond with [sample_meta_data_list]", [i for i in select_meta_data_list if i not in sample_meta_data_list]
3/14: intersect_check
3/15:
    sample_meta_data_list = list(sample_meta_data_list)
    select_meta_data_list = list(select_meta_data_list)
    try:
        True == set(sample_meta_data_list).issuperset(set(select_meta_data_list))
        files = [f for f in os.listdir(data_dir) if f.startswith('RNA')]
        files.sort()
        project = files[0].split('_')[0]
        out_f = open(project + '_metadata.txt','w')
        out_f.write('\t'.join(select_meta_data_list)+'\n')
        idx = [sample_meta_data_list.index(i) for i in select_meta_data_list]
        for f in files:
            metadata = f.split('_')
            pertinent = list(itemgetter(*idx)(metadata))
            out_f.write('\t'.join(pertinent)+'\n')
        out_f.close()
    except:
        print "[select_meta_data_list] values do not correspond with [sample_meta_data_list]", [i for i in select_meta_data_list if i not in sample_meta_data_list]
3/16:
    sample_meta_data_list = list(sample_meta_data_list)
    select_meta_data_list = list(select_meta_data_list)
    try:
        True == set(sample_meta_data_list).issuperset(set(select_meta_data_list))
        
        files = [f for f in os.listdir(data_dir) if f.startswith('RNA')]
        files.sort()
        project = files[0].split('_')[0]
        out_f = open(project + '_metadata.txt','w')
        out_f.write('\t'.join(select_meta_data_list)+'\n')
        idx = [sample_meta_data_list.index(i) for i in select_meta_data_list]
        for f in files:
            metadata = f.split('_')
            pertinent = list(itemgetter(*idx)(metadata))
            out_f.write('\t'.join(pertinent)+'\n')
        out_f.close()
    except ValueError:
        print "[select_meta_data_list] values do not correspond with [sample_meta_data_list]", [i for i in select_meta_data_list if i not in sample_meta_data_list]
3/17: sample_meta_data_list
3/18: sample_meta_data_list = 'Project','ID','Sample','Merged'
3/19: select_meta_data_list
3/20: select_meta_data_list = 'ID','Sample','Wombat'
3/21:
    sample_meta_data_list = list(sample_meta_data_list)
    select_meta_data_list = list(select_meta_data_list)
    try:
        True == set(sample_meta_data_list).issuperset(set(select_meta_data_list))

        files = [f for f in os.listdir(data_dir) if f.startswith('RNA')]
        files.sort()
        project = files[0].split('_')[0]
        out_f = open(project + '_metadata.txt','w')
        out_f.write('\t'.join(select_meta_data_list)+'\n')
        idx = [sample_meta_data_list.index(i) for i in select_meta_data_list]
        for f in files:
            metadata = f.split('_')
            pertinent = list(itemgetter(*idx)(metadata))
            out_f.write('\t'.join(pertinent)+'\n')
        out_f.close()
    except ValueError:
        print "[select_meta_data_list] values do not correspond with [sample_meta_data_list]", [i for i in select_meta_data_list if i not in sample_meta_data_list]
3/22: select_meta_data_list
3/23: metadata
3/24: pertinent
3/25: '-' in pertinent
3/26: '-' in pertinent[0]
3/27:                 hyphen_idx = [x for x in pertinent if '-' in x]
3/28: hyphen_idx
3/29: len(hyphen_idx)
3/30: split_hyphen =True
3/31:
    sample_meta_data_list = list(sample_meta_data_list)
    select_meta_data_list = list(select_meta_data_list)
    try:
        True == set(sample_meta_data_list).issuperset(set(select_meta_data_list))

        files = [f for f in os.listdir(data_dir) if f.startswith('RNA')]
        files.sort()
        project = files[0].split('_')[0]
        out_f = open(project + '_metadata.txt','w')

        if split_hyphen:
            out_f.write('\t'.join(select_meta_data_list) +'\t'+'ID_Group'+'\n')
        else:
            out_f.write('\t'.join(select_meta_data_list)+'\n')

        idx = [sample_meta_data_list.index(i) for i in select_meta_data_list]
        for f in files:
            metadata = f.split('_')
            pertinent = list(itemgetter(*idx)(metadata))
            if split_hyphen:
                hyphen_idx = [x for x in pertinent if '-' in x]
                if len(hyphen_idx) > 1:
                    print 'Multiple meta fields containing hyphens!'
                else:
                    pertinent.append(hyphen_idx[0])
                    out_f.write('\t'.join(pertinent)+'\n')
            else:
                out_f.write('\t'.join(pertinent) + '\n')
        out_f.close()
    except ValueError:
        print "[select_meta_data_list] values do not correspond with [sample_meta_data_list]", [i for i in select_meta_data_list if i not in sample_meta_data_list]
3/32: select_meta_data_list
3/33: select_meta_data_list.pop(2)
3/34:
    sample_meta_data_list = list(sample_meta_data_list)
    select_meta_data_list = list(select_meta_data_list)
    try:
        True == set(sample_meta_data_list).issuperset(set(select_meta_data_list))

        files = [f for f in os.listdir(data_dir) if f.startswith('RNA')]
        files.sort()
        project = files[0].split('_')[0]
        out_f = open(project + '_metadata.txt','w')

        if split_hyphen:
            out_f.write('\t'.join(select_meta_data_list) +'\t'+'ID_Group'+'\n')
        else:
            out_f.write('\t'.join(select_meta_data_list)+'\n')

        idx = [sample_meta_data_list.index(i) for i in select_meta_data_list]
        for f in files:
            metadata = f.split('_')
            pertinent = list(itemgetter(*idx)(metadata))
            if split_hyphen:
                hyphen_idx = [x for x in pertinent if '-' in x]
                if len(hyphen_idx) > 1:
                    print 'Multiple meta fields containing hyphens!'
                else:
                    pertinent.append(hyphen_idx[0])
                    out_f.write('\t'.join(pertinent)+'\n')
            else:
                out_f.write('\t'.join(pertinent) + '\n')
        out_f.close()
    except ValueError:
        print "[select_meta_data_list] values do not correspond with [sample_meta_data_list]", [i for i in select_meta_data_list if i not in sample_meta_data_list]
3/35:
    sample_meta_data_list = list(sample_meta_data_list)
    select_meta_data_list = list(select_meta_data_list)
    try:
        True == set(sample_meta_data_list).issuperset(set(select_meta_data_list))

        files = [f for f in os.listdir(data_dir) if f.startswith('RNA')]
        files.sort()
        project = files[0].split('_')[0]
        out_f = open(project + '_metadata.txt','w')

        if split_hyphen:
            out_f.write('\t'.join(select_meta_data_list) +'\t'+'ID_Group'+'\n')
        else:
            out_f.write('\t'.join(select_meta_data_list)+'\n')

        idx = [sample_meta_data_list.index(i) for i in select_meta_data_list]
        for f in files:
            metadata = f.split('_')
            pertinent = list(itemgetter(*idx)(metadata))
            if split_hyphen:
                hyphen_idx = [x for x in pertinent if '-' in x]
                if len(hyphen_idx) > 1:
                    print 'Multiple meta fields containing hyphens!'
                else:
                    pertinent.append(hyphen_idx[0].split('-')[0])
                    out_f.write('\t'.join(pertinent)+'\n')
            else:
                out_f.write('\t'.join(pertinent) + '\n')
        out_f.close()
    except ValueError:
        print "[select_meta_data_list] values do not correspond with [sample_meta_data_list]", [i for i in select_meta_data_list if i not in sample_meta_data_list]
 6/1: ls
 6/2: import pandas as pd
 6/3: kallisto = pd.DataFrame.read_csv('MMRF_CoMMpass_IA10c_E74GTF_Kallisto_Transcript_Counts.txt',sep='\t',index_col=0)
 6/4: kallisto = pd.read_csv('MMRF_CoMMpass_IA10c_E74GTF_Kallisto_Transcript_Counts.txt',sep='\t',index_col=0)
 8/1: from HetMan.features.expression import *
 8/2: from HetMan import *
 8/3: import sys
 8/4: sys.path.extend(['/home/exacloud/lustre1/CompBio/mgrzad/bergamot'])
 8/5: from HetMan.features.expression import *
 8/6: brca = get_expr_bmeg('TCGA-BRCA')
 8/7: oph = Ophion(choose_bmeg_server(verbose=True))
 8/8: oph
 8/9: %paste
8/10:
expr_query = ('oph.query().has("gid", "project:" + cohort)'
                  '.outgoing("hasMember").incoming("biosampleOfIndividual")'
                  '.mark("sample").incoming("expressionForSample")'
                  '.mark("expression").select(["sample", "expression"])')
8/11: samp_count = eval(expr_query).count().execute()[0]
8/12: cohort = 'TCGA-BRCA'
8/13: samp_count = eval(expr_query).count().execute()[0]
8/14: samp_count
8/15: eval(expr_query)
8/16: eval(expr_query).count()
8/17: expr_query
8/18: samp_count.isnumeric()
8/19:
for qr in eval(expr_query).execute():
    dt = json.loads(qr)
8/20: dt
8/21: qr
8/22: qr
8/23: samp_count
8/24: type
8/25: type(samp_count)
8/26: brca = get_expr_bmeg('TCGA-PCPG')
8/27: samp_count
8/28: qr
8/29: brca = get_expr_bmeg('TCGA-PCPG')
8/30: brca = get_expr_bmeg('TCGA-PCPG')
 9/1: samples = 'samples.txt'
 9/2: mmrf ='mmrf_age_gender_meta.txt'
 9/3: out = 'new_meta_file.txt'
 9/4: %paste
 9/5:
sample_dic = {}
sample_order = []
for line in open(samples):
    if line.startswith('SampleID'):
        continue
    else:
        line = line.strip().split()
        samp = line[0].split('_')[0]
        sample_order.append(samp)
        if samp not in sample_dic:
            sample_dic[samp] = line
 9/6:
for line in open(mmrf):
    if line.startswith('SAMPLE_ID'):
        header = line.strip().split()
    else:
        line = line.strip().split('\t')
        key = line[0].split('_')[1]
        if key in sample_dic:
            sample_dic[key] = sample_dic[key] +line[1:]
 9/7: sample_dic
 9/8: len(sample_dic)
 9/9:
out_f = open(out,'w')
for k in sample_order:
    meta = sample_dic[k]
    out_f.write('\t'.join(meta)+'\n')
out_f.close()
9/10: ls
9/11: pwd
9/12: import pandas as pd
10/1: %paste
10/2:
import os
import sys
import pandas as pd
import numpy as np
from operator import itemgetter
10/3: ls
10/4: abundance_file ='data/SRP050493_control_marrow_counts.txt'
10/5: out ='results/STAR/'
10/6:
mgus_exp = pd.read_csv(abundance_file, sep='\t',index_col=0)
meta = pd.DataFrame(np.random.rand(4, 3), index=['N_unmapped', 'N_multimapping', 'N_noFeature', 'N_ambiguous'])
filler = pd.DataFrame(np.random.rand(mgus_exp.shape[0],2), index=mgus_exp.index)
10/7: filler
10/8:
for i in range(mgus_exp.shape[1]):
    column = mgus_exp.iloc[:,i]
    name = mgus_exp.iloc[:,i].name
    samp = os.path.join(out,name)
    result = pd.concat([column,filler],axis=1)
    result.columns = [0,1,2]
    final_result= pd.concat([meta,result])
    if not os.path.exists(samp):
        os.makedirs(samp)
    final_result.round(0).to_csv(os.path.join(samp,name+'_ReadsPerGene.out.tab'),header=False,sep='\t')
    sj = np.random.randint(1,5,size=(100,9))
    np.savetxt(os.path.join(samp,name+'_SJ.out.tab'),sj,fmt='%i',delimiter='\t')
10/9: out
10/10: out ='int_results/STAR/'
10/11:
for i in range(mgus_exp.shape[1]):
    column = mgus_exp.iloc[:,i]
    name = mgus_exp.iloc[:,i].name
    samp = os.path.join(out,name)
    result = pd.concat([column,filler],axis=1)
    result.columns = [0,1,2]
    final_result= pd.concat([meta,result])
    if not os.path.exists(samp):
        os.makedirs(samp)
    final_result.round(0).to_csv(os.path.join(samp,name+'_ReadsPerGene.out.tab'),header=False,sep='\t')
    sj = np.random.randint(1,5,size=(100,9))
    np.savetxt(os.path.join(samp,name+'_SJ.out.tab'),sj,fmt='%i',delimiter='\t')
11/1: data_dir = '/home/exacloud/lustre1/BioCoders/ProjectCollaborations/MMRF/int_results/STAR'
11/2: ls
11/3: ls data_dir
11/4: ls
11/5: import os
11/6: severe_mm = os.path.join(os.getcwd(),'Severe_MM_meta.txt')
11/7: severe_mm
11/8: data_dir = '/home/exacloud/lustre1/BioCoders/ProjectCollaborations/MMRF/results/STAR'
11/9: out = 'filtered_severe_meta_data.txt'
11/10:
out_f = open(out,'w')
for line in open(severe_mm):
    if line.startswith('SampleID'):
        header = line.strip().split()
    else:
        line = line.strip().split()
        samp_rnaseq = 'MMRF_' + line[0] + '_BM'
        if os.path.exists(os.path.join(data_dir, samp_rnaseq)):
            out_f.write('\t'.join(line)+'\n')
        else:
            continue
11/11: samp_rnaseq
11/12: data_dir
11/13: os.path.exists(os.path.join(data_dir,samp_rnaseq))
11/14: out
11/15: out_f.close()
11/16:
out_f = open(out,'w')
for line in open(severe_mm):
    if line.startswith('SampleID'):
        header = line.strip().split()
    else:
        line = line.strip().split()
        samp_rnaseq = 'MMRF_' + line[0] + '_BM'
        if os.path.exists(os.path.join(data_dir, samp_rnaseq)):
            out_f.write('\t'.join(line)+'\n')
        else:
            continue
out_f.close()
11/17: help
11/18: help()
12/1: abundance_file ="SRP050493_control_marrow_counts.txt"
12/2: out = "SRP050493_control_marrow_counts_modified.txt"
12/3:
gene_set = set()
out_f = open(out, 'w')
for line in open(abundance_file):
    if line.startswith('GENE_ID'):
        line = line.strip().split()
        out_f.write('\t'.join(line) +'\n')
    else:
        line = line.strip().split()
        ens = line[0].split('.')[0]
        if ens not in gene_set:
            gene_set.update(ens)
            out_line = ens + line[1:]
            out_f.write('\t'.join(out_line)+'\n')
        else:
            continue
out_f.close()
12/4: ens
12/5: gene_set
12/6: gene_set.add(ens)
12/7: gene_set
12/8: %paste
12/9:
gene_set = set()
out_f = open(out, 'w')
for line in open(abundance_file):
    if line.startswith('GENE_ID'):
        line = line.strip().split()
        out_f.write('\t'.join(line) +'\n')
    else:
        line = line.strip().split()
        ens = line[0].split('.')[0]
        if ens not in gene_set:
            gene_set.add(ens)
            out_line = ens + line[1:]
            out_f.write('\t'.join(out_line)+'\n')
        else:
            continue
12/10: ens
12/11: out_line
12/12:
gene_set = set()
out_f = open(out, 'w')
for line in open(abundance_file):
    if line.startswith('GENE_ID'):
        line = line.strip().split()
        out_f.write('\t'.join(line) +'\n')
    else:
        line = line.strip().split()
        ens = line[0].split('.')[0]
        if ens not in gene_set:
            gene_set.add(ens)
            out_line = [ens] + line[1:]
            out_f.write('\t'.join(out_line)+'\n')
        else:
            continue
12/13: out_line
12/14: out_f.close()
12/15: out = '/home/exacloud/lustre1/BioCoders/ProjectCollaborations/MMRF/int_results/STAR'
12/16: import os, pd
12/17: import os
12/18: import pandas as pd
12/19: import numpy as np
12/20: abundance_file = 'SRP050493_control_marrow_counts_modified.txt'
12/21:
mgus_exp = pd.read_csv(abundance_file, sep='\t',index_col=0)
meta = pd.DataFrame(np.random.rand(4, 3), index=['N_unmapped', 'N_multimapping', 'N_noFeature', 'N_ambiguous'])
filler = pd.DataFrame(np.random.rand(mgus_exp.shape[0],2), index=mgus_exp.index)
for i in range(mgus_exp.shape[1]):
    column = mgus_exp.iloc[:,i]
    name = mgus_exp.iloc[:,i].name
    samp = os.path.join(out,name)
    result = pd.concat([column,filler],axis=1)
    result.columns = [0,1,2]
    final_result= pd.concat([meta,result])
    if not os.path.exists(samp):
        os.makedirs(samp)
    final_result.round(0).to_csv(os.path.join(samp,name+'_ReadsPerGene.out.tab'),header=False,sep='\t')
    sj = np.random.randint(1,5,size=(100,9))
    np.savetxt(os.path.join(samp,name+'_SJ.out.tab'),sj,fmt='%i',delimiter='\t')
13/1: from features.expression import *
13/2: import ophion
13/3: O = ophion.Ophion('http://bmeg.io')
13/4: O
13/5: O.query().has("gid","project:" "TCGA-BRCA")
13/6: O.query().has("gid","project:" "TCGA-BRCA").execute()
13/7: O.query().has("gid","project:" "TCGA-BRCA").label().groupCount().execute()
14/1: from features.expression import *
14/2: O = ophion.Ophion('http://bmeg.io')
14/3: import ophion
14/4: O = ophion.Ophion('http://bmeg.io')
14/5: O.query().has("gid","project:" "TCGA-BRCA").execute()
14/6: O.query().has("gid","project:" "TCGA-BRCA").label().groupCount().execute()
14/7: O.query().has("gid","project:" "TCGA-BRCA").properties().groupCount().execute()
14/8: O.query().has("gid","project:" "TCGA-BRCA").id().groupCount().execute()
14/9: O.query().has("gid","project:" "TCGA-BRCA").outgoing().groupCount().execute()
14/10: O.query().has("gid","project:" "TCGA-BRCA").outgoing().label().groupCount().execute()
14/11: O.query().has("gid","project:" "TCGA-BRCA").outgoing().label().execute()
14/12: O.query().has("gid","project:" "TCGA-BRCA").outgoing().execute()
14/13: O.query().has("gid","project:" "TCGA-BRCA").outgoing().groupCount().execute()
14/14: O.query().has("gid","project:" "TCGA-BRCA").outgoing().has("tumor_status")execute()
14/15: O.query().has("gid","project:" "TCGA-BRCA").outgoing().has("tumor_status").execute()
14/16: O.query().has("gid","project:" "TCGA-BRCA", O.within("tumor_status")).execute()
14/17: O.query().has("gid","project:" "TCGA-BRCA", O.within("tumor_status")).outgoing().label().groupCount().execute()
14/18: O.query().has("gid","project:" "TCGA-BRCA", O.within("tumor_status")).outgoing().tumor_status().groupCount().execute()
14/19: O.query().has("gid","project:" "TCGA-BRCA", O.within("tumor_status")).outgoing().groupCount().execute()
14/20: O.query().has("gid","project:" "TCGA-BRCA", O.within("tumor_status")).outgoing().execute()
14/21: O.query().has("gid","project:" "TCGA-BRCA", O.within("tumor_status")).outgoing().properties().execute()
14/22: O.query().has("gid","project:" "TCGA-BRCA", O.within("tumor_status")).outgoing().properties().execute()
14/23: cohort = 'TCGA-BRCA'
14/24: expr_data = get_expr_bmeg(cohort)
14/25: %paste
14/26:
def get_sample_type(cohort, expr_data):
    """Loads sample_type data for all samples present in an expression matrix
    of interest (expr_data)
    Args:
        cohort (str): The name of an individualCohort vertex in BMEG.
        expr_data (pandas DataFrame of float), shape = [n_samps, n_feats]
    Returns:
        stype_map (pd.Series): index = samps, values = sample_type
    Examples:
        >>> samp_type = get_sample_type('TCGA-BRCA', brca_expr_data)
    """
    oph = Ophion(choose_bmeg_server(verbose=True))
    samp_query = ('oph.query().has("gid", "project:" + cohort)'
                  '.outgoing("hasMember").incoming("biosampleOfIndividual")'
                  '.mark("sample").incoming("expressionForSample")'
                  '.mark("expression").select(["sample"])')
    samp_count = eval(expr_query).count().execute()[0]
    # ensures BMEG is running
    if not samp_count.isnumeric():
        raise IOError("BMEG could not process query, returned error:\n"
                      + samp_count)
    # ensures the query returns data
    if samp_count == '0':
        raise ValueError("No samples found in BMEG for cohort "
                         + cohort + " !")
    # makes an empty pd.Series where keys are tcga sample ids
    # and the values will be filled the sample_type
    stype_map = pd.Series(index=expr_data.index)
    print(stype_map.index[:3])
    # parses phenotype data and loads it into a list
    # qr = represents one sample's information
    for qr in eval(samp_query).execute():
        dt = json.loads(qr)
        if ('properties' in dt
            and 'sample_type' in dt['properties']
            and 'gid' in dt):
            gid = dt['gid'].split(':')[-1]
            if gid in stype_map.index:
                stype_map[gid] = dt['properties']['sample_type'].strip('["').strip('"]').replace(' ', '_')
    return stype_map
14/27: stype_map = get_sample_type(cohort,expr_data)
14/28:
def get_sample_type(cohort, expr_data):
    """Loads sample_type data for all samples present in an expression matrix
    of interest (expr_data)
    Args:
        cohort (str): The name of an individualCohort vertex in BMEG.
        expr_data (pandas DataFrame of float), shape = [n_samps, n_feats]
    Returns:
        stype_map (pd.Series): index = samps, values = sample_type
    Examples:
        >>> samp_type = get_sample_type('TCGA-BRCA', brca_expr_data)
    """
    oph = Ophion(choose_bmeg_server(verbose=True))
    samp_query = ('oph.query().has("gid", "project:" + cohort)'
                  '.outgoing("hasMember").incoming("biosampleOfIndividual")'
                  '.mark("sample").incoming("expressionForSample")'
                  '.mark("expression").select(["sample"])')
    samp_count = eval(samp_query).count().execute()[0]
    # ensures BMEG is running
    if not samp_count.isnumeric():
        raise IOError("BMEG could not process query, returned error:\n"
                      + samp_count)
    # ensures the query returns data
    if samp_count == '0':
        raise ValueError("No samples found in BMEG for cohort "
                         + cohort + " !")
    # makes an empty pd.Series where keys are tcga sample ids
    # and the values will be filled the sample_type
    stype_map = pd.Series(index=expr_data.index)
    print(stype_map.index[:3])
    # parses phenotype data and loads it into a list
    # qr = represents one sample's information
    for qr in eval(samp_query).execute():
        dt = json.loads(qr)
        if ('properties' in dt
            and 'sample_type' in dt['properties']
            and 'gid' in dt):
            gid = dt['gid'].split(':')[-1]
            if gid in stype_map.index:
                stype_map[gid] = dt['properties']['sample_type'].strip('["').strip('"]').replace(' ', '_')
    return stype_map
14/29: stype_map = get_sample_type(cohort,expr_data)
14/30: stype_map
14/31: stype_map.sum()
14/32: stype_map.count()
14/33: stype_map.unique()
14/34: stype_map.summary()
14/35: stype_map.sum
14/36: stype_map.sum()
14/37: stype_map.dtype
14/38: stype_map.dtypes
14/39: stype_map
14/40: stype_map[1]
14/41: stype_map.sum(axis=1)
14/42: stype_map.unique()
14/43: stype_map.value_counts()
14/44:
    oph = Ophion(choose_bmeg_server(verbose=True))
    samp_query = ('oph.query().has("gid", "project:" + cohort)'
                  '.outgoing("hasMember").incoming("biosampleOfIndividual")'
                  '.mark("sample").incoming("expressionForSample")'
                  '.mark("expression").select(["sample"])')
    samp_count = eval(samp_query).count().execute()[0]
    # ensures BMEG is running
    if not samp_count.isnumeric():
        raise IOError("BMEG could not process query, returned error:\n"
                      + samp_count)
    # ensures the query returns data
    if samp_count == '0':
        raise ValueError("No samples found in BMEG for cohort "
                         + cohort + " !")
    # makes an empty pd.Series where keys are tcga sample ids
    # and the values will be filled the sample_type
    stype_map = pd.Series(index=expr_data.index)
    print(stype_map.index[:3])
    # parses phenotype data and loads it into a list
    # qr = represents one sample's information
    for qr in eval(samp_query).execute():
        dt = json.loads(qr)
        if ('properties' in dt
            and 'sample_type' in dt['properties']
            and 'gid' in dt):
            gid = dt['gid'].split(':')[-1]
            if gid in stype_map.index:
                stype_map[gid] = dt['properties']['sample_type'].strip('["').strip('"]').replace(' ', '_')
14/45: dt
14/46: gid
14/47: stype_map.index
14/48: expr_data.index
14/49: expr_data.index.shape
14/50: stype_map.index.shape
14/51: expr_data.index == expr_data.index
14/52: sum(expr_data.index == expr_data.index)
14/53: dt
14/54: dt['properties']
14/55: dt['properties']['sample_type']
14/56: dt['properties']['sample_type'][3:-3]
14/57: dt['properties']['sample_type'][2:-2]
14/58: dt['properties']['sample_type'][2:-2].replace(' ','_')
14/59:
    stype_map = pd.Series(index=expr_data.index)
    print(stype_map.index[:3])
    # parses phenotype data and loads it into a list
    # qr = represents one sample's information
    for qr in eval(samp_query).execute():
        dt = json.loads(qr)
        if ('properties' in dt
            and 'sample_type' in dt['properties']
            and 'gid' in dt):
            gid = dt['gid'].split(':')[-1]
            if gid in stype_map.index:
                stype_map[gid] = dt['properties']['sample_type'][2:-2].replace(' ','_')
14/60: stype_map.value_counts()
14/61: qr
14/62: dt
15/1: cohort =
15/2: cohort = 'TCGA-BRCA'
15/3: expr_query = ('oph.query().has("gid", "project:" + cohort)' '.outgoing("hasMember").incoming("biosampleOfIndividual")' '.mark("sample").incoming("expressionForSample")' '.mark("expression").select(["sample", "expression"])')
15/4: import ophion
15/5: import ophion
16/1: ls
16/2: import ophion
16/3: oph = = ophion.Ophion('http://bmeg.io')
16/4: oph = ophion.Ophion('http://bmeg.io')
16/5: expr_query = ('oph.query().has("gid", "project:" + cohort)' '.outgoing("hasMember").incoming("biosampleOfIndividual")' '.mark("sample").incoming("expressionForSample")' '.mark("expression").select(["sample", "expression"])')
16/6: eval(expr_query).execute()
16/7: cohort='TCGA-BCRA'
16/8: eval(expr_query).execute()
16/9: from features.expression import *
16/10: cohort='TCGA-BRCA'
16/11: eval(expr_query).execute()
16/12: eval(expr_query).outgoing('synonymForGene':'entrez').execute()
16/13: eval(expr_query).outgoing('synonymForGene:' 'entrez').execute()
16/14: eval(expr_query).outgoing('synonymForGene').groupCount().execute()
16/15: eval(expr_query).outgoing('Gene').groupCount().execute()
16/16:
    expr_list = {}

    # TODO: filter on gene chromosome when BMEG is updated
    expr_query = ('oph.query().has("gid", "project:" + cohort)'
                  '.outgoing("hasMember").incoming("biosampleOfIndividual")'
                  '.mark("sample").incoming("expressionForSample")'
                  '.mark("expression").select(["sample", "expression"])')
    samp_count = eval(expr_query).count().execute()[0]

    # ensures BMEG is running
    if not samp_count.isnumeric():
        raise IOError("BMEG could not process query, returned error:\n"
                      + samp_count)

    # ensures the query returns data
    if samp_count == '0':
        raise ValueError("No samples found in BMEG for cohort "
                         + cohort + " !")
16/17:
    for qr in eval(expr_query).execute():
        dt = json.loads(qr)
        if ('expression' in dt and 'properties' in dt['expression']
                and 'serializedExpressions'
                in dt['expression']['properties']):
            expr_list[dt['sample']['gid']] = json.loads(
                dt['expression']['properties']['serializedExpressions'])
16/18: dr
16/19: dt
16/20: dt.keys9)
16/21: dt.keys()
16/22: dt['expression']
16/23: dt['expression'].keys()
16/24: dt['expression']['gid']
16/25: dt['expression']['properties']
16/26: dt['expression']['properties'].keys()
16/27: dt['expression']['properties']['type']
16/28: dt['expression']['properties']['#label']
16/29: dt['expression']['properties']['serializedExpressions']
16/30: dt['expression']['properties']['serializedExpressions'].keys()
16/31: dt['expression']['properties']
16/32: dt['expression']['properties'].keys()
16/33: dt['expression']['properties']['biosampleId']
16/34: dt['expression'].keys()
16/35: dt['expression']['label']
16/36: dt['expression']['label']
16/37: dt['expression']['gid']
16/38: qr
16/39: dt
16/40: dt.keys()
16/41: dt['sample']
16/42: dt['expression']
16/43: dt['expression'].keys()
16/44: dt['expression']['label']
16/45: dt['expression']['gid']
16/46: dt['expression']['properties']
16/47: dt['expression']['properties'].keys()
16/48: dt['expression']['properties']['type']
16/49: dt['expression']['properties']
16/50: dt['expression']['properties'].keys()
16/51: dt['expression']['properties']['serializedExpressions']
16/52: json.loads(dt['expression']['properties']['serializedExpressions'])
16/53: test = json.loads(dt['expression']['properties']['serializedExpressions'])
16/54: test.keys()
16/55: test
17/1:
import argparse
import os
import sys
import pandas as pd
import numpy as np

base_dir = os.path.dirname(os.path.realpath(__file__))
sys.path += [base_dir + '/../../../../bergamot']
from HetMan.features.expression import get_expr_bmeg

# the following is for get_sample_type and should be moved with it
from HetMan.features.utils import choose_bmeg_server
import json
from ophion import Ophion

data_dir = base_dir + '/../../data/tf_activity/'
17/2: from HetMan.features.expression import get_expr_bmeg
17/3: from features.expression import *
17/4: from ophion import Ophion
17/5: ls
18/1: test_dic = {x:i for x,i in zip(range(1:10),range(10:20))}
18/2: test_dic = {x:i for x,i in zip((range(1,10),range(10,20)))}
18/3: zip((range(1,10),range(10,20)))
21/1: %paste
21/2: ls
21/3:
def get_expr_firehose(cohort, data_dir):
    """Loads RNA-seq gene-level expression data downloaded from Firehose.
    Firehose data is acquired by downloading the firehose_get from
        https://confluence.broadinstitute.org/display/GDAC/Download
    into `data_dir` and then running
        ./firehose_get -o RSEM_genes_normalized data 2016_01_28 brca
    from the command line, assuming firehose_get v0.4.11
    Args:
        cohort (str): The name of a TCGA cohort available in Broad Firehose.
        data_dir (str): The local directory where the Firehose data was
                        downloaded.
    Returns:
        expr_data (pandas DataFrame of float), shape = [n_samps, n_feats]
    Examples:
        >>> expr_data = get_expr_bmeg(
        >>>     'BRCA', '/home/users/grzadkow/compbio/input-data/firehose')
        >>> expr_data = get_expr_bmeg('SKCM', '../firehose')
    """
    expr_tar = tarfile.open(glob.glob(os.path.join(
        data_dir, "stddata__2016_01_28", cohort, "20160128",
        "*Merge_rnaseqv2_*_RSEM_genes_normalized_*.Level_3*.tar.gz"
        ))[0])

    # finds the file in the tarball that contains the expression data, loads
    # it into a formatted dataframe
    expr_fl = expr_tar.extractfile(expr_tar.getmembers()[0])
    expr_data = pd.read_csv(BytesIO(expr_fl.read()),
                            sep='\t', skiprows=[1], index_col=0,
                            engine='python').transpose()

    # parses the expression matrix columns to get the gene names, removes the
    # columns that don't correspond to known genes
    expr_data.columns = [gn.split('|')[0] if isinstance(gn, str) else gn
                         for gn in expr_data.columns]
    expr_data = expr_data.iloc[:, expr_data.columns != '?']

    # parses expression matrix rows to get TCGA sample barcodes
    expr_data.index = ["-".join(x[:4])
                       for x in expr_data.index.str.split('-')]

    return expr_data
21/4: brca_fh = get_expr_firehose('BRCA','stddata__2016_01_28/')
21/5: i%paste
21/6:
import numpy as np
import pandas as pd

import os
import glob

import tarfile
from io import BytesIO
21/7: brca_fh = get_expr_firehose('BRCA','stddata__2016_01_28/')
21/8: cohort ='BRCA'
21/9: data_dir = os.getcwd()
21/10: data_dir
21/11: brca_fh = get_expr_firehose(cohort,data_dir)
21/12: brca_fh
21/13: brca_fh.index
21/14: ls
21/15: data_dir
21/16: cd stddata__2016_01_28/
21/17: cd OV/
21/18: ls
21/19: ls
21/20: cd 20160128/
21/21: ls
21/22: %paste
21/23:
expr_tar = tarfile.open(glob.glob(os.path.join(
    data_dir, "stddata__2016_07_15", cohort, "20160715",
    "*Merge_protein_exp_*protein_normalization*.Level_3*.tar.gz"
    ))[0])

# finds the file in the tarball that contains the expression data, loads
# it into a formatted dataframe
expr_fl = expr_tar.extractfile(expr_tar.getmembers()[0])
expr_data = pd.read_csv(BytesIO(expr_fl.read()),
                        sep='\t', skiprows=[1], index_col=0,
                        engine='python').transpose()

# parses the expression matrix columns to get the gene names, removes the
# columns that don't correspond to known genes
expr_data.columns = [gn.split('|')[0] if isinstance(gn, str) else gn
                     for gn in expr_data.columns]
expr_data = expr_data.iloc[:, expr_data.columns != '?']

# parses expression matrix rows to get TCGA sample barcodes
expr_data.index = ["-".join(x[:4]) for x in expr_data.index.str.split('-')]
21/24: expr_tar
21/25: expr_fil
21/26: expr_fl
21/27: expr_data
21/28: expr_fl.read()
21/29: expr_fil
21/30: expr_fl
21/31: expr_tar
21/32: expr_tar.extractfile(expr_tar.getmembers())
21/33: expr_tar.extractfile(expr_tar.getmembers)
21/34: expr_tar.extractfile()
21/35: expr_tar.getmembers()
21/36: expr_tar.getmembers()[1]
21/37: ls
21/38: cd ../..
21/39: ls
21/40: cd ..
21/41: ls
21/42:
expr_fl = expr_tar.extractfile(expr_tar.getmembers()[1])
expr_data = pd.read_csv(BytesIO(expr_fl.read()),
                        sep='\t', skiprows=[1], index_col=0,
                        engine='python').transpose()

# parses the expression matrix columns to get the gene names, removes the
# columns that don't correspond to known genes
expr_data.columns = [gn.split('|')[0] if isinstance(gn, str) else gn
                     for gn in expr_data.columns]
expr_data = expr_data.iloc[:, expr_data.columns != '?']

# parses expression matrix rows to get TCGA sample barcodes
expr_data.index = ["-".join(x[:4]) for x in expr_data.index.str.split('-')]
21/43: expr_data
21/44: cohort
21/45: cohort = 'OV'
21/46:
expr_tar = tarfile.open(glob.glob(os.path.join(
    data_dir, "stddata__2016_07_15", cohort, "20160715",
    "*Merge_protein_exp_*protein_normalization*.Level_3*.tar.gz"
    ))[0])

# finds the file in the tarball that contains the expression data, loads
# it into a formatted dataframe
expr_fl = expr_tar.extractfile(expr_tar.getmembers()[1])
expr_data = pd.read_csv(BytesIO(expr_fl.read()),
                        sep='\t', skiprows=[1], index_col=0,
                        engine='python').transpose()

# parses the expression matrix columns to get the gene names, removes the
# columns that don't correspond to known genes
expr_data.columns = [gn.split('|')[0] if isinstance(gn, str) else gn
                     for gn in expr_data.columns]
expr_data = expr_data.iloc[:, expr_data.columns != '?']

# parses expression matrix rows to get TCGA sample barcodes
expr_data.index = ["-".join(x[:4]) for x in expr_data.index.str.split('-')]
21/47: data_dir
21/48: ls
21/49: expr_tar
21/50: expr_tar.getmembers()
21/51: type(expr_tar.getmembers())
21/52: [i for i in expr_tar.getmembers() if 'data.txt' in i]
21/53: i
21/54: x = expr_tar.getmembers()
21/55: [i for i in x if 'data.txt' in i]
21/56: x
21/57: len(x)
21/58: 'data.txt' in x
21/59: 'data.txt' in x[0]
21/60: x
21/61: x[0]
21/62: x[0].gname
21/63: x[0].get_info
21/64: x[0]
21/65: list(x[0])
21/66: x[0].isfile
21/67: x[0].isfile()
21/68: x[0].get_info
21/69: x[0].get_info()
21/70: x[0].get_info()['name']
21/71: 'data.txt' in x[0].get_info()['name']
21/72: %paste
21/73:
def member_idx(tar_members):
    if 'data.txt' in tar_members[0].get_info()['name']:
        i = 0
    else:
        i = 1
    return i
21/74:
expr_tar = tarfile.open(glob.glob(os.path.join(
    data_dir, "stddata__2016_07_15", cohort, "20160715",
    "*Merge_protein_exp_*protein_normalization*.Level_3*.tar.gz"
    ))[0])

# finds the file in the tarball that contains the expression data, loads
# it into a formatted dataframe
expr_fl = expr_tar.extractfile(expr_tar.getmembers()[member_idx(expr_tar)])
expr_data = pd.read_csv(BytesIO(expr_fl.read()),
                        sep='\t', skiprows=[1], index_col=0,
                        engine='python').transpose()

# parses the expression matrix columns to get the gene names, removes the
# columns that don't correspond to known genes
expr_data.columns = [gn.split('|')[0] if isinstance(gn, str) else gn
                     for gn in expr_data.columns]
expr_data = expr_data.iloc[:, expr_data.columns != '?']

# parses expression matrix rows to get TCGA sample barcodes
expr_data.index = ["-".join(x[:4]) for x in expr_data.index.str.split('-')]
21/75: member_idx(expr_tar)
21/76: expr_tar[0]
21/77:
def member_idx(tar_members):
    if 'data.txt' in tar_members.getmembers()[0].get_info()['name']:
        i = 0
    else:
        i = 1
    return i
21/78:
expr_tar = tarfile.open(glob.glob(os.path.join(
    data_dir, "stddata__2016_07_15", cohort, "20160715",
    "*Merge_protein_exp_*protein_normalization*.Level_3*.tar.gz"
    ))[0])

# finds the file in the tarball that contains the expression data, loads
# it into a formatted dataframe
expr_fl = expr_tar.extractfile(expr_tar.getmembers()[member_idx(expr_tar)])
expr_data = pd.read_csv(BytesIO(expr_fl.read()),
                        sep='\t', skiprows=[1], index_col=0,
                        engine='python').transpose()

# parses the expression matrix columns to get the gene names, removes the
# columns that don't correspond to known genes
expr_data.columns = [gn.split('|')[0] if isinstance(gn, str) else gn
                     for gn in expr_data.columns]
expr_data = expr_data.iloc[:, expr_data.columns != '?']

# parses expression matrix rows to get TCGA sample barcodes
expr_data.index = ["-".join(x[:4]) for x in expr_data.index.str.split('-')]
21/79: expr_data
21/80: cohort
21/81: cohort['MYC']
21/82: cohort.loc[:,'MYC']
21/83: expr_data.loc[:,'MYC']
21/84: expr_data.columns
21/85: 'MYC' in expr_data.columns
21/86: 'muyc' in expr_data.columns
21/87: 'myc' in expr_data.columns
21/88: ls
21/89: expr.T()
21/90: expr_data.T
21/91: expr = expr_data.T
21/92: expr.loc['c-Myc',:]
21/93: ls
21/94: cd stddata__2016_07_15/
21/95: ls
21/96: cd OV/
21/97: ls
21/98: cd 20160715/
21/99: ls
21/100: cd ..
21/101: ls
21/102: cd ..
21/103: ls
21/104: cd ..
21/105: ls
21/106: cd stddata__2016_01_28/
21/107: ls
21/108: cd OV/
21/109: ls
21/110: cd 20160128/
21/111: ls
22/1:
import numpy as np
import pandas as pd

import os
import glob

import tarfile
from io import BytesIO
22/2:
expr_tar = tarfile.open(glob.glob(os.path.join(
    data_dir, "analyses__2016_01_28", cohort, "20160128",
    "*CopyNumber_Gistic2*Level_4*.tar.gz"
    ))[0])
22/3: ls
22/4: data_dir = os.getcwd()
22/5: cohort
22/6: cohort = 'OV'
22/7:
expr_tar = tarfile.open(glob.glob(os.path.join(
    data_dir, "analyses__2016_01_28", cohort, "20160128",
    "*CopyNumber_Gistic2*Level_4*.tar.gz"
    ))[0])
22/8: expr_tar
22/9: expr_tar.getmembers()
22/10: %paste
22/11:
def member_idx(tar_members, cnv=False):
    for i in range(len(tar_members.getmembers())):
        if cnv:
            pass
        else:
            if 'data.txt' in tar_members.getmembers()[i].get_info()['name']:
                break
    
    return i
22/12:
def member_idx(tar_members, cnv=False):
    for i in range(len(tar_members.getmembers())):
        if cnv:
            if 'data.all_data_by_genes.txt' in tar_members.getmembers()[i].get_info()['name']:
                break
        else:
            if 'data.txt' in tar_members.getmembers()[i].get_info()['name']:
                break

    return i
22/13: expr_fl = expr_tar.extractfile(expr_tar.getmembers()[member_idx(expr_tar)])
22/14: expr_fl
22/15: member_idx(expr_tar)
22/16: expr_tar.getmembers()
22/17: expr_tar.getmembers()[27]
22/18: len(expr_tar.getmembers())
22/19: tar_members.getmembers()[i].get_info()
22/20: o
22/21: i
22/22:
def member_idx(tar_members, cnv=False):
    for i in range(len(tar_members.getmembers())):
        if cnv:
            if 'data.all_data_by_genes.txt' in tar_members.getmembers()[i].get_info()['name']:
                break
        else:
            if 'data.txt' in tar_members.getmembers()[i].get_info()['name']:
                break
        return i
22/23: member_idx(expr_tar)
22/24:
def member_idx(tar_members, cnv=False):
    for i in range(len(tar_members.getmembers())):
        if cnv:
            if 'data.all_data_by_genes.txt' in tar_members.getmembers()[i].get_info()['name']:
                return i
        else:
            if 'data.txt' in tar_members.getmembers()[i].get_info()['name']:
                return i
22/25: member_idx(expr_tar)
22/26: i
22/27:
def member_idx(tar_members, cnv=False):
    for i in range(len(tar_members.getmembers())):
        if cnv:
            if 'data.all_data_by_genes.txt' in tar_members.getmembers()[i].get_info()['name']:
                return i
        else:
            if 'data.txt' in tar_members.getmembers()[i].get_info()['name']:
                return i
22/28: i
23/1: ls
23/2: ls
23/3: cd firehose/
23/4: ls
23/5: cohort
23/6: cohort = 'OV"
23/7: cohort = 'OV;'
23/8: cohort = 'OV'
23/9:
import numpy as np
import pandas as pd

import os
import glob

import tarfile
from io import BytesIO
23/10:
expr_tar = tarfile.open(glob.glob(os.path.join(
    data_dir, "analyses__2016_01_28", cohort, "20160128",
    "*CopyNumber_Gistic2*Level_4*.tar.gz"
    ))[0])
23/11: data_dir
23/12: data_dir = os.getcwd()
23/13: ls
23/14:
expr_tar = tarfile.open(glob.glob(os.path.join(
    data_dir, "analyses__2016_01_28", cohort, "20160128",
    "*CopyNumber_Gistic2*Level_4*.tar.gz"
    ))[0])
23/15: expr_tar
23/16: expr_tar.getmembers()
23/17:
def member_idx(tar_members, cnv=False):
    for i in range(len(tar_members.getmembers())):
        if cnv:
            if 'data.all_data_by_genes.txt' in tar_members.getmembers()[i].get_info()['name']:
                return i
        else:
            if 'data.txt' in tar_members.getmembers()[i].get_info()['name']:
                return i
23/18: member_idx(expr_tar,True)]
23/19: member_idx(expr_tar,True)
23/20: tar_members = expr_tar
23/21: cnv = True
23/22:
    for i in range(len(tar_members.getmembers())):
        if cnv:
            if 'data.all_data_by_genes.txt' in tar_members.getmembers()[i].get_info()['name']:
                return i
        else:
            if 'data.txt' in tar_members.getmembers()[i].get_info()['name']:
                return i
23/23: member_idx(tar_members,cnv)
23/24: i
23/25: tar_members.getmembers()[i].get_info()
23/26: i = 0
23/27: tar_members.getmembers()[i].get_info()
23/28: tar_members.getmembers()[i].get_info()['name']
23/29: tar_members.getmembers()
23/30:
def member_idx(tar_members, cnv=False):
    for i in range(len(tar_members.getmembers())):
        if cnv:
            if 'all_data_by_genes.txt' in tar_members.getmembers()[i].get_info()['name']:
                return i
        else:
            if 'data.txt' in tar_members.getmembers()[i].get_info()['name']:
                return i
23/31: member_idx(tar_members,cnv)
23/32: tar_members.getmembers()[23]
23/33:
expr_fl = expr_tar.extractfile(expr_tar.getmembers()[member_idx(expr_tar,True)])
expr_data = pd.read_csv(BytesIO(expr_fl.read()),
                        sep='\t', skiprows=[1], index_col=0,
                        engine='python').transpose()
23/34: expr_fl
23/35:
expr_fl = expr_tar.extractfile(expr_tar.getmembers()[member_idx(expr_tar,True)])
expr_data = pd.read_csv(BytesIO(expr_fl.read()),
                        sep='\t', skiprows=[1], index_col=0,
                        engine='python').transpose()
23/36: expr_data
23/37: expr = expr_data.T()
23/38: expr = expr_data.T
23/39: expr
23/40: expr.iloc[:,1:2]
23/41: expr.iloc[:,:2]
23/42: expr.iloc[:,2:3]
23/43:
    data_type = {'cnv':tarfile.open(glob.glob(os.path.join(
    data_dir, "analyses__2016_01_28", cohort, "20160128",
    "*CopyNumber_Gistic2*Level_4*.tar.gz"))[0]), 'rppa':tarfile.open(glob.glob(os.path.join(
    data_dir, "stddata__2016_07_15", cohort, "20160715",
    "*Merge_protein_exp_*protein_normalization*.Level_3*.tar.gz"
    ))[0])}
23/44: data_type.keys()
23/45:
expr_tar = tarfile.open(glob.glob(os.path.join(
    data_dir, "stddata__2016_07_15", cohort, "20160715",
    "*Merge_methylation*humanmethylation27*.Level_3*.tar.gz"
    ))[0])
23/46: expr_tar.getmembers()
23/47: expr_fl = expr_tar.extractfile(expr_tar.getmembers()[member_idx(expr_tar,False)])
23/48: expr_fl
23/49:
expr_data = pd.read_csv(BytesIO(expr_fl.read()),
                        sep='\t', skiprows=[1], index_col=0,
                        engine='python').transpose()
23/50: expr_data
23/51: ls
23/52: expr_data.columns
24/1: ls
24/2:
import numpy as np
import pandas as pd

import os
import glob

import tarfile
from io import BytesIO
24/3:
def get_expr_firehose(cohort, data_dir, type_key):
    """Loads RNA-seq gene-level expression data downloaded from Firehose.
    Firehose data is acquired by downloading the firehose_get from
        https://confluence.broadinstitute.org/display/GDAC/Download
    into `data_dir` and then running
        ./firehose_get -o RSEM_genes_normalized data 2016_01_28 brca
    from the command line, assuming firehose_get v0.4.11
    Args:
        cohort (str): The name of a TCGA cohort available in Broad Firehose.
        data_dir (str): The local directory where the Firehose data was
                        downloaded.
    Returns:
        expr_data (pandas DataFrame of float), shape = [n_samps, n_feats]
    Examples:
        >>> expr_data = get_expr_bmeg(
        >>>     'BRCA', '/home/users/grzadkow/compbio/input-data/firehose')
        >>> expr_data = get_expr_bmeg('SKCM', '../firehose')
    """
    # TODO : incorporate humanmethylation27 and humanmethylation450
    data_type = {'cnv':tarfile.open(glob.glob(os.path.join(data_dir, "analyses__2016_01_28", cohort, "20160128","*CopyNumber_Gistic2*Level_4*.tar.gz"))[0]),
                 'rppa':tarfile.open(glob.glob(os.path.join(data_dir, "stddata__2016_07_15", cohort, "20160715","*Merge_protein_exp_*protein_normalization*.Level_3*.tar.gz"))[0]),
                 'expr':tarfile.open(glob.glob(os.path.join(data_dir, "stddata__2016_01_28", cohort, "20160128","*Merge_rnaseqv2_*_RSEM_genes_normalized_*.Level_3*.tar.gz"))[0])}

    if type_key:
        expr_tar = data_type[type_key]
    if type_key == 'cnv':
        cnv = True
    else:
        cnv = False

    # finds the file in the tarball that contains the expression data, loads
    # it into a formatted dataframe
    expr_fl = expr_tar.extractfile(expr_tar.getmembers()[0])
    if cnv:
        expr_data = pd.read_csv(BytesIO(expr_fl.read()),
                                sep='\t', skiprows=[1], index_col=0,
                                engine='python').iloc[:, 2:].transpose()
    else:
        expr_data = pd.read_csv(BytesIO(expr_fl.read()),
                                sep='\t', skiprows=[1], index_col=0,
                                engine='python').transpose()
    # parses the expression matrix columns to get the gene names, removes the
    # columns that don't correspond to known genes
    expr_data.columns = [gn.split('|')[0] if isinstance(gn, str) else gn
                         for gn in expr_data.columns]
    expr_data = expr_data.iloc[:, expr_data.columns != '?']

    # parses expression matrix rows to get TCGA sample barcodes
    expr_data.index = ["-".join(x[:4])
                       for x in expr_data.index.str.split('-')]

    return expr_data
24/4: cohort = 'OV'
24/5: type_key = 'rppa'
24/6: ov_data = get_expr_firehose(cohort,os.getcwd(),type_key)
24/7: ov_data
24/8: ov_rppa_data = get_expr_firehose(cohort,os.getcwd(),type_key)
24/9: ov_cnv_data = get_expr_firehose(cohort,os.getcwd(),'cnv')
24/10: cnv
24/11: type_key
24/12: type_key ='cnv'
24/13: ov_cnv_data = get_expr_firehose(cohort,os.getcwd(),type_key)
24/14:
    data_type = {'cnv':tarfile.open(glob.glob(os.path.join(data_dir, "analyses__2016_01_28", cohort, "20160128","*CopyNumber_Gistic2*Level_4*.tar.gz"))[0]),
                 'rppa':tarfile.open(glob.glob(os.path.join(data_dir, "stddata__2016_07_15", cohort, "20160715","*Merge_protein_exp_*protein_normalization*.Level_3*.tar.gz"))[0]),
                 'expr':tarfile.open(glob.glob(os.path.join(data_dir, "stddata__2016_01_28", cohort, "20160128","*Merge_rnaseqv2_*_RSEM_genes_normalized_*.Level_3*.tar.gz"))[0])}

    if type_key:
        expr_tar = data_type[type_key]
    if type_key == 'cnv':
        cnv = True
    else:
        cnv = False
24/15: data_dir = os.getcwd()
24/16:
    data_type = {'cnv':tarfile.open(glob.glob(os.path.join(data_dir, "analyses__2016_01_28", cohort, "20160128","*CopyNumber_Gistic2*Level_4*.tar.gz"))[0]),
                 'rppa':tarfile.open(glob.glob(os.path.join(data_dir, "stddata__2016_07_15", cohort, "20160715","*Merge_protein_exp_*protein_normalization*.Level_3*.tar.gz"))[0]),
                 'expr':tarfile.open(glob.glob(os.path.join(data_dir, "stddata__2016_01_28", cohort, "20160128","*Merge_rnaseqv2_*_RSEM_genes_normalized_*.Level_3*.tar.gz"))[0])}

    if type_key:
        expr_tar = data_type[type_key]
    if type_key == 'cnv':
        cnv = True
    else:
        cnv = False
24/17: cnv
24/18: expr_tar
24/19:     expr_fl = expr_tar.extractfile(expr_tar.getmembers()[0])
24/20:
    if cnv:
        expr_data = pd.read_csv(BytesIO(expr_fl.read()),
                                sep='\t', skiprows=[1], index_col=0,
                                engine='python').iloc[:, 2:].transpose()
    else:
        expr_data = pd.read_csv(BytesIO(expr_fl.read()),
                                sep='\t', skiprows=[1], index_col=0,
                                engine='python').transpose()
24/21: expr_fl.read()
24/22:
def member_idx(tar_members, cnv=False):
    for i in range(len(tar_members.getmembers())):
        if cnv:
            if 'all_data_by_genes.txt' in tar_members.getmembers()[i].get_info()['name']:
                return i
        else:
            if 'data.txt' in tar_members.getmembers()[i].get_info()['name']:
                return i
24/23:
    data_type = {'cnv':tarfile.open(glob.glob(os.path.join(data_dir, "analyses__2016_01_28", cohort, "20160128","*CopyNumber_Gistic2*Level_4*.tar.gz"))[0]),
                 'rppa':tarfile.open(glob.glob(os.path.join(data_dir, "stddata__2016_07_15", cohort, "20160715","*Merge_protein_exp_*protein_normalization*.Level_3*.tar.gz"))[0]),
                 'expr':tarfile.open(glob.glob(os.path.join(data_dir, "stddata__2016_01_28", cohort, "20160128","*Merge_rnaseqv2_*_RSEM_genes_normalized_*.Level_3*.tar.gz"))[0])}

    if type_key:
        expr_tar = data_type[type_key]
    if type_key == 'cnv':
        cnv = True
    else:
        cnv = False

    # finds the file in the tarball that contains the expression data, loads
    # it into a formatted dataframe
    expr_fl = expr_tar.extractfile(expr_tar.getmembers()[0])
    if cnv:
        expr_data = pd.read_csv(BytesIO(expr_fl.read()),
                                sep='\t', skiprows=[1], index_col=0,
                                engine='python').iloc[:, 2:].transpose()
    else:
        expr_data = pd.read_csv(BytesIO(expr_fl.read()),
                                sep='\t', skiprows=[1], index_col=0,
                                engine='python').transpose()
24/24: cnv
24/25:
    data_type = {'cnv':tarfile.open(glob.glob(os.path.join(data_dir, "analyses__2016_01_28", cohort, "20160128","*CopyNumber_Gistic2*Level_4*.tar.gz"))[0]),
                 'rppa':tarfile.open(glob.glob(os.path.join(data_dir, "stddata__2016_07_15", cohort, "20160715","*Merge_protein_exp_*protein_normalization*.Level_3*.tar.gz"))[0]),
                 'expr':tarfile.open(glob.glob(os.path.join(data_dir, "stddata__2016_01_28", cohort, "20160128","*Merge_rnaseqv2_*_RSEM_genes_normalized_*.Level_3*.tar.gz"))[0])}

    if type_key:
        expr_tar = data_type[type_key]
    if type_key == 'cnv':
        cnv = True
    else:
        cnv = False

    # finds the file in the tarball that contains the expression data, loads
    # it into a formatted dataframe
    expr_fl = expr_tar.extractfile(expr_tar.getmembers()[member_idx(expr_tar, cnv)])
    if cnv:
        expr_data = pd.read_csv(BytesIO(expr_fl.read()),
                                sep='\t', skiprows=[1], index_col=0,
                                engine='python').iloc[:, 2:].transpose()
    else:
        expr_data = pd.read_csv(BytesIO(expr_fl.read()),
                                sep='\t', skiprows=[1], index_col=0,
                                engine='python').transpose()
24/26: expr_data
24/27:
    expr_data.columns = [gn.split('|')[0] if isinstance(gn, str) else gn
                         for gn in expr_data.columns]
    expr_data = expr_data.iloc[:, expr_data.columns != '?']

    # parses expression matrix rows to get TCGA sample barcodes
    expr_data.index = ["-".join(x[:4])
                       for x in expr_data.index.str.split('-')]
24/28: expr_data
24/29:
    expr_data.index = ["-".join(x[:3])
                       for x in expr_data.index.str.split('-')]
24/30: expr_data
24/31: expr_data.unique()
24/32: expr_data.unique
24/33: expr_data.index
24/34: expr_data.index.unique
24/35:
def get_expr_firehose(cohort, data_dir, type_key):
    """Loads RNA-seq gene-level expression data downloaded from Firehose.
    Firehose data is acquired by downloading the firehose_get from
        https://confluence.broadinstitute.org/display/GDAC/Download
    into `data_dir` and then running
        ./firehose_get -o RSEM_genes_normalized data 2016_01_28 brca
    from the command line, assuming firehose_get v0.4.11
    Args:
        cohort (str): The name of a TCGA cohort available in Broad Firehose.
        data_dir (str): The local directory where the Firehose data was
                        downloaded.
    Returns:
        expr_data (pandas DataFrame of float), shape = [n_samps, n_feats]
    Examples:
        >>> expr_data = get_expr_bmeg(
        >>>     'BRCA', '/home/users/grzadkow/compbio/input-data/firehose')
        >>> expr_data = get_expr_bmeg('SKCM', '../firehose')
    """
    # TODO : incorporate humanmethylation27 and humanmethylation450
    data_type = {'cnv':tarfile.open(glob.glob(os.path.join(data_dir, "analyses__2016_01_28", cohort, "20160128","*CopyNumber_Gistic2*Level_4*.tar.gz"))[0]),
                 'rppa':tarfile.open(glob.glob(os.path.join(data_dir, "stddata__2016_07_15", cohort, "20160715","*Merge_protein_exp_*protein_normalization*.Level_3*.tar.gz"))[0]),
                 'expr':tarfile.open(glob.glob(os.path.join(data_dir, "stddata__2016_01_28", cohort, "20160128","*Merge_rnaseqv2_*_RSEM_genes_normalized_*.Level_3*.tar.gz"))[0])}

    if type_key:
        expr_tar = data_type[type_key]
    if type_key == 'cnv':
        cnv = True
    else:
        cnv = False

    # finds the file in the tarball that contains the expression data, loads
    # it into a formatted dataframe
    expr_fl = expr_tar.extractfile(expr_tar.getmembers()[member_idx(expr_tar, cnv)])
    if cnv:
        expr_data = pd.read_csv(BytesIO(expr_fl.read()),
                                sep='\t', skiprows=[1], index_col=0,
                                engine='python').iloc[:, 2:].transpose()
    else:
        expr_data = pd.read_csv(BytesIO(expr_fl.read()),
                                sep='\t', skiprows=[1], index_col=0,
                                engine='python').transpose()
    # parses the expression matrix columns to get the gene names, removes the
    # columns that don't correspond to known genes
    expr_data.columns = [gn.split('|')[0] if isinstance(gn, str) else gn
                         for gn in expr_data.columns]
    expr_data = expr_data.iloc[:, expr_data.columns != '?']

    # parses expression matrix rows to get TCGA sample barcodes
    expr_data.index = ["-".join(x[:3])
                       for x in expr_data.index.str.split('-')]

    return expr_data
24/36: ov_exp = get_expr_firehose('OV',data_dir,'expression')
24/37: ov_exp = get_expr_firehose('OV',data_dir,'expr')
24/38: ov_exp
24/39: ov_exp.index
24/40: ov_exp.index.unique
24/41: ov_exp.index.unique()
24/42:
def get_expr_firehose(cohort, data_dir, type_key):
    """Loads RNA-seq gene-level expression data downloaded from Firehose.
    Firehose data is acquired by downloading the firehose_get from
        https://confluence.broadinstitute.org/display/GDAC/Download
    into `data_dir` and then running
        ./firehose_get -o RSEM_genes_normalized data 2016_01_28 brca
    from the command line, assuming firehose_get v0.4.11
    Args:
        cohort (str): The name of a TCGA cohort available in Broad Firehose.
        data_dir (str): The local directory where the Firehose data was
                        downloaded.
    Returns:
        expr_data (pandas DataFrame of float), shape = [n_samps, n_feats]
    Examples:
        >>> expr_data = get_expr_bmeg(
        >>>     'BRCA', '/home/users/grzadkow/compbio/input-data/firehose')
        >>> expr_data = get_expr_bmeg('SKCM', '../firehose')
    """
    # TODO : incorporate humanmethylation27 and humanmethylation450
    data_type = {'cnv':tarfile.open(glob.glob(os.path.join(data_dir, "analyses__2016_01_28", cohort, "20160128","*CopyNumber_Gistic2*Level_4*.tar.gz"))[0]),
                 'rppa':tarfile.open(glob.glob(os.path.join(data_dir, "stddata__2016_07_15", cohort, "20160715","*Merge_protein_exp_*protein_normalization*.Level_3*.tar.gz"))[0]),
                 'expr':tarfile.open(glob.glob(os.path.join(data_dir, "stddata__2016_01_28", cohort, "20160128","*Merge_rnaseqv2_*_RSEM_genes_normalized_*.Level_3*.tar.gz"))[0])}

    if type_key:
        expr_tar = data_type[type_key]
    if type_key == 'cnv':
        cnv = True
    else:
        cnv = False

    # finds the file in the tarball that contains the expression data, loads
    # it into a formatted dataframe
    expr_fl = expr_tar.extractfile(expr_tar.getmembers()[member_idx(expr_tar, cnv)])
    if cnv:
        expr_data = pd.read_csv(BytesIO(expr_fl.read()),
                                sep='\t', skiprows=[1], index_col=0,
                                engine='python').iloc[:, 2:].transpose()
    else:
        expr_data = pd.read_csv(BytesIO(expr_fl.read()),
                                sep='\t', skiprows=[1], index_col=0,
                                engine='python').transpose()
    # parses the expression matrix columns to get the gene names, removes the
    # columns that don't correspond to known genes
    expr_data.columns = [gn.split('|')[0] if isinstance(gn, str) else gn
                         for gn in expr_data.columns]
    expr_data = expr_data.iloc[:, expr_data.columns != '?']

    # parses expression matrix rows to get TCGA sample barcodes
    expr_data.index = ["-".join(x[:4])
                       for x in expr_data.index.str.split('-')]

    return expr_data
24/43: ov_exp_test = get_expr_firehose('OV',data_dir,'expr')
24/44: ov_exp_test.index
24/45: ov_exp_test.index.unique()
24/46: ov_rppa_test = get_expr_firehose('OV',data_dir,'rppa')
24/47: ov_rppa_test
24/48: tar_members
24/49: expr_fl
24/50: expr_tar
24/51: ls
25/1: import pandas as pd
25/2: import bambi as bm
26/1: ls
26/2:
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Lasso
from sklearn.preprocessing import StandardScaler
26/3: data = pd.read_csv('expression.txt',sep='\t',index_col=0).T
26/4:
def pretty_print_linear(coefs, names=None, sort=False):
    if names == None:
        names = ["X%s" % x for x in range(len(coefs))]
    lst = zip(coefs, names)
    if sort:
        lst = sorted(lst, key=lambda x: -np.abs(x[0]))
    return " + ".join("%s * %s" % (round(coef, 3), name)
                      for coef, name in lst)


def split(data):
    train, test = train_test_split(data, test_size=0.2)
    return train, test


def fit(train, alpha=.3):
    result = {}
    #train = np.log2(train)
    train = train_og
    scaler = StandardScaler()
    targs = list(train.columns)
    for i in targs:
        las_obj = '{}-{}'.format(i,alpha)
        X = scaler.fit_transform(train.loc[:,train.columns != i])
        Y = train.loc[:,train.columns == i].values.flatten()
        names = train.loc[:,train.columns != i].columns
        lasso = Lasso(alpha=alpha)
        lasso.fit(X, Y)
        if las_obj not in result:
            result[las_obj]=[lasso,names]
    return result


def score(model,test):
    pass


def tune(train):
    tune_results = {}
    train1,train2,train3,train4,train4,test1 = np.array_split(train, 6)
    train_list = [train1,train2,train3,train4,train4,test1]
    alpha = np.linspace(0.001,10,10)
    for t in range(len(train_list)):
        for i in alpha:
            tune_eval = 'train{}-{}'.format(t,i)
            results = fit(train_list[t],i)
            if tune_eval not in tune_results:
                tune_results[tune_eval] = results

    return tune_results
26/5: train,test = split(data)
26/6: train
26/7: test
26/8:
def fit(train, alpha=.3):
    result = {}
    #train = np.log2(train)
    #train = train_og
    scaler = StandardScaler()
    targs = list(train.columns)
    for i in targs:
        las_obj = '{}-{}'.format(i,alpha)
        X = scaler.fit_transform(train.loc[:,train.columns != i])
        Y = train.loc[:,train.columns == i].values.flatten()
        names = train.loc[:,train.columns != i].columns
        lasso = Lasso(alpha=alpha)
        lasso.fit(X, Y)
        if las_obj not in result:
            result[las_obj]=[lasso,names]
    return result
26/9: tune_results = tune(train)
27/1: import pickle
27/2: tune_results = pickle.load('tune_results_dict')
27/3: tune_results = pickle.load(open('tune_results_dict','rb'))
27/4: tune_results
27/5: len(tune_results)
27/6: tune_results.keys()
27/7: tune_results.keys()
27/8: tune_results['train1-10.0']
27/9: tune_results['train1-10.0'][0]
27/10: tune_results['train1-10.0'].keys()
27/11: tune_results.keys()
28/1: ls
28/2: import HetMan
28/3: ls
28/4: ls
28/5: cd ..
28/6: ls
28/7: cd
28/8: pj
29/1:
import argparse
import os
import sys
import pandas as pd
import numpy as np

base_dir = os.path.dirname(os.path.realpath(__file__))
#sys.path += [base_dir + '/../../../../bergamot']
from HetMan.features.expression import get_expr_bmeg

# the following is for get_sample_type and should be moved with it
from HetMan.features.utils import choose_bmeg_server
import json
from ophion import Ophion

data_dir = base_dir + '/../../data/tf_activity/'
29/2:
import argparse
import os
import sys
import pandas as pd
import numpy as np
29/3:
from HetMan.features.utils import choose_bmeg_server
import json
from ophion import Ophion
29/4: ls
30/1: ls
30/2: cd ..
30/3: ls
30/4: cd ..
30/5: ls
30/6: import HetMan
30/7: ls
30/8: echo $PYTHONPATH
30/9: cd libls
30/10: cd lib
30/11: ls
30/12: cd HetMan/
30/13: ls
30/14: from HetMan import *
30/15: cd ..
30/16: from HetMan import *
30/17: features
30/18: features.expression
30/19: from HetMan.features.expression import *
31/1: ls
31/2: from ophion import Ophion
31/3: from prep_for_viper import *
31/4: aml = get_expr_bmeg('TCGA-LAML')
31/5: aml
31/6: cohort ='TCGA-LAML'
31/7: stype = get_sample_type(cohort, aml)
31/8: oph = Ophion(choose_bmeg_server(verbose=True))
32/1: from ophion import Ophion
32/2:
from HetMan.features.expression import get_expr_bmeg

# the following is for get_sample_type and should be moved with it
from HetMan.features.utils import choose_bmeg_server
import json
32/3:
    oph = Ophion(choose_bmeg_server(verbose=True))

    samp_query = ('oph.query().has("gid", "cohort:" + cohort)'
                  '.outgoing("hasSample").mark("sample")'
                  '.incoming("expressionFor").mark("exprssion")'
                  '.select(["sample"])')
32/4:

    samp_count = eval(samp_query).count().execute()[0]
32/5: %paste
32/6: ls
32/7: samp_count = eval(samp_query).count().execute()[0]
32/8: cohort ='TCGA-LAML'
32/9: samp_count = eval(samp_query).count().execute()[0]
32/10: samp_count
32/11: pwd
32/12: ls
32/13: cd ..
32/14: ls
32/15: cd
32/16: ls
32/17: cd tf_activity/
32/18: ls
32/19: from prep_for_viper import *
32/20: aml
32/21: ls
32/22: dir()
32/23: ls
32/24: cohort
32/25: aml
32/26: expr_data = get_expr_bmeg(cohort)
32/27: ?expression
32/28: cohort
32/29: which Hetman
32/30: HetMan
32/31: get_expr_bemg
32/32: get_expr_bmeg
33/1: import pandas as pd
33/2: aml = pd.read_csv('TCGA-AML.txt',index_col=0,sep='\t').T
33/3: aml
33/4: aml.to_csv('LAML_transposed.txt',sep='\t')
34/1:
import warnings
warnings.filterwarnings('ignore')

# general packages
import sys
import regex as re

# scientific packages
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import patsy as pt
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from scipy import optimize

# pymc3 libraries
import pymc3 as pm
from pymc3.backends.base import merge_traces
import theano as thno
import theano.tensor as T

from ipywidgets import interactive, fixed
34/2:
import warnings
warnings.filterwarnings('ignore')

# general packages
import sys
import regex as re

# scientific packages
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import patsy as pt
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from scipy import optimize

# pymc3 libraries
import pymc3 as pm
from pymc3.backends.base import merge_traces
import theano as thno
import theano.tensor as T

from ipywidgets import interactive, fixed
35/1:
import warnings
warnings.filterwarnings('ignore')

# general packages
import sys
import regex as re

# scientific packages
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import patsy as pt
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from scipy import optimize

# pymc3 libraries
import pymc3 as pm
from pymc3.backends.base import merge_traces
import theano as thno
import theano.tensor as T
36/1:
import warnings
warnings.filterwarnings('ignore')

# general packages
import sys
import regex as re

# scientific packages
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import patsy as pt
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from scipy import optimize

# pymc3 libraries
import pymc3 as pm
from pymc3.backends.base import merge_traces
import theano as thno
import theano.tensor as T
37/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
38/1: ls
38/2:
# general packages
import sys
import regex as re

# scientific packages
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import patsy as pt
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from scipy import optimize

# pymc3 libraries
import pymc3 as pm
from pymc3.backends.base import merge_traces
import theano as thno
import theano.tensor as T
39/1: out = '/home/exacloud/lustre1/BioCoders/ProjectCollaborations/MMRF/TEP/results/STAR'
39/2: abundance_file = 'GSE68086_TEP_data_matrix.txt'
39/3:
import os
import sys
import pandas as pd
import numpy as np
from operator import itemgetter
39/4:
mgus_exp = pd.read_csv(abundance_file, sep='\t',index_col=0)
meta = pd.DataFrame(np.random.rand(4, 3), index=['N_unmapped', 'N_multimapping', 'N_noFeature', 'N_ambiguous'])
filler = pd.DataFrame(np.random.rand(mgus_exp.shape[0],2), index=mgus_exp.index)
for i in range(mgus_exp.shape[1]):
    column = mgus_exp.iloc[:,i]
    name = mgus_exp.iloc[:,i].name
    samp = os.path.join(out,name)
    result = pd.concat([column,filler],axis=1)
    result.columns = [0,1,2]
    final_result= pd.concat([meta,result])
    if not os.path.exists(samp):
        os.makedirs(samp)
    final_result.round(0).to_csv(os.path.join(samp,name+'_ReadsPerGene.out.tab'),header=False,sep='\t')
    sj = np.random.randint(1,5,size=(100,9))
    np.savetxt(os.path.join(samp,name+'_SJ.out.tab'),sj,fmt='%i',delimiter='\t')
40/1: import pandas as pd
40/2: ls
40/3: brca_nes = pd.read_csv('BRCA-TFA-NES.txt',sep='\t')
40/4: brca_nes
40/5: ls()
40/6: ls
40/7: brca_exp = pd.read_csv('tmp-TCGA-BRCA-expression.tsv',sep='\t')
40/8: brca_exp
40/9: brca_exp = pd.read_csv('tmp-TCGA-BRCA-expression.tsv',sep='\t',index_col=0)
40/10: brca_exp
40/11: brca_nes.shape
40/12: brca_nes.columns == brca_exp.columns
40/13: sum(brca_nes.columns == brca_exp.columns)
40/14: test_nes = pd.DataFrame(index=brca_exp.index,columns=brca_exp.columns)
40/15: test_nes
40/16: pd.concat([test_nes,brca_nes])
40/17: pd.merge([test_nes,brca_nes])
40/18: pd.merge(test_nes,brca_nes])
40/19: pd.merge(test_nes,brca_nes)
40/20: pd.merge(test_nes,brca_nes]
40/21: test_nes.join(brca_nes)
40/22: pd.merge(test_nes,brca_nes,left_index=True)
40/23: pd.concat([test_nes,brca_nes],axis=1)
40/24: merged_brca_nes = pd.concat([test_nes,brca_nes],axis=1)
40/25: merged_brca_nes.fillna(0.0)
40/26: out = merged_brca_nes.fillna(0.0)
41/1: import pystan
41/2:
tfa_model_code = """
    data {
        int<lower=1> N;     // number of samples
        int<lower=1> G;     // number of genetic features
        real r[N, G];       // RNA-seq expression values
        real c[N, G];       // copy number GISTIC values
        real p[N, G];       // pseudo TFA measurements
        int <lower=G> P;                // number of pathway interactions
        int <lower=1, upper=G> po[P];   // pathway out-edges
        int <lower=1, upper=G> pi[P];   // pathway in-edges
    }

    parameters {
        vector<lower=0, upper=1>[P] wght;   // pathway interaction weights
        vector<lower=0.01, upper=10>[2] wght_prior;
        vector<lower=0, upper=1>[G] comb;   // RNA-CNA combinations
        vector<lower=0.01, upper=10>[2] comb_prior;
        vector<lower=0>[G] prec;   // precision of activities
        vector<lower=0.01, upper=20>[2] prec_prior;
        matrix[N, G] act;                   // inferred gene activities
    }
    transformed parameters{
        matrix[N, G] act_sum;
        matrix[N, G] pred_p;
        for (g in 1:G) {
            for (n in 1:N) {
                act_sum[n, g] = (comb[g] * r[n, g]) + ((1.0 - comb[g]) * c[n, g]);
                pred_p[n, g] = 0;
                for (i in 1:P) {
                    if (pi[i] == g)
                        pred_p[n, g] = pred_p[n, g] + act[n, po[i]] * wght[i];
                }
                // print(pred_p[n, g]);
            }
        }
    }
    model {
        for (i in 1:P) {
            wght[i] ~ beta(wght_prior[1], wght_prior[2]);
        }
        for (g in 1:G) {
            comb[g] ~ beta(comb_prior[1], comb_prior[2]);
        }

        for (g in 1:G) {
            prec[g] ~ gamma(prec_prior[1], prec_prior[2]);
        }

        for (g in 1:G) {
            for (n in 1:N) {
                act[n, g] ~ normal(act_sum[n, g], pow(prec[g], -1.0));
                p[n, g] ~ normal(pred_p[n, g], 0.01);
            }
        }
    }
"""
41/3:
model = pystan.StanModel(model_code=

}
41/4:
tfa_model_code = """
    data {
        int<lower=1> N;     // number of samples
        int<lower=1> G;     // number of genetic features
        real r[N, G];       // RNA-seq expression values
        real c[N, G];       // copy number GISTIC values
        real p[N, G];       // pseudo TFA measurements
        int <lower=G> P;                // number of pathway interactions
        int <lower=1, upper=G> po[P];   // pathway out-edges
        int <lower=1, upper=G> pi[P];   // pathway in-edges
    }

    parameters {
        vector<lower=0, upper=1>[P] wght;   // pathway interaction weights
        vector<lower=0.01, upper=10>[2] wght_prior;
        vector<lower=0, upper=1>[G] comb;   // RNA-CNA combinations
        vector<lower=0.01, upper=10>[2] comb_prior;
        vector<lower=0>[G] prec;   // precision of activities
        vector<lower=0.01, upper=20>[2] prec_prior;
        matrix[N, G] act;                   // inferred gene activities
    }
    transformed parameters{
        matrix[N, G] act_sum;
        matrix[N, G] pred_p;
        for (g in 1:G) {
            for (n in 1:N) {
                act_sum[n, g] = (comb[g] * r[n, g]) + ((1.0 - comb[g]) * c[n, g]);
                pred_p[n, g] = 0;
                for (i in 1:P) {
                    if (pi[i] == g)
                        pred_p[n, g] = pred_p[n, g] + act[n, po[i]] * wght[i];
                }
                // print(pred_p[n, g]);
            }
        }
    }
    model {
        for (i in 1:P) {
            wght[i] ~ beta(wght_prior[1], wght_prior[2]);
        }
        for (g in 1:G) {
            comb[g] ~ beta(comb_prior[1], comb_prior[2]);
        }

        for (g in 1:G) {
            prec[g] ~ gamma(prec_prior[1], prec_prior[2]);
        }

        for (g in 1:G) {
            for (n in 1:N) {
                act[n, g] ~ normal(act_sum[n, g], pow(prec[g], -1.0));
                p[n, g] ~ normal(pred_p[n, g], 0.01);
            }
        }
    }
"""
41/5: tfa_model_code
41/6: print(tfa_model_code)
41/7: model=pystan.StanModel(model_code=tfa_model_code,verbose=True)
41/8: model_code = 'parameters {real y;} model {y ~ normal(0,1);}'
41/9: model = pystan.StanModel(model_code=model_code, verbose=True)
41/10: model
41/11: model.sampling()
41/12: model=pystan.StanModel(model_code=tfa_model_code,verbose=True)
41/13: model
41/14: model.sampling()
41/15: os
41/16: import
41/17: import os
41/18: os.getcwd()
41/19: import sys
41/20: cd ..
41/21: ls
41/22: cd ..
41/23: cd experiments/
41/24: ls
41/25: cd predict_TFA/
41/26: ls
41/27: cd stan/
41/28: ls
41/29: base_dir = os.getcwd()
41/30: sys.path.extend([os.path.join(base_dir, '../../../..')])
41/31:
import synapseclient
import dill as pickle
from glob import glob
41/32:
from HetMan.features.cohorts import TFACohort
import HetMan.predict.bayesian_pathway.multi_protein as mpt
41/33: from ophion import Ophion
41/34: ophion
41/35: sys
41/36: sys.apth
41/37: sys.path
41/38: sys.path.extend(['/home/exacloud/lustre1/CompBio/estabroj/bergamot/ophion/client/python/'])
41/39: from
41/40: from ophion import Ophion
41/41: reset
41/42: import sys
41/43: sys.path.extend(['/home/exacloud/lustre1/CompBio/estabroj/bergamot/ophion/client/python/'])
41/44: sys.path
41/45: from ophion import Ophion
41/46: ophion
41/47: from ophion import Ophion
41/48: ophion
41/49: Ophion
41/50: reset
41/51: pwd
41/52: ls
41/53: from run import *
42/1: ls
42/2: cd ..
42/3: ls
42/4: cd ..
42/5: ls
42/6: cd experiments/predict_TFA/
42/7: ls
42/8: cd stan/
42/9: ls
42/10: from run import *
42/11: argv=['BRCA','controls-expression-of',1]
42/12:
cdata = TFACohort(argv[0], intx_types=[argv[1]],
                                cv_seed=(int(argv[2]) * 41) + 1, cv_prop=0.8)
42/13: cdata
42/14: clf = mpt.StanTFADefault(argv[1])
42/15: clf
42/16: clf.fit_coh(cdata,pheno='inter')
   1: from run import *
   2: argv = ['BRCA','controls-expression-of',1]
   3:
    cdata = TFACohort(argv[0], intx_types=[argv[1]],
                                cv_seed=(int(argv[2]) * 41) + 1, cv_prop=0.8)
   4: cdata
   5: clf = mpt.StanTFADefault(argv[1])
   6: clf
   7: cfl.
   8: clf.fit_coh(cdata, pheno='inter')
   9: x_rna = cdata.omic_mats.keys()
  10: x_rna
  11: x_rna = cdata.omic_mats['rna']
  12: x_rna
  13: x_cna = cdata.omic_mats['cna']
  14: cdata.genes
  15: test_prot = cdata.test_prot
  16: test_prot
  17: use_genes = (cdata.genes['rna'] & cdata.genes['cna'] & set(cdata.test_prot.columns))
  18: len(use_genes)
  19: y_use = cdata.test_prot.loc[,use_genes]
  20: y_use = cdata.test_prot.loc[:,use_genes]
  21: path_out = [x +1 for x in range(len(use_genes))]
  22: path_in = [x + 1 for x in range(len(use_genes))]
  23: path_out
  24: path_wght = [0.8 for _ in use_genes]
  25: path_wght
  26: path_wght += [0.05 for _ in use_path]
  27: path_obj
  28: path_obj = None
  29: get_type_networks
  30: from pathways
  31: from pathways import *
  32: ls
  33: cd ..
  34: cd ..
  35: ls
  36: cd ..
  37: ls
  38: cd features/
  39: ls
  40: from pathways import *
  41: ls
  42: less __init__.py
  43: from expression import *
  44: import sys
  45: sys.path
  46: from HetMan.features.pathways import *
  47: ls
  48: cd ..
  49: ls
  50: cd experiments/predict_TFA/
  51: ls
  52: cd stan/
  53: ls
  54: tfa_mat
  55: get_tfa
  56: from HetMan.features.cohorts import *
  57: tfa_mat = get_tfa_data()
  58: tfa_mat
  59: intx_types
  60: intx_types = 'controls-expression-of'
  61: path = get_type_networks(intx_types,tfa_mat.columns)
  62: path = get_type_networks([intx_types],tfa_mat.columns)
  63: path
  64: len(path)
  65: path.keys9)
  66: path.keys()
  67: len(path['controls-expression-of'])
  68: use_path = sorted([(up_gn,down_gn) for up_gn, down_gn in path['controls-expression-of'] if up_gn in use_genes and down_gn in use_genes])
  69: use_path
  70: len(use_path)
  71: len(use_genes)
  72: path_out += [x_rna.columns.get_loc(up_gn) + 1 for up_gn, down_gn in use_path]
  73: path_in += [x_rna.columns.get_loc(down_gn) + 1 for up_gn, down_gn in use_path]
  74: path_out
  75: path_in
  76: use_path
  77: path_wght
  78: path_wght += [ 0.05 for _ in use_path]
  79: tfa_model_code
  80: from HetMan.predict.bayesian_pathway.multi_protein import *
  81: tfa_model_code
  82: sm=pystan.StanModel(model_code=tfa_model_code,model_name='TFAPredict',verbose=True)
  83: sm
  84: sm.model_code
  85: data_dict = {'N':x_rna.shape[0],'G':x_rna.shape[1],'r':x_rna,'c':x_cna,'p':np.nan_to_num(y_use),'P':len(path_out),'po':path_out,'pi':path_in}
  86: data_dict
  87: n_chains
  88: n_chains=4
  89: parallel_jobs=1
  90: fit_obj=sm.sampling(chains=n_chains,n_jobs=parallel_jobs,iter=75,data=data_dict,init[{'wght':path_wght} for _ in range(n_chains)],verbose=True)
  91: fit_obj=sm.sampling(chains=n_chains,n_jobs=parallel_jobs,iter=75,data=data_dict,init=[{'wght':path_wght} for _ in range(n_chains)],verbose=True)
  92: cdata.omic_mats.keys()
  93: cdata.omic_mats['rna'].shape
  94: cdata.omic_mats['cna'].shape
  95: x_cna.shape
  96: y_use.shape
  97: y
  98: tfa_mat
  99: y_use = tfa_mat.loc[:,use_genes]
 100: data_dict = {'N':x_rna.shape[0],'G':x_rna.shape[1],'r':x_rna,'c':x_cna,'p':np.nan_to_num(y_use),'P':len(path_out),'po':path_out,'pi':path_in}
 101: fit_obj=sm.sampling(chains=n_chains,n_jobs=parallel_jobs,iter=75,data=data_dict,init=[{'wght':path_wght} for _ in range(n_chains)],verbose=True)
 102: y_use
 103: x_rna.shape[0]
 104: x_rna.shape[1]
 105: x_rna = x_rna.loc[:,use_genes]
 106: x_rna.shape
 107: x_cna.shape
 108: x_cna = x_rna.loc[:,use_genes]
 109: x_cna = x_cna.loc[:,use_genes]
 110: data_dict = {'N':x_rna.shape[0],'G':x_rna.shape[1],'r':x_rna,'c':x_cna,'p':np.nan_to_num(y_use),'P':len(path_out),'po':path_out,'pi':path_in}
 111: fit_obj=sm.sampling(chains=n_chains,n_jobs=parallel_jobs,iter=75,data=data_dict,init=[{'wght':path_wght} for _ in range(n_chains)],verbose=True)
 112: x_rna.shape
 113: y_rna.shape
 114: x_cna.shape
 115: y_use.shape
 116: y_use.index
 117: y_use.index - x_rna.index
 118: set(y_use.index) - set(x_rna.index)
 119: use_index = set(y_use.index) - set(x_rna.index)
 120: use_index = list(set(y_use.index) - set(x_rna.index))
 121: use_index
 122: y_use.loc[-use_index,:]
 123: y_use.loc[!use_index,:]
 124: y_use.loc[~use_index,:]
 125: y_use.iloc[~use_index,:]
 126: y_use.iloc[use_index,:]
 127: use_index = list(set(x_rnd.index))
 128: use_index = list(set(x_rna.index))
 129: y_use[use_index,:]
 130: y_use.loc[use_index,:]
 131: y_use = y_use.loc[use_index,:]
 132: data_dict = {'N':x_rna.shape[0],'G':x_rna.shape[1],'r':x_rna,'c':x_cna,'p':np.nan_to_num(y_use),'P':len(path_out),'po':path_out,'pi':path_in}
 133: fit_obj=sm.sampling(chains=n_chains,n_jobs=parallel_jobs,iter=75,data=data_dict,init=[{'wght':path_wght} for _ in range(n_chains)],verbose=True)
 134: len(use_genes)
 135: path_out = [x +1 for x in range(len(use_genes))]
 136: path_in = [x + 1 for x in range(len(use_genes))]
 137: path_out += [x_rna.columns.get_loc(up_gn) + 1 for up_gn, down_gn in use_path]
 138: path_in += [x_rna.columns.get_loc(down_gn) + 1 for up_gn, down_gn in use_path]
 139: path_wght = [0.8 for _ in use_genes]
 140: path_wght += [ 0.05 for _ in use_path]
 141: data_dict = {'N':x_rna.shape[0],'G':x_rna.shape[1],'r':x_rna,'c':x_cna,'p':np.nan_to_num(y_use),'P':len(path_out),'po':path_out,'pi':path_in}
 142: len(path_out)
 143: fit_obj=sm.sampling(chains=n_chains,n_jobs=parallel_jobs,iter=75,data=data_dict,init=[{'wght':path_wght} for _ in range(n_chains)],verbose=True)
 144: history()
 145: history
 146: history > interactive_sesh
 147: history
 148: interactive_sesh = history
 149: history
 150: history.write
 151: %save interactive_history
 152: %save 'interactive_history'
 153: import atexit
 154:
def save_history():
    """save the IPython history to a plaintext file"""
    histfile = os.path.join(ip.profile_dir.location, "history.txt")
    print("Saving plaintext history to %s" % histfile)
    lines = []
    # get previous lines
    # this is only necessary because we truncate the history,
    # otherwise we chould just open with mode='a'
    if os.path.exists(histfile):
        with open(histfile, 'r') as f:
            lines = f.readlines()

    # add any new lines from this session
    lines.extend(record[2] + '\n' for record in ip.history_manager.get_range())

    with open(histfile, 'w') as f:
        # limit to LIMIT entries
        f.writelines(lines[-LIMIT:])
 155: LIMIT=100000
 156: atexit.register(save_history)
 157: ls
 158:

     ...:         # limit to LIMIT entries
 159: %paste
 160: %history -g -f history.txt
